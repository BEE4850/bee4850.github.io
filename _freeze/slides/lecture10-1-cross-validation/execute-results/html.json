{
  "hash": "013ac86a956fe9c2a36a1ac91d8bd76f",
  "result": {
    "engine": "julia",
    "markdown": "---\ntitle: \"Cross Validation\"\nsubtitle: \"Lecture 17\"\nauthor: \"Vivek Srikrishnan\"\ncourse: \"BEE 4850\"\ninstitution: \"Cornell University\"\ndate: \"March 24, 2024\"\nformat:\n    revealjs:\n        slide-number: c/t\n        show-slide-number: all\n        center-title-slide: true\n        width: 1280\n        height: 720\n        transition: none\n        toc: true\n        toc-depth: 1\n        toc-title: \"Overview\"\n        history: false\n        link-external-newwindow: true\n        theme: ../sass/slides.scss\n        template-partials:\n            - title-slide.html\n        menu:\n            numbers: true\n        html-math-method: mathjax\n        include-in-header: mathjax-config.html\n        date-format: long\n        email-obfuscation: javascript\n        chalkboard:\n            theme: whiteboard\n            buttons: true\n        mermaid: \n            theme: dark\nengine: julia\nfilters:\n  - code-fullscreen\n---\n\n\n\n\n\n\n\n# Cross-Validation\n\n## Can We Drive Model Error to Zero?\n\nEffectively, no. **Why**?\n\n::: {.fragment .fade-in}\n\n- Inherent noise: even a perfect model wouldn't perfectly predict observations.\n- Model mis-specification (the cause of bias)\n- Model estimation is never \"right\" (the cause of variance) \n\n:::\n\n## Quantifying Generalization Error\n\nThe goal is then to minimize the generalized (expected) error:\n\n$$\\mathbb{E}\\left[L(X, \\theta)\\right] = \\int_X L(x, \\theta) \\pi(x)dx$$\n\nwhere $L(x, \\theta)$ is an error function capturing the discrepancy between $\\hat{f}(x, \\theta)$ and $y$.\n\n## In-Sample Error\n\nSince we don't know the \"true\" distribution of $y$, we could try to approximate it using the training data:\n\n$$\\hat{L} = \\min_{\\theta \\in \\Theta} L(x_n, \\theta)$$\n\nBut: **This is minimizing in-sample error and is likely to result an optimistic score.**\n\n## Held Out Data\n\nInstead, let's divide our data into a training dataset $y_k$ and testing dataset $\\tilde{y}_l$.\n\n1. Fit the model to $y_k$;\n2. Evaluate error on $\\tilde{y}_l$.\n\nThis results in an unbiased estimate of $\\hat{L}$ but is noisy.\n\n## $k$-Fold Cross-Validation\n\nWhat if we repeated this procedure for multiple held-out sets?\n\n1. Randomly split data into $k = n / m$ equally-sized subsets.\n2. For each $i = 1, \\ldots, k$, fit model to $y_{-i}$ and test on $y_i$.\n\nIf data are large, this is a good approximation.\n\n## Leave-One-Out Cross-Validation (LOOCV)\n\nThe problem with $k$-fold CV, when data is scarce, is withholding $n/k$ points.\n\n**LOO-CV**: Set $k=n$\n\n**The trouble**: estimates of $L$ are highly correlated since every two datasets share $n-2$ points.\n\n**The benefit**: LOO-CV approximates seeing \"the next datum\".\n\n## LOO-CV Algorithm\n\n1. Drop one value $y_i$.\n2. Refit model on rest of data $y_{-i}$.\n3. Evaluate $L(y_i | y_{-i})$.\n4. Repeat on rest of data set.\n\n# References\n\n## Refernences (Scroll for Full List)\n\n",
    "supporting": [
      "lecture10-1-cross-validation_files"
    ],
    "filters": [],
    "includes": {}
  }
}