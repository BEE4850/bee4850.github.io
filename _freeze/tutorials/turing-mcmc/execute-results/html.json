{
  "hash": "98ddf02c6874ec53a5436c320740c24e",
  "result": {
    "markdown": "---\ntitle: \"Markov Chain Monte Carlo With Turing\"\nexecute:\n    error: true\n    warning: true\nformat:\n    html: default\n    ipynb: default\n---\n\n\n\n## Overview\n\nThis tutorial will give some examples of using `Turing.jl` and Markov Chain Monte Carlo to sample from posterior distributions.\n\n## Setup\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing Turing\nusing Distributions\nusing Plots\ndefault(fmt = :png) # the tide gauge data is long, this keeps images a manageable size\nusing LaTeXStrings\nusing StatsPlots\nusing Measures\nusing StatsBase\nusing Optim\nusing Random\nusing DataFrames\nusing DataFramesMeta\nusing Dates\nusing CSV\n```\n:::\n\n\nAs this tutorial involves random number generation, we will set a random seed to ensure reproducibility.\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nRandom.seed!(1);\n```\n:::\n\n\n## Fitting A Linear Regression Model\n\nLet's start with a simple example: fitting a linear regression model to simulated data.\n\n::: {.callout-tip}\n## Positive Control Tests\n\nSimulating data with a known data-generating process and then trying to obtain the parameters for that process is an important step in any workflow.\n:::\n\n### Simulating Data\n\nThe data-generating process for this example will be:\n$$\n\\begin{gather}\ny = 5 + 2x + \\varepsilon \\\\\n\\varepsilon \\sim \\text{Normal}(0, 3),\n\\end{gather}\n$$\nwhere $\\varepsilon$ is so-called \"white noise\", which adds stochasticity to the data set. The generated dataset is shown in @fig-scatter-regression.\n\n::: {.cell .column-margin execution_count=4}\n\n::: {.cell-output .cell-output-display execution_count=5}\n![Scatterplot of our generated data.](turing-mcmc_files/figure-html/fig-scatter-regression-output-1.png){#fig-scatter-regression}\n:::\n:::\n\n\n### Model Specification\n\nThe statistical model for a standard linear regression problem is\n$$\n\\begin{gather}\ny = a + bx + \\varepsilon \\\\\n\\varepsilon \\sim \\text{Normal}(0, \\sigma).\n\\end{gather}\n$$\n\nRearranging, we can rewrite the likelihood function as:\n$$y \\sim \\text{Normal}(\\mu, \\sigma),$$\nwhere $\\mu = a + bx$. This means that we have three parameters to fit: $a$, $b$, and $\\sigma^2$.\n\nNext, we need to select priors on our parameters. We'll use relatively generic distributions to avoid using the information we have (since we generated the data ourselves), but in practice, we'd want to use any relevant information that we had from our knowledge of the problem. Let's use relatively diffuse normal distributions for the trend parameters $a$ and $b$ and a half-normal distribution (a normal distribution truncated at 0, to only allow positive values) for the variance $\\sigma^2$, as recommended by @Gelman2006-wv.\n\n$$\n\\begin{gather}\na \\sim \\text{Normal(0, 10)} \\\\\nb \\sim \\text{Normal(0, 10)} \\\\\n\\sigma \\sim \\text{Half-Normal}(0, 25)\n\\end{gather}\n$$\n\n### Using Turing\n\n#### Coding the Model\n\n`Turing.jl` uses the `@model` macro to specify the model function. We'll follow the setup in the [Turing documentation](https://turinglang.org/dev/tutorials/05-linear-regression).\n\nTo specify distributions on parameters (and the data, which can be thought of as uncertain parameters in Bayesian statistics), use a tilde `~`, and use equals `=` for transformations (which we don't have in this case).\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\n@model function linear_regression(x, y)\n    # set priors\n    σ ~ truncated(Normal(0, 25); lower=0)      # <1>\n    a ~ Normal(0, 10)                           # <2>\n    b ~ Normal(0, 10)                           # <2>\n\n    # compute the likelihood\n    for i = 1:length(y)                         # <3>\n        # compute the mean value for the data point\n        μ = a + b * x[i]\n        y[i] ~ Normal(μ, σ)\n    end\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nlinear_regression (generic function with 2 methods)\n```\n:::\n:::\n\n\n1. Standard deviations must be positive, so we use a normal distribution truncated at zero.\n2. We'll keep these both relative uninformative to reflect a more \"realistic\" modeling scenario.\n3. In this case, we specify the likelihood with a loop. We could also rewrite this as a joint likelihood over all of the data using linear algebra, which might be more efficient for large and/or complex models or datasets, but the loop is more readable in this simple case.\n\n\n#### Fitting The Model\n\nNow we can call the sampler to draw from the posterior. We'll use the [No-U-Turn sampler](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo#No_U-Turn_Sampler) [@Hoffman2014-gv], which is a Hamiltonian Monte Carlo algorithm (a different category of MCMC sampler than the Metropolis-Hastings algorithm discussed in class). We'll also use 4 chains so we can test that the chains are well-mixed, and each chain will be run for 5,000 iterations^[Hamiltonian Monte Carlo samplers often need to be run for fewer iterations than Metropolis-Hastings samplers, as the exploratory step uses information about the gradient of the statistical model, versus the random walk of Metropolis-Hastings. The disadvantage is that this gradient information must be available, which is not always the case for external simulation models. Simulation models coded in Julia can usually be automatically differentiated by Turing's tools, however.]\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\n# set up the sampler\nmodel = linear_regression(x, y)     # <1>\nn_chains = 4                         # <2>\nn_per_chain = 5000                  # <3>\nchain = sample(model, NUTS(), MCMCThreads(), n_per_chain, n_chains, drop_warmup=true) # <4>\n@show chain                         # <5>\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n└ @ AbstractMCMC ~/.julia/packages/AbstractMCMC/F9Hbk/src/sample.jl:296\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n┌ Info: Found initial step size\n└   ϵ = 0.00625\n┌ Info: Found initial step size\n└   ϵ = 0.025\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n┌ Info: Found initial step size\n└   ϵ = 0.2\n\r┌ Info: Found initial step size\n└   ϵ = 0.003125\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSampling (1 threads):  50%|██████████████▌              |  ETA: 0:00:00\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rSampling (1 threads): 100%|█████████████████████████████| Time: 0:00:01\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nchain = MCMC chain (5000×15×4 Array{Float64, 3})\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre>Chains MCMC chain (5000×15×4 Array{Float64, 3}):\nIterations        = 1001:1:6000\nNumber of chains  = 4\nSamples per chain = 5000\nWall duration     = 19.75 seconds\nCompute duration  = 16.95 seconds\nparameters        = σ, a, b\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\nSummary Statistics\n <span class=\"ansi-bold\"> parameters </span> <span class=\"ansi-bold\">    mean </span> <span class=\"ansi-bold\">     std </span> <span class=\"ansi-bold\">    mcse </span> <span class=\"ansi-bold\">   ess_bulk </span> <span class=\"ansi-bold\">   ess_tail </span> <span class=\"ansi-bold\">    rhat</span> ⋯\n <span class=\"ansi-bright-black-fg\">     Symbol </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\">    Float64 </span> <span class=\"ansi-bright-black-fg\">    Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64</span> ⋯\n           σ    5.3881    0.9886    0.0097   10835.5758   10595.6871    1.0003 ⋯\n           a    7.3483    2.1028    0.0231    8360.9948    8903.0798    1.0002 ⋯\n           b    1.7993    0.1865    0.0021    8290.7784    8803.3686    1.0003 ⋯\n<span class=\"ansi-cyan-fg\">                                                                1 column omitted</span>\nQuantiles\n <span class=\"ansi-bold\"> parameters </span> <span class=\"ansi-bold\">    2.5% </span> <span class=\"ansi-bold\">   25.0% </span> <span class=\"ansi-bold\">   50.0% </span> <span class=\"ansi-bold\">   75.0% </span> <span class=\"ansi-bold\">   97.5% </span>\n <span class=\"ansi-bright-black-fg\">     Symbol </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span>\n           σ    3.8905    4.6873    5.2471    5.9322    7.7226\n           a    3.1190    5.9784    7.3761    8.7487   11.4444\n           b    1.4350    1.6773    1.7968    1.9214    2.1732\n</pre>\n```\n:::\n\n:::\n:::\n\n\n1. Initialize the model with the data.\n2. We use multiple chains to help diagnose convergence.\n3. This sets the number of iterations for each chain.\n4. Sample from the posterior using NUTS and drop the iterations used to warmup the sampler. The `MCMCThreads()` call tells the sampler to use available processor threads for the multiple chains, but it will just sample them in serial if only one thread exists.\n5. The `@show` macro makes the display of the output a bit cleaner.\n\nHow can we interpret the output? The first parts of the summary statistics are straightforward: we get the mean, standard deviation, and Monte Carlo standard error (`mcse`) of each parameter. We also get information about the effective sample size (ESS)^[The ESS reflects the efficiency of the sampler: this is an estimate of the equivalent number of independent samples; the more correlated the samples, the lower the ESS.] and $\\hat{R}$, which measures the ratio of within-chain variance and across-chain variance as a check for convergence^[The closer $\\hat{R}$ is to 1, the better.].\n\nIn this case, we can see that we were generally able to recover the \"true\" data-generating values of $\\sigma = 4$ and $b = 2$, but $a$ is slightly off (the mean is 3, rather than the data-generating value of 5). In fact, there is substantial uncertainty about $a$, with a 95% credible interval of $(3.1, 11.4)$  (compared to $(1.4, 2.2)$ for $b$). This isn't surprising: given the variance of the noise $\\sigma^2$, there are many different intercepts which could fit within that spread.\n\nLet's now plot the chains for visual inspection.\n\n::: {.cell execution_count=7}\n``` {.julia .cell-code}\nplot(chain)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n![Output from the MCMC sampler. Each row corresponds to a different parameter: $\\sigma$, $a$, and $b$. Each chain is shown in a different color. The left column shows the sampler traceplots, and the right column the resulting posterior distributions.](turing-mcmc_files/figure-html/fig-chains-regression-output-1.png){#fig-chains-regression}\n:::\n:::\n\n\nWe can see from @fig-chains-regression that our chains mixed well and seem to have converged to similar distributions! The traceplots have a \"hairy caterpiller\" appearance, suggesting relatively little autocorrelation. We can also see how much more uncertainty there is with the intercept $a$, while the slope $b$ is much more constrained.\n\nAnother interesting comparison we can make is with the maximum-likelihood estimate (MLE), which we can obtain through optimization.\n\n::: {.cell .column-margin execution_count=8}\n``` {.julia .cell-code}\nmle_model = linear_regression(x, y)\nmle = optimize(mle_model, MLE()) # <1>\ncoef(mle)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n3-element Named Vector{Float64}\nA  │ \n───┼────────\nσ  │ 4.75545\na  │ 7.65636\nb  │ 1.77736\n```\n:::\n:::\n\n\n1. This is where we use the `Optim.jl` package in this tutorial.\n\nWe could also get the maximum *a posteriori* (MAP) estimate, which includes the prior density, by replacing `MLE()` with `MAP()`.\n\n### Model Diagnostics and Posterior Predictive Checks\n\nOne advantage of the Bayesian modeling approach here is that we have access to a *generative model*, or a model which we can use to generate datasets. This means that we can now use Monte Carlo simulation, sampling from our posteriors, to look at how uncertainty in the parameter estimates propagates through the model. Let's write a function which gets samples from the MCMC chains and generates datasets.\n\n::: {.cell execution_count=9}\n``` {.julia .cell-code}\nfunction mc_predict_regression(x, chain)\n    # get the posterior samples\n    a = Array(group(chain, :a))     # <1>\n    b = Array(group(chain, :b))     # <1>\n    σ = Array(group(chain, :σ))   # <1>\n\n    # loop and generate alternative realizations\n    μ = a' .+ x * b'\n    y = zeros((length(x), length(a)))\n    for i = 1:length(a)\n        y[:, i] = rand.(Normal.(μ[:, i], σ[i]))\n    end\n    return y\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\nmc_predict_regression (generic function with 1 method)\n```\n:::\n:::\n\n\n1. The `Array(group())` syntax is more general than we need, but is useful if we have multiple variables which were sampled as a group, for example multiple regression coefficients. Otherwise, we can just use *e.g.* `Array(chain, :a)`.\n\nNow we can generate a predictive interval and median and compare to the data.\n\n::: {.cell execution_count=10}\n``` {.julia .cell-code}\nx_pred = 0:20\ny_pred = mc_predict_regression(x_pred, chain)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n21×20000 Matrix{Float64}:\n -0.440341  14.2143    3.73736    3.98402  …  13.9896   12.1475    5.46884\n -7.87522    4.36333  -7.19389    3.34537      9.48413  11.4606    3.81313\n 17.3151     8.9222    0.292523  12.9925       7.6094   11.5593   10.3064\n  2.28767    7.4078    9.09122    9.90026     21.4866   21.4643    4.38994\n 23.1321     9.28746   9.56973   17.9069      14.6245    6.80164  23.3068\n 10.0429    12.6741   23.6552    24.0107   …  17.07     12.9291   19.7511\n 23.3753    33.5452   11.6653    14.3672      21.4006   24.598    25.4434\n 11.6998     7.79426  16.9991    17.8128       8.01083  19.9234   17.2112\n 16.3753    22.476    20.8614    20.1019      15.169    22.8508   21.7635\n 17.1411    14.5313   32.2826    23.3866      27.5636   22.508    19.5266\n 27.7594    28.5052   30.7478    35.5763   …  22.858    27.8321   17.396\n 28.2636    24.2078   23.8358    27.5576      23.1143   30.4325   25.8818\n 35.3991    39.3917   40.5648    30.0351      29.1015   31.9592   38.7204\n 27.9246    34.618    30.7003    31.2341      20.0888   18.9489   24.3442\n 44.3301    21.7205   26.0331    34.8189      34.9326   26.6271   37.492\n 29.0048    45.0313   38.5674    34.2622   …  38.0261   42.037    26.6537\n 38.1531    35.6617   28.2519    36.4022      42.352    38.8477   38.4148\n 41.646     26.0779   38.9314    42.054       30.3764   37.7851   40.6086\n 38.4945    51.9845   42.9168    42.3723      37.3804   47.7923   38.3248\n 37.3901    44.5299   44.5209    46.5456      35.1081   43.7139   47.2316\n 55.3581    41.1191   45.0658    35.4023   …  40.9341   35.7394   36.0986\n```\n:::\n:::\n\n\nNotice the dimension of `y_pred`: we have 20,000 columns, because we have 4 chains with 5,000 samples each. If we had wanted to subsample (which might be necessary if we had hundreds of thousands or millions of samples), we could have done that within `mc_linear_regression` before simulation.\n\n::: {.cell execution_count=11}\n``` {.julia .cell-code}\n# get the boundaries for the 95% prediction interval and the median\ny_ci_low = quantile.(eachrow(y_pred), 0.025)\ny_ci_hi = quantile.(eachrow(y_pred), 0.975)\ny_med = quantile.(eachrow(y_pred), 0.5)\n```\n:::\n\n\nNow, let's plot the prediction interval and median, and compare to the original data.\n\n::: {.cell execution_count=12}\n``` {.julia .cell-code}\n# plot prediction interval\nplot(x_pred, y_ci_low, fillrange=y_ci_hi, xlabel=L\"$x$\", ylabel=L\"$y$\", fillalpha=0.3, fillcolor=:blue, label=\"95% Prediction Interval\", legend=:topleft, linealpha=0) # <1>\nplot!(x_pred, y_med, color=:blue, label=\"Prediction Median\") # <2>\nscatter!(x, y, color=:red, label=\"Data\") # <3>\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n![Posterior 95% predictive interval and median for the linear regression model. The data is plotted in red for comparison.](turing-mcmc_files/figure-html/fig-prediction-regression-output-1.png){#fig-prediction-regression}\n:::\n:::\n\n\n1. Plot the 95% posterior prediction interval as a shaded blue ribbon.\n2. Plot the posterior prediction median as a blue line.\n3. Plot the data as discrete red points.\n\nFrom @fig-prediction-regression, it looks like our model might be slightly under-confident, as with 20 data points, we would expect 5% of them (or 1 data point) to be outside the 95% prediction interval. It's hard to tell with only 20 data points, though! We could resolve this by tightening our priors, but this depends on how much information we used to specify them in the first place. The goal shouldn't be to hit a specific level of uncertainty, but if there is a sound reason to tighten the priors, we could do so.\n\nNow let's look at the residuals from the posterior median and the data. The partial autocorrelations plotted in @fig-residuals-regression are not fully convincing, as there are large autocorrelation coefficients with long lags, but the dataset is quite small, so it's hard to draw strong conclusions. We won't go further down this rabbit hole as we know our data-generating process involved independent noise, but for a real dataset, we might want to try a model specification with autocorrelated errors to compare.\n\n::: {.cell .column-margin execution_count=13}\n``` {.julia .cell-code}\n# calculate the median predictions and residuals\ny_pred_data = mc_predict_regression(x, chain)\ny_med_data = quantile.(eachrow(y_pred_data), 0.5)\nresiduals = y_med_data .- y\n\n# plot the residuals and a line to show the zero\nplot(pacf(residuals, 1:4), line=:stem, marker=:circle, legend=:false, grid=:false, linewidth=2, xlabel=\"Lag\", ylabel=\"Partial Autocorrelation\", markersize=8, tickfontsize=14, guidefontsize=16, legendfontsize=16)\nhline!([0], linestyle=:dot, color=:red) # <1>\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n![Partial autocorrelation function of model residuals, relative to the predictive median.](turing-mcmc_files/figure-html/fig-residuals-regression-output-1.png){#fig-residuals-regression}\n:::\n:::\n\n\n## Fitting Extreme Value Models to Tide Gauge Data\n\nLet's now look at an example of fitting an extreme value distribution (namely, a [generalized extreme value distribution](https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution), or GEV) to tide gauge data. GEV distributions have three parameters:\n\n- $\\mu$, the *location* parameter, which reflects the positioning of the bulk of the GEV distribution;\n- $\\sigma$, the *scale* parameter, which reflects the width of the bulk;\n- $\\xi$, the *shape* parameter, which reflects the thickness and boundedness of the tail.\n\nThe shape parameter $\\xi$ is often of interest, as there are three classes of GEV distributions corresponding to different signs:\n\n- $\\xi < 0$ means that the distribution is bounded;\n- $\\xi = 0$ means that the distribution has a thinner tail, so the \"extreme extremes\" are less likely;\n- $\\xi > 0$ means that the distribution has a thicker tail.\n\n### Load Data\n\nFirst, let's load the data. We'll use [data from the University of Hawaii Sea Level Center](https://uhslc.soest.hawaii.edu/datainfo/) [@Caldwell2015-kc] for San Francisco, from 1897-2013. If you don't have this data and are working with the notebook, download it [here](https://uhslc.soest.hawaii.edu/data/csv/rqds/hourly/h551a.csv). We'll assume it's in a `data/` subdirectory, but change the path as needed.\n\nThe dataset consists of dates and hours and the tide-gauge measurement, in mm. We'll load the dataset into a `DataFrame`.\n\n::: {.cell execution_count=14}\n``` {.julia .cell-code}\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n    df = @chain fname begin                        # <1> \n        CSV.File(; delim=',', header=false)        # <2>\n        DataFrame                                  # <3>\n        rename(\"Column1\" => \"year\",                # <4>\n                \"Column2\" => \"month\",              # <4>\n                \"Column3\" => \"day\",                # <4>\n                \"Column4\" => \"hour\",               # <4>\n                \"Column5\" => \"gauge\")              # <4>\n        # need to reformat the decimal date in the data file\n        @transform :datetime = DateTime.(:year, :month, :day, :hour) # <5>\n        # replace -99999 with missing\n        @transform :gauge = ifelse.(abs.(:gauge) .>= 9999, missing, :gauge) # <6>\n        select(:datetime, :gauge) # <7>\n    end\n    return df\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\nload_data (generic function with 1 method)\n```\n:::\n:::\n\n\n1. This uses the `DataFramesMeta.jl` package, which makes it easy to string together commands to load and process data\n2. Load the file, assuming there is no header.\n3. Convert to a `DataFrame`.\n4. Rename columns for ease of access.\n5. Reformat the decimal datetime provided in the file into a Julia `DateTime`.\n6. Replace missing data with `missing`.\n7. Select only the `:datetime` and `:gauge` columns.\n\n::: {#tbl-data .cell .column-margin tbl-cap='Processed hourly tide gauge data from San Francisco, from 8/1/1897-1/31/2023.' execution_count=15}\n``` {.julia .cell-code}\ndat = load_data(\"data/h551a.csv\")\nfirst(dat, 6)\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div><div style = \"float: left;\"><span>6×2 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">datetime</th><th style = \"text-align: left;\">gauge</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"DateTime\" style = \"text-align: left;\">DateTime</th><th title = \"Union{Missing, Int64}\" style = \"text-align: left;\">Int64?</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">1897-08-01T08:00:00</td><td style = \"text-align: right;\">3292</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">1897-08-01T09:00:00</td><td style = \"text-align: right;\">3322</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">1897-08-01T10:00:00</td><td style = \"text-align: right;\">3139</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">1897-08-01T11:00:00</td><td style = \"text-align: right;\">2835</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">1897-08-01T12:00:00</td><td style = \"text-align: right;\">2377</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">1897-08-01T13:00:00</td><td style = \"text-align: right;\">2012</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.julia .cell-code}\n@df dat plot(:datetime, :gauge, label=\"Observations\", bottom_margin=9mm) # <1>\nxaxis!(\"Date\", xrot=30)\nyaxis!(\"Mean Water Level\")\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n![Hourly mean water at the San Francisco tide gauge from 1897-2023.](turing-mcmc_files/figure-html/fig-raw-data-output-1.png){#fig-raw-data fig-alt='Tide gauge data'}\n:::\n:::\n\n\n1. This uses the `DataFrame` plotting recipe with the `@df` macro from `StatsPlots.jl`. This is not needed (you could replace *e.g.* `:datetime` with `dat.datetime`), but it cleans things up slightly.\n\nNext, we need to detrend the data to remove the impacts of sea-level rise. We do this by removing a one-year moving average, centered on the data point, per the recommendation of @Arns2013-tl.\n\n::: {.cell execution_count=17}\n``` {.julia .cell-code}\n# calculate the moving average and subtract it off\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n\n# plot\n@df dat_ma plot(:datetime, :residual, label=\"Detrended Observations\", bottom_margin=9mm)\nxaxis!(\"Date\", xrot=30)\nyaxis!(\"Mean Water Level\")\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n![Mean water level from the San Francisco tide gauge, detrended using a 1-year moving average centered on the data point, per the recommendation of @Arns2013-tl.](turing-mcmc_files/figure-html/fig-data-detrend-output-1.png){#fig-data-detrend fig-alt='Detrended tide gauge data'}\n:::\n:::\n\n\nThe last step in preparing the data is to find the annual maxima. We can do this using the `groupby`, `transform`, and `combine` functions from `DataFrames.jl`, as below.\n\n::: {.cell execution_count=18}\n``` {.julia .cell-code}\n# calculate the annual maxima\ndat_ma = dropmissing(dat_ma) # <1>\ndat_annmax = combine(dat_ma -> dat_ma[argmax(dat_ma.residual), :],      # <2>\n                groupby(transform(dat_ma, :datetime => x->year.(x)), :datetime_function)) # <2>\ndelete!(dat_annmax, nrow(dat_annmax)) # <3>\n\n# make a histogram of the maxima to see the distribution\nhistogram(dat_annmax.residual, label=false)\nylabel!(\"Count\")\nxlabel!(\"Mean Water Level (mm)\")\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n![Histogram of annual block maxima  from 1898-2022 from the San Francisco tide gauge dataset.](turing-mcmc_files/figure-html/fig-annmax-histogram-output-1.png){#fig-annmax-histogram fig-alt='Annual maxima histogram'}\n:::\n:::\n\n\n1. If we don't drop the values which are missing, they will affect the next call to `argmax`.\n2. This first groups the data based on the year (with `groupby` and using `Dates.year()` to get the year of each data point), then pulls the rows which correspond to the maxima for each year (using `argmax`).\n3. This will delete the last year, in this case 2023, because the dataset only goes until March 2023 and this data point is almost certainly an outlier due to the limited data from that year.\n\n### Fit The Model\n\n::: {.cell execution_count=19}\n``` {.julia .cell-code}\n@model function gev_annmax(y)               \n    μ ~ Normal(1000, 100)                   # <1>\n    σ ~ truncated(Normal(0, 100); lower=0)  # <2>\n    ξ ~ Normal(0, 0.5)                      # <3>\n\n    y ~ GeneralizedExtremeValue(μ, σ, ξ)    # <4>\nend\n\ngev_model = gev_annmax(dat_annmax.residual) # <5>\nn_chains = 4                                # <6>\nn_per_chain = 5000                          # <7>\ngev_chain = sample(gev_model, NUTS(), MCMCThreads(), n_per_chain, n_chains; drop_warmup=true)   # <8>\n@show gev_chain\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n└ @ AbstractMCMC ~/.julia/packages/AbstractMCMC/F9Hbk/src/sample.jl:296\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n┌ Info: Found initial step size\n└   ϵ = 0.2\n┌ Info: Found initial step size\n└   ϵ = 0.0125\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n┌ Info: Found initial step size\n└   ϵ = 0.2\n\r┌ Info: Found initial step size\n└   ϵ = 0.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSampling (1 threads):  50%|██████████████▌              |  ETA: 0:00:03\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rSampling (1 threads): 100%|█████████████████████████████| Time: 0:00:07\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ngev_chain = MCMC chain (5000×15×4 Array{Float64, 3})\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=20}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre>Chains MCMC chain (5000×15×4 Array{Float64, 3}):\nIterations        = 1001:1:6000\nNumber of chains  = 4\nSamples per chain = 5000\nWall duration     = 14.61 seconds\nCompute duration  = 14.08 seconds\nparameters        = μ, σ, ξ\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\nSummary Statistics\n <span class=\"ansi-bold\"> parameters </span> <span class=\"ansi-bold\">      mean </span> <span class=\"ansi-bold\">     std </span> <span class=\"ansi-bold\">    mcse </span> <span class=\"ansi-bold\">   ess_bulk </span> <span class=\"ansi-bold\">   ess_tail </span> <span class=\"ansi-bold\">    rh</span> ⋯\n <span class=\"ansi-bright-black-fg\">     Symbol </span> <span class=\"ansi-bright-black-fg\">   Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\"> Float64 </span> <span class=\"ansi-bright-black-fg\">    Float64 </span> <span class=\"ansi-bright-black-fg\">    Float64 </span> <span class=\"ansi-bright-black-fg\"> Float</span> ⋯\n           μ   1257.8174    5.6701    0.0513   12216.6653   12642.3312    1.00 ⋯\n           σ     57.1982    4.3214    0.0366   14201.5441   12575.6044    1.00 ⋯\n           ξ      0.0292    0.0625    0.0008    5915.1552    7463.8803    1.00 ⋯\n<span class=\"ansi-cyan-fg\">                                                               2 columns omitted</span>\nQuantiles\n <span class=\"ansi-bold\"> parameters </span> <span class=\"ansi-bold\">      2.5% </span> <span class=\"ansi-bold\">     25.0% </span> <span class=\"ansi-bold\">     50.0% </span> <span class=\"ansi-bold\">     75.0% </span> <span class=\"ansi-bold\">     97.5% </span>\n <span class=\"ansi-bright-black-fg\">     Symbol </span> <span class=\"ansi-bright-black-fg\">   Float64 </span> <span class=\"ansi-bright-black-fg\">   Float64 </span> <span class=\"ansi-bright-black-fg\">   Float64 </span> <span class=\"ansi-bright-black-fg\">   Float64 </span> <span class=\"ansi-bright-black-fg\">   Float64 </span>\n           μ   1246.8628   1253.9821   1257.8132   1261.6027   1269.2078\n           σ     49.4284     54.1859     56.9353     59.9665     66.3174\n           ξ     -0.0811     -0.0151      0.0250      0.0699      0.1627\n</pre>\n```\n:::\n\n:::\n:::\n\n\n1. Location parameter prior: We know that this is roughly on the 1000 mm order of magnitude, but want to keep this relatively broad.\n2. Scale parameter prior: This parameter must be positive, so we use a normal truncated at zero.\n3. Shape parameter prior: These are usually small and are hard to constrain, so we will use a more informative prior.\n4. The data is independently GEV-distributed as we've removed the long-term trend and are using long blocks.\n5. Initialize the model.\n6. We use multiple chains to help diagnose convergence.\n7. This sets the number of iterations for each chain.\n8. Sample from the posterior using NUTS and drop the iterations used to warmup the sampler.\n\n::: {.cell execution_count=20}\n``` {.julia .cell-code}\nplot(gev_chain)\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n![Traceplots (left) and marginal distributions (right) from the MCMC sampler for the GEV model.](turing-mcmc_files/figure-html/fig-gev-traceplot-output-1.png){#fig-gev-traceplot fig-alt='MCMC sampler output'}\n:::\n:::\n\n\nFrom @fig-gev-traceplot, it looks like all of the chains have converged to the same distribution; the Gelman-Rubin diagnostic is also close to 1 for all parameters. Next, we can look at a corner plot to see how the parameters are correlated.\n\n::: {.cell execution_count=21}\n``` {.julia .cell-code}\ncorner(gev_chain)\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n![Corner plot for the GEV model.](turing-mcmc_files/figure-html/fig-gev-corner-output-1.png){#fig-gev-corner fig-alt='GEV corner plot'}\n:::\n:::\n\n\n@fig-gev-corner suggests that the location and scale parameters $\\mu$ and $\\sigma$ are positively correlated. This makes some intuitive sense, as increasing the location parameter shifts the bulk of the distribution in a positive direction, and the increasing scale parameter then increases the likelihood of lower values. However, if these parameters are increased, the shape parameter $\\xi$ decreases, as the tail of the GEV  does not need to be as thick due to the increased proximity of outliers to the bulk.\n\n",
    "supporting": [
      "turing-mcmc_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}