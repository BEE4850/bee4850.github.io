---
title: "Homework 1 Solutions"
format:
    html:        
        warning: true
        error: true
    pdf:
        warning: true
        error: true
        keep-tex: true
        include-in-header: 
            text: |
                \usepackage{fvextra}
                \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
        include-before-body:
            text: |
                \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
                showspaces = false,
                showtabs = false,
                breaksymbolleft={},
                breaklines
                % Note: setting commandchars=\\\{\} here will cause an error 
                }
    ipynb:
        warning: true
        error: true
jupyter: julia-1.9
format-links: [pdf, ipynb]
freeze: false
---

## Overview

### Instructions

The goal of this homework assignment is to introduce you to simulation-based data analysis. 

* Problem 1 asks you to explore whether a difference between data collected from two groups might be statistically meaningful or the result of noise. This problem repeats the analysis from [Statistics Without The Agonizing Pain](https://www.youtube.com/watch?v=5Dnw46eC-0o) by John Rauser (which is a neat watch!).
* Problem 2 asks you to evaluate an interview method for finding the level of cheating on a test to determine whether cheating was relatively high or low. This problem was adapted from [Bayesian Methods for Hackers](https://dataorigami.net/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/).

### Load Environment

The following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts.

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

The following packages are included in the environment (to help you find other similar packages in other languages). The code below loads these packages for use in the subsequent notebook (the desired functionality for each package is commented next to the package).

```{julia}
#| output: false

using Random # random number generation and seed-setting
using DataFrames # tabular data structure
using CSVFiles # reads/writes .csv files
using Distributions # interface to work with probability distributions
using Plots # plotting library
using StatsBase # adds some basic statistical functionality (means, medians, empirical cumulative densities, etc)
using StatsPlots # for qqplots
```

## Problems

### Problem 1 
```{julia}
data = DataFrame(load("data/bites.csv")) # load data into DataFrame

# print data variable (semi-colon suppresses echoed output in Julia, which in this case would duplicate the output)
@show data; 
```

```{julia}
# split data into vectors of bites for each group
beer = data[data.group .== "beer", :bites]
water = data[data.group .== "water", :bites]

observed_difference = mean(beer) - mean(water)
@show observed_difference;
```

**In this problem**:

* Conduct the above procedure to generate 50,000 simulated datasets under the skeptic's hypothesis. 
* Plot a histogram of the results and add a dashed vertical line to show the experimental difference (if you are using Julia, feel free to look at the [Making Plots with Julia tutorial](https://viveks.me/simulation-data-analysis/tutorials/julia-plots.html) on the class website). 
* Draw conclusions about the plausibility of the skeptic's hypothesis that there is no difference? Feel free to use any quantitative or qualitative assessments of your simulations and the observed difference.

***Solution***:

First, we write a function (`simulate_differences()`) to generate a new data set and compute the group differences under the skeptic's hypothesis by shuffling the data across the two groups (this is called *the non-parametric bootstrap*, which we will talk about more later):

```{julia}
# simulate_differences: function which simulates a new group difference based on the skeptic's hypothesis of no "real" difference between groups by shuffling the input data across groups
# inputs:
#   y₁: vector of bite counts for the beer-drinking group
#   y₂: vector of bite counts for the water-drinking group
# output:
#   a simulated difference between shuffled group averages
function simulate_differences(y₁, y₂)
    # concatenate both vectors into a single vector
    y = vcat(y₁, y₂)

    # create new experimental groups consistent with skeptic's hypothesis
    y_shuffle = shuffle(y)   # shuffle the combined data
    n₁ = length(y₁)
    x₁ = y_shuffle[1:n₁]
    x₂ = y_shuffle[(n₁+1):end]

    # compute difference between new group means
    diff = mean(x₁) - mean(x₂)
    return diff
end
```

Next, we evaluate this function 10,000 times and plot the resulting histogram of differences. 

In Julia (and in Python), it is convenient to use a *comprehension* to automatically allocate the output of the `for` loop to a vector. The syntax for a comprehension is `[some_function(input) for input in some_range]`. In this case, the index `input` doesn't appear in the comprehension as we're just repeating the exact same calculation every time:

```{julia}
shuffled_differences = [simulate_differences(beer, water) for i in 1:50_000]
```

Without a comprehension, this loop would look something like:

```julia
shuffled_diffs = zeros(10_000)
for i in 1:length(shuffled_diffs)
    shuffled_differences[i] = simulate_differences(beer, water)
end
```

Back to the problem. Now let's plot the histogram. When you see a `!` after a function name in Julia, it means that this is a *mutating function*, which changes the object that it acts on, rather than returning a new object and preserving the old one. In this case, the plot object is changed to add new elements to the original histogram.

```{julia}
#| label: fig-bites
#| fig-cap: "Histogram of simulated differences between the average bites for the beer group and the average bites for the water group under the assumption that differences are due only to random chance. A positive value indicates that the beer group is bitten more often. The red line is the group difference from the experimental data."
#| fig-align: center
#| fig-alt: "Histogram of simulated group differences."
hist = histogram(shuffled_differences, label="Simulated Differences") # plot the basic histogram
# MAKE SURE TO ADD AXIS LABELS!
xlabel!(hist, "Increased Number of Average Bites for Beer Group")
ylabel!(hist, "Count")

# now add a vertical line for the experimentally-observed difference
vline!(hist, [observed_difference], color=:red, linestyle=:dash, label="Observed Difference")
```

What do we see from @fig-bites?

* The simulated differences follow roughly a normal distribution centered at a value of zero. This is not surprising given the hypothesis that there is no "true" difference in bite frequency between the two groups.
* The observed data are extremely unlikely if the skeptic's hypothesis is true. We can calculate the probability of seeing data that extreme given this hypothesis (also called the *p-value*) by finding the empirical cumulative density function of the simulated data vector:

  ```{julia}
  empirical_cdf = ecdf(shuffled_differences)
  1 - empirical_cdf(observed_difference)
  ```
    
  This shows that we would only expect, *given the skeptic's hypothesis*, to see data at least this extreme by chance in 0.04% of experiments. If we don't think that our experiment is likely to be an outlier, this suggests that the skeptic's hypothesis is quite unlikely. However, this does not mean our mechanistic theory for the group difference is correct: this would require more work and maybe a more targeted experiment!

### Problem 2 

**In this problem**:

* Derive and code a simulation model for the above interview procedure given the "true" probability of cheating $p$.
* Simulate your model (for a class of 100 students) 50,000 times under the your hypothesis and the TA's hypothesis, and plot the two resulting datasets.
* If you received 31 "Yes, I cheated" responses while interviewing your class, what could you conclude? Feel free to use any qualitative or quantiative assessments to justify your conclusions.
* How useful do you think the interview procedure is to identify systemic teaching? What changes to the design might you make?

***Solution***:

Let's start by sketching out the simulation model. For every student, we first need to simulate the outcome of the first coin flip, which has a 50% probability of heads. If this coin flip comes up as heads, then the student answers honestly, and admits to cheating with probability $p$. If the coin flip comes up tails, the student flips another coin and answers that they cheated with probability 50%. After looping over this procedure for each student in the class, we add up the "true" values.

We might code this model as follows.

```{julia}
# cheating_model: function which simulates the outcome of the interview procedure described in this problem and returns the number of confessions obtained.
# inputs:
#   p: base probability of cheating under a given hypothesis
#   n: vector of bite counts for the water-drinking group
# output:
#   a simulated number of confessions for one realization of the process.
function cheating_model(p, n)
    # initialize the storage vector for whether students admit to cheating
    # we do this with a boolean vector, which is a little faster, but storing integers is basically the same thing
    cheat = zeros(Bool, n)
    # loop over every student to simulate the interview process
    for i in 1:n
        # initial coin flip
        # rand() simulates a uniform random number between 0 and 1
        if rand() >= 0.5
            # if this came up heads, simulate whether the student cheated based on the cheating probability
            if rand() < p
                cheat[i] = true
            else
                cheat[i] = false
            end
        else
            # otherwise, simulate another coin flip
            if rand() >= 0.5
                cheat[i] = true
            else
                cheat[i] = false
            end
        end
    end
    # return the total number of cheating admissions
    return sum(cheat)
end
```

Now we simulate under our assumption of low cheating and the TA's assumption of widespread cheating and plot the results.

```{julia}
#| label: fig-cheating
#| fig-cap: "Histograms of the number of simulated confessions obtained under the hypothesis of a 5% cheating rate (blue) and a 30% cheating rate (red). The green line is the observed number of cheating confessions."
#| fig-align: center
#| fig-alt: "Histogram of simulated confessions."

# conduct the simulations
low_cheat_data = [cheating_model(0.05, 100) for i in 1:50_000]
high_cheat_data = [cheating_model(0.30, 100) for i in 1:50_000]

# plot the histograms with axis labels and a vertical line for the "real" outcome of the procedure
histogram(low_cheat_data, color=:blue, label="Cheating Rate 5%", alpha=0.4)
histogram!(high_cheat_data, color=:red, label="Cheating Rate 30%", alpha=0.4)
xlabel!("Number of Students who Confess to Cheating")
ylabel!("Count")
vline!([31], linestyle=:dash, color=:green, linewidth=3, label="Observed Outcome")
```

From @fig-cheating, we can see:

* The distributions look roughly normal (not surprising given the Central Limit Theorem!). 
* There is some overlap around the 25-42 confession count rate. Lower than 25 students confessing would strongly suggest that the TA is overstating the rate of cheating, while more than 40 would strongly suggest that we are underestimating the cheating rate. 
* Note that neither of these is "confirmation" of either theory, but evidence about the relative proportion of cheating being more or less consistent with one of our hypotheses.
* The outcome of 31 confessions is in the range where the evidence isn't completely clear either way. However, it appears more likely that this outcome is explained by a lower rate of cheating than a higher rate. For example, under our hypothesis, the "high-tail" p-value of the outcome is:

  ```{julia}
  ecdf_low = ecdf(low_cheat_data)
  1 - ecdf_low(31)
  ```

  while under the TA's hypothesis, the "low-tail" p-value is

  ```{julia}
  ecdf_high = ecdf(high_cheat_data)
  ecdf_high(31)
  ```

  So there is a `{julia} Int(round(100 * (1 - ecdf_low(31)), digits=0))`% probability of seeing 31 or more confessions under our assumed 5% cheating rate, while there is only a `{julia} Int(round(100 * ecdf_high(31), digits=0))`% probability of seeing 31 or fewer confessions under the TA's assumed 30% cheating rate, which seems to suggest a lower rate is more likely. 
* We can't draw any strong conclusions from this, but we will talk more later this semester about how to quantify consistency of models with data based on simulations!
* This interview process is noisy, but appears to work to separate very large differences in cheating rates. On the other hand, it might not work so well if we cared about the difference between 5% cheating and 10% cheating rates, as the difference in the "true" confessions would be swamped by the coin flips. If we wanted to tease out those differences, we could use a weighted coin (to increase the number of "honest" confessions).

Later in the semester, we will look at methods for how we might quantify what base cheating rates are consistent with a given outcome (though we won't apply them to this example!).

## Problem 3

**In this problem**: 

* Write a function `galton_sim` which simulates `n` Galton board trials (assume the board has 8 board rows, as in the image above) and returns a vector with the number of balls which fall into each bin. You can assume (for now) that the board is fair, *e.g.* that the probability of a left or right bounce is 0.5; you may want to make this probability a function parameter so you can change it later.
* Run your simulation for a sample of 50 balls. Create a histogram of the results, with each bar corresponding to one bin. Make sure you use a random seed for reproducibility, and label your axes!
* Each Galton board trial can be represented as a realization from a [binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution). But as we noted above, by the Central Limit Theorem, the distribution of a  large enough number of trials should be approximately normal. Use a [quantile-quantile (Q-Q) plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot) to compare a fitted normal distribution with your simulation results. How well does a normal distribution fit the data?
* Repeat your simulation experiment with 250 trials and compare to a normal distribution. Does it describe the empirical distribution better?
* If the probability of a left bounce is 70%, what does this do to the fit of a normal distribution? What other distribution might you use if not a normal and why?

***Solution***:

First, we write the `galton_sim` function.

```{julia}
#| echo: true
#| output: false
#| code-overflow: wrap

# p is the probability of a left bounce
function galton_sim(n_balls, n_rows; p=0.5)
    n_bins = n_rows + 1 # number of bins balls can fall into
    # this subfunction simulates the trajectory of a single ball; this is not entirely necessary but cleans things up a bit
    function simulate_ball(n_rows, p)
        bin = 0 # start in the center
        # for every row, see if the ball bounces to the left (0) or to the right (+1)
        # the accounting here is a little different than might be intuitive: it's easier to calculate the bin if we start from 0 (all left bounces) and just count the number of right increments. We could also have set this up to start in the middle and bounce to the left/right with -/+ 1.
        for row in 1:n_rows
            bin = bin + sample([0, 1], ProbabilityWeights([p, 1-p]))  # prob
        end
        return bin
    end
    # simulate outcome for every ball
    # the use of a comprehension lets us avoid pre-allocation and looping
    bins = [simulate_ball(n_rows, p) for _ in 1:n_balls] # the _ just avoids introducing a new counter variable we don't use

    return bins # return simulated outcomes
end
```

Next, we run our simulation for 50 balls and create a histogram.

```{julia}
#| echo: true
#| output: true
#| fig-cap: 50 Ball Galton Board Histogram

Random.seed!(1)

galton_simulation = galton_sim(50, 8)
histogram(galton_simulation, legend=:false) # plot histogram
xlabel!("Galton Board Bin")
ylabel!("Ball Count")
```

How well does a normal distribution fit this data? We use `StatsPlots.qqplot()` to check.

```{julia}
#| echo: true
#| output: true
#| label: fig-galton-qq-1
#| fig-cap: 50 Ball Galton Board QQ Plot

qqplot(Normal, galton_simulation)
xlabel!("Theoretical Normal Quantiles")
ylabel!("Empirical Quantiles")
```

@fig-galton-qq-1 might be slightly tricky to interpret as the data is not continuous. We can see how the repeated samples form horizontal lines across the 1-1 line. But in general, the fit is pretty good; the values seem to follow those predicted by a normal once you account for the discretization.

Let's repeat this with 250 samples:

```{julia}
#| echo: true
#| output: true
#| fig-cap: 250 Ball Galton Board Histogram

galton_simulation = galton_sim(250, 8)
histogram(galton_simulation, legend=:false) # plot histogram
xlabel!("Galton Board Bin")
ylabel!("Ball Count")
```

```{julia}
#| echo: true
#| output: true
#| label: fig-galton-qq-2
#| fig-cap: 250 Ball Galton Board QQ Plot

qqplot(Normal, galton_simulation)
xlabel!("Theoretical Normal Quantiles")
ylabel!("Empirical Quantiles")
```

@fig-galton-qq-2 is similar to the last 50-ball simulation, which is not entirely surprising given that the Central Limit Theorem should only work better with larger samples.

Finally, let's test with a 70% probability of a left bounce.

```{julia}
#| echo: true
#| output: true
#| label: fig-galton-hist-skewed

galton_simulation = galton_sim(250, 8, p=0.7)
histogram(galton_simulation, legend=:false) # plot histogram
xlabel!("Galton Board Bin")
ylabel!("Ball Count")
```

```{julia}
#| echo: true
#| output: true
#| label: fig-galton-qq-3

qqplot(Normal, galton_simulation)
xlabel!("Theoretical Normal Quantiles")
ylabel!("Empirical Quantiles")
```

We can see a bit more of a skew in @fig-galton-hist-skewed, though @fig-galton-qq-3 still looks reasonable. We could think about using a SkewNormal distribution to capture that skew.

## Problem 4

**In this problem**:

* Write down a model which encodes the Showcase rules as a function of the showcase value and your bid. You can assume that your wagering is independent of your opponent.
* Select and fit a distribution to the above statistics (you have some freedom to pick a distribution, but make sure you justify it).
* Using 1,000 samples from your price distribution in your model, plot the expected winnings for bids from $20,000 through $72,000. 
* Find the bid which maximizes your expected winnings. If you were playing *The Price Is Right*, is this the strategy you would adopt, or are there other considerations you would take into account which were not included in this model?

***Solution***:

Here's a function for the outcome of a bid:

```{julia}
function showcase_model(value, bid)
    if bid > value
        winnings = 0
    elseif value - bid < 250
        winnings = 2 * value
    else
        winnings = value
    end
    return winnings
end
```

We'll use a truncated normal distribution to reflect an assumption that moderate showcase values (near the median) are more typical than the extremes (this is an assumption, not part of the problem; other choices are fine!).

```{julia}
#| echo: true
#| output: true

showcase_dist = truncated(Normal(33_048, 5_000); lower=20_432, upper=72_409)
histogram(rand(showcase_dist, 100_000))
```

Now let's write a function to calculate the expected winnings for a given bid, which we will then evaluate through the bidding range.

```{julia}
#| echo: true
#| output: true
#| label: fig-expected-winnings

function showcase_winnings(bid, value_dist)
    # sample 100,000 values of showcases
    showcase_values = rand(value_dist, 100_000)
    winnings = [showcase_model(val, bid) for val in showcase_values]
    return mean(winnings)
end

bids = 20_000:1_000:72_000
exp_winnings = [showcase_winnings(bid, showcase_dist) for bid in bids]
plot(bids, exp_winnings)
```

```{julia}
#| echo: true
#| output: true

bids[argmax(exp_winnings)]
```

So with this assumed distribution, the lowest bid maximizes the expected winnings because there is a large skew to the showcase values, meaning that higher bids pose a correspondingly higher probability of overbidding. This would likely not translate well to the actual show given that the lowest bid also increases the probability of being outbid by the opponent. We also might have actual clues from the showcase itself about the relative value within the distribution, rather than assuming that it's random.