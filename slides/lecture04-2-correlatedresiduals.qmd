---
title: "Correlated Discrepancies"
subtitle: "Lecture 07"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "February 12, 2025"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: mathjax
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        mermaid: 
            theme: dark
engine: julia
filters:
  - code-fullscreen
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using DataFrames
using DataFramesMeta
using CSV
using Distributions
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using Optim
using LaTeXStrings
using Measures
using PrettyTables

Random.seed!(1)

plot_font = "Palatino Roman"
default(
    fontfamily=plot_font,
    linewidth=3, 
    framestyle=:box, 
    label=nothing, 
    grid=false,
    guidefontsize=18,
    legendfontsize=16,
    tickfontsize=16,
    titlefontsize=20,
    bottom_margin=10mm,
    left_margin=5mm
)
```

```{julia}
#| echo: false
#| output: false

# Dataset from https://zenodo.org/record/3973015
# The CSV is read into a DataFrame object, and we specify that it is comma delimited
dat_path = joinpath(@__DIR__, "data", "climate")
forcings_all_85 = CSV.read(joinpath(dat_path, "ERF_ssp585_1750-2500.csv"), DataFrame, delim=",")

# Separate out the individual components
forcing_co2_85 = forcings_all_85[!,"co2"]
# Get total aerosol forcings
forcing_aerosol_rad_85 = forcings_all_85[!,"aerosol-radiation_interactions"]
forcing_aerosol_cloud_85 = forcings_all_85[!,"aerosol-cloud_interactions"]
forcing_aerosol_85 = forcing_aerosol_rad_85 + forcing_aerosol_cloud_85
forcing_total_85 = forcings_all_85[!,"total"]
forcing_non_aerosol_85 = forcing_total_85 - forcing_aerosol_85
forcing_other_85 = forcing_total_85 - (forcing_co2_85 + forcing_aerosol_85)

forcings_all_26 = CSV.read(joinpath(dat_path, "ERF_ssp126_1750-2500.csv"), DataFrame, delim=",")

# Separate out the individual components
forcing_co2_26 = forcings_all_26[!,"co2"]
# Get total aerosol forcings
forcing_aerosol_rad_26 = forcings_all_26[!,"aerosol-radiation_interactions"]
forcing_aerosol_cloud_26 = forcings_all_26[!,"aerosol-cloud_interactions"]
forcing_aerosol_26 = forcing_aerosol_rad_26 + forcing_aerosol_cloud_26
forcing_total_26 = forcings_all_26[!,"total"]
forcing_non_aerosol_26 = forcing_total_26 - forcing_aerosol_26
forcing_other_26 = forcing_total_26 - (forcing_co2_26 + forcing_aerosol_26)

t = time_forcing = Int64.(forcings_all_85[!,"year"]) # Ensure that years are interpreted as integers
sim_years = 1850:2100 # model years for projections
sim_idx = indexin(sim_years, t)

temps = CSV.read(joinpath(dat_path, "HadCRUT.5.0.1.0.analysis.summary_series.global.annual.csv"), DataFrame, delim=",")

time_obs = temps[:, 1]
temp_obs = temps[:, 2]
temp_lo = temps[:, 3]
temp_hi = temps[:, 4]

# generate simulations
hind_years = 1850:2022 # model years to simulate for fitting
sim_years = 1850:2100 # model years for projections
hind_idx = indexin(hind_years, t) # find indices in t vector of simulation years
sim_idx = indexin(sim_years, t)

temp_obs = temp_obs[indexin(hind_years, time_obs)] # filter to simulated years for plotting
temp_lo = temp_lo[indexin(hind_years, time_obs)] # filter to simulated years for plotting
temp_hi = temp_hi[indexin(hind_years, time_obs)] # filter to simulated years for plotting

temp_lo = temp_lo .- mean(temp_obs[1:20]) # re-normalize to be consistent with the model
temp_hi = temp_hi .- mean(temp_obs[1:20]) # re-normalize to be consistent with the model
temp_obs = temp_obs .- mean(temp_obs[1:20]) # re-normalize to be consistent with the model
temp_sd = (temp_hi - temp_lo) / 2 # estimate standard deviation using 95% CI
```

# Review


## Planetary Energy Balance

![Representation of Planetary Energy Balance](https://www.e-education.psu.edu/meteo469/sites/www.e-education.psu.edu.meteo469/files/lesson04/ebm_0_dim.gif)

::: {.caption}
Source: Reprinted from A Climate Modeling Primer, A. Henderson-Sellers and K. McGuffie, Wiley, pg. 58, (1987) via <https://www.e-education.psu.edu/meteo469/node/137>.
:::

## The Energy Balance Model (EBM)

:::: {.columns}
::: {.column width=60%}
\begin{align*}
\overbrace{\frac{dH}{dt}}^{\text{change in heat}} &= \overbrace{F}^{\text{RF}} - \overbrace{\lambda T}^{\substack{\text{change in} \\ \text{temperature}}} \\[1em]
\Rightarrow C\frac{dT}{dt} &= F - \lambda T - \gamma(T-T_D)\\
C_D\frac{dT_D}{dt} &= \gamma(T-T_D)
\end{align*}
:::
::: {.column width=40%}
![Two Layer EBM Schematic](https://www.researchgate.net/publication/326356068/figure/fig1/AS:1132452110184458@1647009031736/A-physically-based-emulator-the-two-layer-energy-balance-model-The-model-consists-of-an.jpg)

::: {.caption}
Source: @Palmer2018-ha
:::
:::
::::

## EBM Discretization

Use Euler discretization:

\begin{align*}
T(t+1) &= T(t) + \frac{F(t) - \lambda T(t) - \gamma(T(t) - T_D(d))}{C} \Delta t \\[0.5em]
T_D(t+1) &= T_D(t) + \frac{\gamma (T(t) - T_D(t))}{C_D} \Delta t
\end{align*}

## Equilibrium Climate Sensitivity (ECS)

Under steady-state conditions (constant $F$ and $dT/dt = 0$), $$T = \frac{F}{\lambda}.$$

When we double atmospheric CO~2~, we refer to the equilibrium temperature $S$ as the **equilibrium climate sensitivity**:

$$S = \underbrace{F_{2\times \text{CO}_2}}_{\approx 4 \text{W/m}^2}/\lambda$$

## Probability Models for Simulation Models

Most common setting (*e.g.* @Brynjarsdottir2014-ve):

$$\mathbf{y} = \underbrace{\eta(\mathbf{x}; \theta)}_{\text{model}} + \underbrace{\delta(x)}_{\text{discrepancy}} + \underbrace{\varepsilon}_{\text{error}}$$


## Model-Data Discrepancy

For example, $\delta$ might capture:

- Bias (e.g.: model consistently over/underpredicts);
- Accumulations of error (e.g.: persistent model underestimates);
- Partial observations (e.g.: do you count every animal?)


# Correlated Discrepancies

## MLE for Gaussian Discrepancy

:::: {.columns} 
::: {.column width=50%}
```{julia}
#| echo: false
#| output: asis

function ebm(rf_nonaerosol, rf_aerosol; p=(3.2, 1.0, 1.3, 100.0, 800.0, -0.1))
    # set up model parameters
    S, γ, α, d, D, T₀ = p # this unpacks the parameter tuple into variables
    F2xCO₂ = 4.0 # radiative forcing [W/m²] for a doubling of CO₂
    λ = F2xCO₂ / S

    c = 4.184e6 # heat capacity/area [J/K/m²]
    C = c*d # heat capacity of mixed layer (per area)
    CD = c*D # heat capacity of deep layer (per area)
    F = rf_nonaerosol + α*rf_aerosol # radiative forcing
    Δt = 31558152. # annual timestep [s]

    T = zero(F)
    T[1] = T₀
    TD = zero(F)
    for i in 1:length(F)-1
        T[i+1] = T[i] + (F[i] - λ*T[i] - γ*(T[i]-TD[i]))/C * Δt
        TD[i+1] = TD[i] + γ*(T[i]-TD[i])/CD * Δt
    end
    # return after normalizing to reference period
    return T
end

ebm_wrap(params) = ebm(forcing_non_aerosol_85[hind_idx], forcing_aerosol_85[hind_idx], p = params)

function gaussian_iid_homosked(params, temp_dat, temp_err, m)
    S, γ, α, d, D, T₀, σ = params 
    ebm_sim = m((S, γ, α, d, D, T₀))
    ll = sum(logpdf.(Normal.(ebm_sim, sqrt.(σ^2 .+ temp_err.^2)), temp_dat))
    return ll
end

lower = [1.0, 0.5, 0.0, 50.0, 200.0, temp_lo[1], 0.0]
upper = [5.0, 1.5, 2.0, 200.0, 1000.0, temp_hi[1], 10.0]
p0 = [3.0, 1.0, 1.0, 100.0, 800.0,temp_obs[1], 5.0]

result = Optim.optimize(params -> -gaussian_iid_homosked(params, temp_obs, temp_sd, ebm_wrap), lower, upper, p0)
θ_iid = result.minimizer
θ_iid_rd = round.(θ_iid; digits=1)
parnames = ["S", "γ", "α", "d", "D", "T₀", "σ"]
pretty_table(DataFrame(Parameters=parnames, MLE=θ_iid_rd); backend=Val(:markdown), show_subheader=false, show_row_number=false)
```
:::
::: {.column width=50%}
```{julia}
#| label: fig-gaussian-mle
#| fig-cap: MLE Fit for Gaussian discrepancy
#| echo: true
#| code-fold: true

n_samples = 10_000
temp_iid = ebm_wrap(θ_iid) # simulate IID best fit
# simulate projections with discrepancy and errors
temp_iid_proj = zeros(n_samples, length(temp_sd))
for i = 1:length(temp_sd)
    temp_iid_err = rand(Normal(0, sqrt.(θ_iid[end]^2 .+ temp_sd[i]^2)), n_samples)
    temp_iid_proj[:, i] = temp_iid[i] .+ temp_iid_err
end
# calculate quantiles
temp_iid_q = mapslices(col -> quantile(col, [0.05, 0.5, 0.95]), temp_iid_proj; dims=1)

p = scatter(time_obs, temp_obs, color=:black, label="Observations", ylabel="(°C)", xlabel="Year", title="Temperature Anomaly")
plot!(p, time_obs, temp_iid_q[2, :], ribbon=(temp_iid_q[2, :] - temp_iid_q[1, :], temp_iid_q[3, :] - temp_iid_q[2, :]), color=:red, fillalpha=0.3, label="Gaussian Discrepancy")
plot!(size=(600, 500))
```
:::
::::

## Analyzing Residual Assumptions

::: {.fragment .fade-in}
```{julia}
#| label: fig-gaussian-residuals-diagnostics
#| fig-cap: Residual diagnostics
#| echo: true
#| code-fold: true
#| layout-ncol: 2
#| output: true

resids_homogauss = temp_obs - temp_iid

p1 = qqnorm(resids_homogauss, xlabel="Theoretical Values", ylabel="Empirical Values", title="Normal Q-Q Plot", size=(600, 500))
pacf_homogauss = pacf(resids_homogauss, 1:5)
p2 = plot(1:5, pacf_homogauss, marker=:circle, line=:stem, linewidth=3, markersize=8, tickfontsize=16, guidefontsize=18, legend=false, ylabel="Partial Autocorrelation", xlabel="Time Lag", title="Partial Autocorrelation Plot", size=(600, 500))
hline!(p2, [0], color=:black, linestyle=:dash)

display(p1)
display(p2)
```
:::

## AR(1) Residuals

:::: {.columns}
::: {.column width=50%}
### Autocorrelated

$$
\begin{align*}
y_t &= \text{EBM}(\theta; F_t) + \delta_t + \varepsilon_t \\
\delta_t &= \rho \delta_{t-1} + \omega^2 \\
\varepsilon_t &\sim N(0, \sigma^2_{\text{obs}, t}) \\
\omega_t &\sim N(0, \sigma^2)
\end{align*}
$$
:::
::: {.column width=50%}
### Independent

$$
\begin{align*}
y_t &= \text{EBM}(\theta; F_t) + \delta_t + \varepsilon_t \\
\delta_t &\sim N(0, \sigma^2) \\
\varepsilon_t &\sim N(0, \sigma^2_{\text{obs}, t})
\end{align*}
$$

:::
::::

## Likelihood Function

Without observation errors ($\varepsilon$), can whiten residuals:

$$
\begin{align*}
y_1 & \sim N\left(0, \frac{\sigma^2}{1 - \rho^2}\right) \\
y_t - \rho y_{t-1}  &\sim N(0, \sigma^2) 
\end{align*}
$$

When observation errors are given in the data (as in here), this also works.

## Non-Identifiability of Parameters

But when observation errors are also uncertain, run into non-identifiability:

$$N(0, \sigma^2) + N(0, \sigma_\text{obs}^2) = N(0, {\color{red}\sigma^2 + \sigma_\text{obs}^2})$$

## Joint Likelihood for AR(1)

Could also use joint likelihood for residuals $y_t - \text{EBM}(\theta; F_t)$:

$$
\begin{align*}
\mathbf{y} &\sim \mathcal{N}(\mathbf{0}, \Sigma) \\
\Sigma &= \frac{\sigma^2}{1 - \rho^2} \begin{pmatrix}1 + \sigma_{\text{obs}, 1}^2 & \rho & \ldots & \rho^{T-1}  \\ \rho & 1 + \sigma_{\text{obs}, 2}^2 & \ldots & \rho^{T-2} \\ \vdots & \vdots & \ddots & \vdots \\ \rho^{T-1} & \rho^{T-2} & \ldots & 1+ \sigma_{\text{obs}, T}^2\end{pmatrix}
\end{align*}
$$

```{julia}
#| output: false
#| echo: false

function ar_loglik(params, temp_obs, temp_err, m)
    S, γ, α, d, D, T₀, ρ, σ = params 
    ebm_sim = m((S, γ, α, d, D, T₀))
    residuals = temp_obs - ebm_sim
    # whiten residuals
    ll = 0
    for t = 1:length(temp_sd)
        if t == 1
            ll += logpdf(Normal(0, sqrt(σ^2 / (1 - ρ^2) + temp_err[1]^2)), residuals[1])
        else
            resid_wn = residuals[t] - ρ * residuals[t-1]
            ll += logpdf(Normal(0, sqrt(σ^2 + temp_err[t]^2)), resid_wn)
        end
    end
    return ll
end
```

## AR(1) MLE

```{julia}
#| output: asis

lower = [1.0, 0.5, 0.0, 50.0, 200.0, temp_lo[1], -1.0, 0.0]
upper = [5.0, 1.5, 2.0, 200.0, 1000.0, temp_hi[1], 1.0, 10.0]
p0 = [3.0, 1.0, 1.0, 100.0, 800.0,temp_obs[1], 0.0, 5.0]

result = Optim.optimize(params -> -ar_loglik(params, temp_obs, temp_sd, ebm_wrap), lower, upper, p0)
θ_ar = result.minimizer
θ_ar_rd = round.(θ_ar; digits=1)

θ_iid_exp = Vector{Any}(undef, length(θ_ar_rd))
θ_iid_exp[1:end-2] = θ_iid_rd[1:end-1]
θ_iid_exp[end-1] = "-"
θ_iid_exp[end] = θ_iid_rd[end]

parnames = ["S", "γ", "α", "d", "D", "T₀", "ρ", "σ"]
pretty_table(DataFrame(Parameters=parnames, IID=θ_iid_exp, AR=θ_ar_rd); backend=Val(:markdown), show_subheader=false, show_row_number=false)
```

## AR(1) Hindcast

:::: {.columns}
::: {.column width=60%}
```{julia}
#| label: fig-ebm-fits
#| fig-cap: Hindcast of the EBM with IID and AR(1) residuals.
#| echo: true
#| code-fold: true

n = 10_000

# get model hindcasts
temp_iid = ebm_wrap(θ_iid[1:end-1])
temp_ar = ebm_wrap(θ_ar[1:end-2])

# get iid and AR residuals from relevant processes
residuals_iid = stack(rand.(Normal.(0, sqrt.(temp_sd.^2 .+ θ_iid[end].^2)), n), dims=1)
residuals_ar = zeros(length(hind_idx), n)
for t = 1:length(temp_sd)
    if t == 1
        residuals_ar[t, :] = rand(Normal(0, sqrt(θ_ar[end]^2 / (1 - θ_ar[end-1]^2) + temp_sd[1]^2)), n)
    else
        residuals_ar[t, :] = θ_ar[end-1] * residuals_ar[t-1, :] + rand(Normal(0, sqrt(θ_ar[end]^2 + temp_sd[t]^2)), n)
    end
end

# add residuals back to model simulations
model_sim_iid = (residuals_iid .+ temp_iid)'
model_sim_ar = (residuals_ar .+ temp_ar)'

# get quantiles
q90_iid = mapslices(col -> quantile(col, [0.05, 0.5, 0.95]), model_sim_iid; dims=1) # compute 90% prediction interval
q90_ar = mapslices(col -> quantile(col, [0.05, 0.5, 0.95]), model_sim_ar; dims=1) # compute 90% prediction interval

p = scatter(time_obs, temp_obs, yerr=(temp_obs - temp_lo, temp_hi - temp_obs), color=:black, label="Observations", ylabel="(°C)", xlabel="Year", title="Temperature Anomaly", markersize=5)
plot!(p, hind_years, q90_iid[2, :], ribbon=(q90_iid[2, :] - q90_iid[1, :], q90_iid[3, :] - q90_iid[2, :]), fillalpha=0.2, linewidth=3, label="IID")
plot!(p, hind_years, q90_ar[2, :], ribbon=(q90_ar[2, :] - q90_ar[1, :], q90_ar[3, :] - q90_ar[2, :]), fillalpha=0.2, linewidth=3, label="AR")
plot!(size=(700, 550))
```
:::
::: {.column width=40%}
Coverage Rates:

```{julia}
#| echo: false
#| output: false

function cover_rate(q, dat)
   dat_out = sum((q[1, :] .> dat) .|| (q[3, :] .< dat))
   cr = (length(dat) - dat_out) / length(dat) * 100
   return round(cr; digits=1)
end
```

- IID: `{julia} cover_rate(q90_iid, temp_obs)`%
- AR(1): `{julia} cover_rate(q90_ar, temp_obs)`%

:::
::::

## Have We Captured Residual Autocorrelation?

:::: {.columns}
::: {.column width=50%}
Use simulations to look at distributions of residual probability assumptions.
:::
::: {.column width=50%}
```{julia}
#| label: fig-autocor-check
#| fig-cap: Distribution of residual partial autocorrelations
#| echo: true
#| code-fold: true

resids_ar_sim = model_sim_ar .- temp_obs'
boxplot(mapslices(col -> pacf(col, 1:5), resids_ar_sim; dims=1)', label=:false, xlabel="Lag", ylabel="Partial Autocorrelation")
plot!(size=(500, 500))
```
:::
::::

## Hindcasts May Not Differ Much...

```{julia}
#| label: fig-ebm-hindcast
#| fig-cap: Hindcast of the EBM with IID and AR(1) residuals.
plot!(p, size=(1200, 600))
```

## ...But Projections Can

```{julia}
#| label: fig-ebm-rcp-proj
#| fig-cap: Differences in the 90% confidence intervals
#| echo: true
#| code-fold: true

ebm_sim_85(params) = ebm(forcing_non_aerosol_85[sim_idx], forcing_aerosol_85[sim_idx], p = params)
ebm_sim_26(params) = ebm(forcing_non_aerosol_26[sim_idx], forcing_aerosol_26[sim_idx], p = params)

# iid residuals
y_err = zeros(length(sim_idx))
y_err[1:length(hind_idx)] = temp_sd

residuals_iid = stack(rand.(Normal.(0, sqrt.(y_err.^2 .+ θ_iid[end]^2)), n), dims=1)
model_iid_85 = ebm_sim_85(θ_iid[1:end-1])
model_iid_26 = ebm_sim_26(θ_iid[1:end-1])
model_sim_iid_85 = (residuals_iid .+ model_iid_85)'
model_sim_iid_26 = (residuals_iid .+ model_iid_26)'
q90_iid_85 = mapslices(col -> quantile(col, [0.05, 0.5, 0.95]), model_sim_iid_85; dims=1) # compute 90% prediction interval```
q90_iid_26 = mapslices(col -> quantile(col, [0.05, 0.5, 0.95]), model_sim_iid_26; dims=1) # compute 90% prediction interval```

# AR residuals
residuals_ar = zeros(length(sim_idx), n)
for t = 1:length(sim_idx)
    if t == 1
        residuals_ar[t, :] = rand(Normal(0, sqrt(θ_ar[end]^2 / (1 - θ_ar[end-1]^2 + temp_sd[1]^2))), n)
    elseif t <= length(hind_idx)
        residuals_ar[t, :] = θ_ar[end-1] * residuals_ar[t-1, :] + rand(Normal(0, sqrt(θ_ar[end]^2 + temp_sd[t]^2)), n)
    else
        residuals_ar[t, :] = θ_ar[end-1] * residuals_ar[t-1, :] + rand(Normal(0, θ_ar[end]), n)
    end
end
model_ar_85 = ebm_sim_85(θ_ar[1:end-2])
model_ar_26 = ebm_sim_26(θ_ar[1:end-2])
model_sim_ar_26 = (residuals_ar .+ model_ar_26)'
model_sim_ar_85 = (residuals_ar .+ model_ar_85)'
q90_ar_26 = mapslices(col -> quantile(col, [0.05, 0.5, 0.95]), model_sim_ar_26; dims=1) # compute 90% prediction interval
q90_ar_85 = mapslices(col -> quantile(col, [0.05, 0.5, 0.95]), model_sim_ar_85; dims=1) # compute 90% prediction interval

p_sim= scatter(time_obs, temp_obs, color=:black, label="Data", ylabel="Temperature Anomaly (°C)", xlabel="Year")
plot!(p_sim, sim_years, q90_iid_26[2, :], ribbon=(q90_iid_26[2, :] - q90_iid_26[1, :], q90_iid_26[3, :] - q90_iid_26[2, :]), fillalpha=0.2, linewidth=3, color=:royalblue, label="IID/SSP1-2.6")
plot!(p_sim, sim_years, q90_ar_26[2, :], ribbon=(q90_ar_26[2, :] - q90_ar_26[1, :], q90_ar_26[3, :] - q90_ar_26[2, :]), fillalpha=0.2, linewidth=3, color=:firebrick1, label="AR/SSP1-2.6")
plot!(p_sim, sim_years, q90_iid_85[2, :], ribbon=(q90_iid_85[2, :] - q90_iid_85[1, :], q90_iid_85[3, :] - q90_iid_85[2, :]), fillalpha=0.2, linewidth=3, color=:blue3, label="IID/SSP5-8.5")
plot!(p_sim, sim_years, q90_ar_85[2, :], ribbon=(q90_ar_85[2, :] - q90_ar_85[1, :], q90_ar_85[3, :] - q90_ar_85[2, :]), fillalpha=0.2, linewidth=3, color=:firebrick, label="AR/SSP5-8.5")
plot!(p_sim, size=(1100, 550))
xlims!((2000, 2100))
ylims!((0.75, 5.5))
```

## Projections Can Change...

```{julia}
#| label: fig-2100-histogram
#| fig-cap: Projections of global mean temperature in 2100
#| layout-ncol: 2
#| echo: true
#| code-fold: true

p1 = histogram(model_sim_iid_26[:, end], color=:blue, xlabel="°C", ylabel="Count", label="IID", size=(600, 550), alpha=0.4)
histogram!(p1, model_sim_ar_26[:, end], color=:red, label="AR", alpha=0.4)
p2 = histogram(model_sim_iid_85[:, end], color=:blue, xlabel="°C", ylabel="Count", label="IID", size=(600, 550), alpha=0.4)
histogram!(p2, model_sim_ar_85[:, end], color=:red, label="AR", alpha=0.4)

display(p1)
display(p2)
```

# More General Models

## Lynx and Hare Pelts

```{julia}
#| label: fig-lynxhare-data
#| fig-cap: Lynx and Hare pelt dataset
#| echo: true
#| code-fold: true


lh_obs = DataFrame(CSV.File("data/ecology/Lynx_Hare.csv"))
plot(lh_obs[!, :Year], lh_obs[!, :Lynx], xlabel="Year", ylabel="Pelts (thousands)", markersize=5, markershape=:circle, markercolor=:red, color=:red, linewidth=3, label="Lynx")
plot!(lh_obs[!, :Year], lh_obs[!, :Hare], markersize=5, markershape=:circle, markercolor=:blue, color=:blue, linewidth=3, label="Hare")
plot!(size=(1100, 500))
```

## Predator-Prey Dynamics

$$
\begin{align*}
\frac{dH}{dt} &= H_t \underbrace{b_H}_{\substack{\text{birth} \\ \text{rate}}} - H_t (\underbrace{L_t m_H}_{\substack{\text{impact of} \\ \text{lynxes}}}) \\
\frac{dL}{dt} &= L_t (H_t b_L) - L_t m_L
\end{align*}
$$

## Predator-Prey Probability Model

$$\underbrace{h_t}_{\substack{\text{hare} \\ \text{pelts}}} \sim \text{LogNormal}(\log(\underbrace{p_H}_{\substack{\text{obs} \\ \text{rate}}} H_T), \sigma_H)$$
$$l_t \sim \text{LogNormal}(\log(p_L L_T), \sigma_L)$$

:::: {.columns}
::: {.column width=50%}

$$
\begin{align*}
\frac{dH}{dt} &= H_t b_H - H_t (L_t m_H) \\
H_T &= H_1 + \int_1^T \frac{dH}{dt}dt
\end{align*}
$$
:::
::: {.column width=50%}

$$
\begin{align*}
\frac{dL}{dt} &= L_t (H_t b_L) - L_t m_L \\
L_T &= L_1 + \int_1^T \frac{dL}{dt}dt
\end{align*}
$$

:::
::::

# Key Points

## Key Points

- Think generatively about probability models for calibrating models:
- **Discrepancy**: corrects for mismatches between model output and "state" of system
- **Observation errors**: Probability distribution for observations given discrepancy adjustment
- Choice of probability model (including discrepancy) can impact projections even if hindcast ("validation") does not appear very different.


# Discussion of Shmueli (2010)

## Questions To Seed Discussion

- What do you think are the differences between predictive and explanatory modeling?
- What can go wrong when we conflate the two?
- Can you think of approaches or workflows which bridge the two paradigms?

# Upcoming Schedule

## Next Classes

**Monday**: Feb Break!

**Wednesday**: Bayesian Statistics

## Assessments

**Homework 2** available; due *next* Friday (2/21).

**No quiz or reading this week!**

# References

## References (Scroll for Full List)
