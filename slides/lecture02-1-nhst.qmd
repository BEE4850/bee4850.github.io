---
title: "Hypothesis Testing and Decision-Making"
subtitle: "Lecture 02"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "January 27, 2024"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: mathjax
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        mermaid: 
            theme: dark
engine: julia
filters:
  - code-fullscreen
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using DataFrames
using DataFramesMeta
using CSV
using Dates
using Distributions
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using GLM
using LaTeXStrings
using Measures

Random.seed!(1)

plot_font = "Computer Modern"
default(
    fontfamily=plot_font,
    linewidth=3, 
    framestyle=:box, 
    label=nothing, 
    grid=false,
    guidefontsize=18,
    legendfontsize=16,
    tickfontsize=16,
    titlefontsize=20,
    bottom_margin=10mm,
    left_margin=5mm
)
```

# Statistics and Decision-Making

## Science as Decision-Making Under Uncertainty

:::: {.columns}
::: {.column width=60%}
Goal is to draw insights:

- About causes and effects;
- About interventions.

But our models are simplifications and our observations are uncertain!
:::

::: {.column width=40%}
![XKCD 2440](https://imgs.xkcd.com/comics/epistemic_uncertainty_2x.png){width=90%}

::: {.caption}
Source: [XKCD 2440](https://xkcd.com/2440)
:::
:::
::::

::: {.notes}
These decisions are complicated by model simplifications and observational uncertainties.

So we can never actually "know" something is true: we instead assess the consistency of evidence with predictions from theory.

But often the predictions are not black-and-white and must be understood probabilistically.
:::

## Data Generation Approximates Reality

:::: {.columns}
::: {.column width=33%}
![Estimand Estimator Cake](memes/estimand_cake.png){width=100%}
:::
::: {.column width=33%}
::: {.fragment .fade-in}
![Estimand Estimator Cake](memes/estimator_cake.png){width=100%}
:::
:::
::: {.column width=33%}
::: {.fragment .fade-in}
![Estimate Cake](memes/estimate_cake.png){width=100%}
:::
:::
::::

::: {.caption}
Source: Richard McElreath
:::

::: {.notes}
Goal is to start with some "true" process, then apply a procedure (experimental/observational + statistical) and recover what is hopefully a good estimate.

But lots can go wrong in this process!
:::

## Bayesian (Risk-Based) Decision Analysis

Take some decision $d(x)$ based on $x$.

$$\overbrace{R(d(x))}^{\text{risk}} = \int_Y \overbrace{\mathcal{L}(d(x), y)}^{\text{loss function}} \overbrace{\pi(y | x)}^{\substack{\text{probability} \\ \text{of outcome}}}dy$$

::: {.fragment .fade-in}
Then the **optimal decision** is $\hat{\alpha} = \underset{\alpha}{\operatorname{argmin}} R(\alpha)$.

:::

## Pascal's Wager as BDA

:::: {.columns}
::: {.column width=70%}
- Loss of mistaken belief: -c
- Loss of mistaken disbelief: $-\infty$
- Loss of correct disbelief: +c
- Loss of correct belief: $+\infty$

Pascal's conclusion: "Optimal" decision is belief regardless of asserted probability of God's existence.
:::

::: {.column width=30%}
![Blaise Pascal](https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/Blaise_Pascal_Versailles.JPG/1024px-Blaise_Pascal_Versailles.JPG){width=100%}

::: {.caption}
Source: [Wikipedia](https://en.wikipedia.org/wiki/Pascal%27s_wager)
:::
:::
::::

::: {.notes}
Blaise Pascal formulated a decision problem as evidence that rationality could not be used to justify sin/a lack of belief in God. This was done in what we now recognize as Bayesian terms (though it predates Bayes). 
:::

## Standard Parameter Estimators

What if we want to estimate a parameter $\hat{\theta}$ from data $x$? 

| Loss Function | $\mathcal{L}(\hat{\theta}, \theta)$ | $\hat{\alpha}$ |
|:--------:|:---------:|:-------|
| Quadratic | $\|\hat{\theta} - \theta\|^2$ | $\text{Mean}(x)$ |
| Linear | $\|\hat{\theta} - \theta\|$ | $\text{Median}(x)$ |
| 0-1 | $\begin{cases} 0 & \hat{\theta} \neq \theta \\ 1 & \hat{\theta} = \theta \end{cases}$ | $\text{Mode}(x)$ |


## Risk-Based Analysis: The Original Statistical Decision-Making

:::: {.columns}
::: {.column width=50%}
![Orbit of Ceres](https://upload.wikimedia.org/wikipedia/en/4/43/Ceres_Orbit_c.png)

::: {.caption}
Source: [Wikipedia](https://en.wikipedia.org/wiki/Ceres_%28dwarf_planet%29)
:::
:::

::: {.column width=50%}
![Piazzi's Measurements](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Ceres-Beobachtung_von_Piazzi.png/1920px-Ceres-Beobachtung_von_Piazzi.png)

::: {.caption}
Source: [Wikipedia](https://en.wikipedia.org/wiki/Giuseppe_Piazzi)
:::
:::
::::

::: {.notes}
This type of statistical decision-making prompted the introduction of ordinary least squares by Gauss in the 1800s. The question was to predict the position of Ceres given a few error-prone observations by Giuseppe Piazzi.
:::

## Origin of Ordinary Least Squares

:::: {.columns}
::: {.column width=50%}
Gauss (1809): Risk/Bayesian argument for OLS estimator from quadratic loss.

::: {.fragment .fade-in}
![German 10 Mark Note with Gauss](http://old.nationalcurvebank.org///gaussdist/banknote.jpg)

::: {.caption}
Source: [National Curve Bank](https://nationalcurvebank.org/deposits/gaussdist.html)
:::
:::

:::

::: {.column width=50%}
![Gauss](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Carl_Friedrich_Gau%C3%9F%2C_Pastellgem%C3%A4lde_von_Johann_Christian_August_Schwartz%2C_1803%2C_ohne_Rahmen.jpg/800px-Carl_Friedrich_Gau%C3%9F%2C_Pastellgem%C3%A4lde_von_Johann_Christian_August_Schwartz%2C_1803%2C_ohne_Rahmen.jpg){width=60%}

::: {.caption}
Source: [Wikipedia](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss)
:::
:::

::::

# Hypothesis Testing

## Questions We Might Like To Answer

::: {.incremental}
- Are high water levels influenced by environmental change?
- Does some environmental condition have an effect on water quality/etc?
- Does a drug or treatment have some effect?
:::

## Onus probandi incumbit ei qui dicit, non ei qui negat

:::: {.columns width=50%}
::: {.column width=50%}
**Core assumption**: Burden of proof is on someone claiming an  effect (or a similar hypothesis).

:::

::: {.column width=50%}
![Null Hypothesis Meme](memes/skinner_null_hypothesis.png){width=50%}
:::
::::

::: {.notes}
The title of this slide is a reference to the burden of proof is on the person who affirms, not one who denies. Can think of this as similar to Ockham's razor or someone mantras: we want to propose hypotheses about scientific phenomena and then see if the evidence supports it. 
:::


## Null Hypothesis Significance Testing

:::: {.columns}
::: {.column width=60%}
- Check if the data is consistent with a "null" model;
- If the data is unlikely from the null model (to some level of **significance**), this is evidence for the alternative.
- If the data is consistent with the null, there is no need for an alternative hypothesis.
:::

::: {.column width=40%}
![Alternative Hypothesis Meme](memes/mordor_alternative_hypothesis.png)
:::
::::

::: {.notes}
For scientific hypotheses, this has been encoded in the NHST paradigm:

- Think of a "null" hypothesis and look for evidence that it is reasonably consistent with the data.
- If the data can be explained by the null, then we have no clear evidence for the alternative hypothesis.
- If the data is highly unlikely under the null hypothesis, then that gives us reason to reject the null and favor the alternative.

:::

## From Null Hypothesis to Null Model

::: {.quote}
> ...the null hypothesis must be exact, that is free of vagueness and ambiguity, because it must supply the basis of the 'problem of distribution,' of which the test of significance is the solution.

::: {.cite} 
--- R. A. Fisher, *The Design of Experiments*, 1935.
:::
:::

::: {.notes}
The trick is to go from a null scientific hypothesis to a null statistical model, hence Fisher's comment about the need for a null hypothesis to be "exact".
:::

## Example: High Water Nonstationarity

```{julia}
#| output: true
#| echo: true
#| code-fold: true
#| code-overflow: wrap
#| fig-align: center
#| label: fig-surge-data
#| fig-cap: Annual maxima surge data from the San Francisco, CA tide gauge.

# load SF tide gauge data
# read in data and get annual maxima
function load_data(fname)
    date_format = DateFormat("yyyy-mm-dd HH:MM:SS")
    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data
    df = @chain fname begin
        CSV.read(DataFrame; header=false)
        rename("Column1" => "year", "Column2" => "month", "Column3" => "day", "Column4" => "hour", "Column5" => "gauge")
        # need to reformat the decimal date in the data file
        @transform :datetime = DateTime.(:year, :month, :day, :hour)
        # replace -99999 with missing
        @transform :gauge = ifelse.(abs.(:gauge) .>= 9999, missing, :gauge)
        select(:datetime, :gauge)
    end
    return df
end

dat = load_data("data/surge/h551.csv")

# detrend the data to remove the effects of sea-level rise and seasonal dynamics
ma_length = 366
ma_offset = Int(floor(ma_length/2))
moving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]
dat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))

# group data by year and compute the annual maxima
dat_ma = dropmissing(dat_ma) # drop missing data
dat_annmax = combine(dat_ma -> dat_ma[argmax(dat_ma.residual), :], groupby(transform(dat_ma, :datetime => x->year.(x)), :datetime_function))
delete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet
rename!(dat_annmax, :datetime_function => :Year)
select!(dat_annmax, [:Year, :residual])
dat_annmax.residual = dat_annmax.residual / 1000 # convert to m

# make plots
p1 = plot(
    dat_annmax.Year,
    dat_annmax.residual;
    xlabel="Year",
    ylabel="Annual Max Tide Level (m)",
    label=false,
    marker=:circle,
    markersize=5,
    tickfontsize=16,
    guidefontsize=18,
    left_margin=5mm, 
    bottom_margin=5mm
)

n = nrow(dat_annmax)
linfit = lm(@formula(residual ~ Year), dat_annmax)
pred = coef(linfit)[1] .+ coef(linfit)[2] * dat_annmax.Year

plot!(p1, dat_annmax.Year, pred, linewidth=3, label="Linear Trend")
```

::: {.notes}
For example, consider this annual extreme high water dataset from San Francisco from 1897 through 2022. A linear fit gives us an observed trend of 0.4 mm/yr. Is that meaningful? 

Note that we didn't do anything yet to justify whether linear regression is a non-stupid thing to do (hint: it's a stupid thing to do): we'll talk about this more later.
:::

## The Null: Is The Trend Real?

$\mathcal{H}_0$ (Null Hypothesis):

- The "trend" is just due to chance, there is no long-term trend in the data.
  
::: {.fragment .fade-in}
- Statistically: 

$$y = \underbrace{b}_{\text{constant}} + \underbrace{\varepsilon}_{\text{residuals}}, \qquad \varepsilon \underbrace{\sim}_{\substack{\text{distributed} \\ {\text{according to}}}} \mathcal{N}(0, \sigma^2) $$

:::

## An Alternative Hypothesis

$\mathcal{H}$:

- The trend is *likely* non-zero in time.
  
::: {.fragment .fade-in}
- Statistically: 

$$y = a \times t + b + \varepsilon, \qquad \varepsilon \sim Normal(0, \sigma^2) $$

:::

## Null Test

Comparing $\mathcal{H}$ with $\mathcal{H}_0$:

- $\mathcal{H}$: $a \neq 0$
- $\mathcal{H}_0$: $a = 0$

::: {.note}
In this example, our null is an example of a *point-null* hypothesis.
:::

## Computing the Test Statistic

For this type of null hypothesis test, our **test statistic** is the slope of the OLS fit $$\hat{a} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{(x_i - \bar{x})^2}.$$

::: {.fragment .fade-in}
**Assuming the null**, the **sampling distribution** of the statistic is $$\frac{\hat{a}}{SE_{\hat{a}}} \sim t_{n-2}.$$
:::

## Statistical Significance

Is the value of the test statistic consistent with the null hypothesis?


:::: {.columns}
::: {.column width=40%}
More formally, could the test statistic have been reasonably observed from a random sample **given the null hypothesis**?
:::

::: {.column width=60%}
```{julia}
null_test = TDist(n-2)
plot(null_test, label="Test Distribution (Null)")
vline!([coef(linfit)[2] / stderror(linfit)[2]], color=:red, label="OLS Test Statistic")
plot!(size=(700, 400))
```
:::
::::

## p-Values: Quantification of "Surprise"

:::: {.columns}
::: {.column width=50%}

One-Tailed Test: 

```{julia}
#| output: true
#| echo: false
#| label: fig-p-value-one-tail
#| fig-cap: Illustration of a p-value

test_dist = Normal(0, 3)
x = -10:0.01:10
plot(x, pdf.(test_dist, x), linewidth=3, legend=false, color=:black, xticks=false,  yticks=false, ylabel="Probability", xlabel="Test Statistic", bottom_margin=10mm, left_margin=10mm, guidefontsize=16)
vline!([5], linestyle=:dash, color=:purple, linewidth=3, )
areaplot!(5:0.01:10, pdf.(test_dist, 5:0.01:10), color=:green, alpha=0.4)
quiver!([-4.5], [0.095], quiver=([1], [-0.02]), color=:black, linewidth=2)
annotate!([-5], [0.11], text("Null\nSampling\nDistribution", color=:black))
quiver!([6.5], [0.03], quiver=([-1], [-0.015]), color=:green, linewidth=2)
annotate!([6.85], [0.035], text("p-value", :green))
quiver!([3.5], [0.02], quiver=([1.5], [0]), color=:purple, linewidth=2)
annotate!([2], [0.02], text("Observed\nTest Statistic", :purple))
plot!(size=(600, 400))
```
:::
::: {.column width=50%}
Two-Tailed Test:

```{julia}
#| output: true
#| echo: false
#| label: fig-p-value-two-tail
#| fig-cap: Illustration of a two-tailed p-value

vline!([-5], linestyle=:dash, color=:purple, linewidth=3, )
areaplot!(-10:0.01:-5, pdf.(test_dist, -10:0.01:-5), color=:green, alpha=0.4)
plot!(size=(600, 400))
```
:::
::::

::: {.notes}
In the tide gauge test, for a two-tailed test, the p-value is 0.02. What does that mean?
:::

# Statistical Significance

## Error Types

<table>
  <tr>
    <td></td>
    <td></td>
    <td colspan="2">**Null Hypothesis Is**</td>
  </tr>
  <tr>
    <td></td>
    <td></td>
    <td>True</td>
    <td>False </td>
  </tr>
  <tr>
    <td rowspan="2">**Decision About Null Hypothesis**</td>
    <td>Don't reject</td>
    <td>True negative (probability $1-\alpha$)</td>
    <td>Type II error (probability $\beta$)</td>
  </tr>
  <tr>
    <td>Reject</td>
    <td>Type I Error (probability $\alpha$)</td>
    <td>True positive (probability $1-\beta$)</td>
  </tr>
</table>

::: {.notes}
The general testing framework is built around Type I (false positive) and Type II (false negative) errors.
:::

## Navigating Type I and II Errors

The standard null hypothesis significance framework is based on balancing the chance of making **Type I (false positive)** and **Type II (false negative)** errors.

**Idea**: Set a significance level $\alpha$ which is an "acceptable" probability of making a Type I error.

**Aside**: The probability $1-\beta$ of correctly rejecting $H_0$ is the *power*.

::: {.notes}
Note that these are frequentist concepts, not applicable to a single dataset (which give rise to p-values, which are random values).

If we only run a single experiment all we can claim is that if we had run a long series of experiments we would have had 100α% false positives had H0 been true and 100β% false negatives had H1 been true provided we got the power calculations right. Note the conditionals.
:::

## p-Value and Significance

**Common practice**: If the p-value is sufficiently small (below $\alpha$), **reject the null hypothesis** with $1-\alpha$ confidence, or declare that **the alternative hypothesis is statistically significant** at the $1-\alpha$ level.

This can mean:

::: {.fade-in .fragment}
1. The null hypothesis is not true for that data-generating process;
2. The null hypothesis *is* true but the data is an outlying sample.
:::

::: {.notes}
This is a strange hybrid of two schools of frequentist statistics, that of Fisher and of Neyman-Pearson. Fisher viewed p-values as weak evidence which was part of an inductive process (even when assuming unbiased sampling and accurate measurement), while the Neyman-Pearson significance framework was based on quantitative specifications of alternative hypotheses with explicitly calculated power.
:::

## What p-Values Are Not

:::: {.columns}
::: {.column width=50%}
1. Probability that the null hypothesis is true (this is **never computed**);
2. An indication of the effect size (or the stakes of that effect).
:::
::: {.column width=50%}
$$ \underbrace{p(S \geq \hat{S}) | \mathcal{H}_0)}_{\text{p-value}} \neq \underbrace{p(\mathcal{H}_0 | S \geq \hat{S})}_{\substack{\text{probability of} \\ \text{null}}}!$$
:::
::::

::: {.notes}
A p-value is a random variable which depends on the data. It's evidence which can be collected from a single experiment but can not establish . 

Over repeated experiments, reasoning about validity of the null should have the right properties assuming the experiments are conducted faithfully.
:::

# Problems with Null Hypothesis Testing

## Statistical Significance &ne; Scientific Significance

:::: {.columns}
::: {.column width=50%}

Statistical significance does not mean anything about:

1. whether the alternative hypothesis is "true";
2. an accurate reflection of the data-generating process.

:::
::: {.column width=50%}
![Hypothesis vs. Causal Meme](memes/godzilla_doge_causal.png)
:::
:::

## What is Any Statistical Test Doing?

1. Assume the null hypothesis $\mathcal{H}_0$.
2. Compute the test statistic $\hat{S}$ for the sample.
3. **Obtain the sampling distribution of the test statistic $S$ under $H_0$.**
4. Calculate $\mathbb{P}(S > \hat{S})$ (*the p-value*).

## Why Was $\mathcal{H}_0$ chosen?

:::: {.columns}
::: {.column width=50%}
- Often out of convenience for the test.
- Point-null hypotheses are almost always wrong for the social and environmental sciences.
:::

::: {.column width=50%}
![Point-Null Hypothesis Meme](memes/point_null_space.png)
:::
::::

## Non-Uniqueness of "Null" Models

:::: {.columns}
::: {.column width=50%}
Is there a trend in the SF tide gauge trend data?

- Trend as regression ($p \approx 0.02$)
- Mann-Kendall test for monotonic trend ($p \approx 0.5$)
:::
::: {.column width=50%}
![Non-Uniqueness of Null Models](figures/mcelreath_hypothesis_nonunique.png)

::: {.caption}
Source: @mcelreath2020statistical [Fig. 1.2]
:::
:::
::::

## Statistical Test Zoo

:::: {.columns}
::: {.column width=50%}
![Zoo of Statistical Tests](figures/mcelreath_test_zoo.png)

::: {.caption}
Source: @mcelreath2020statistical [Fig. 1.1]
:::
:::

::: {.column width=50%}
::: {.fragment .fade-in}
![Zoo of Statistical Tests](memes/pepe_silvia_tests.png)
:::
:::

::::

## Multiple Comparisons

:::: {.columns}
::: {.column width=50%}
If you conduct multiple statistical tests, you **must** account for all of these in the p-value computation and assessment of significance.

**Important**: This includes model selection!
:::

::: {.column width=50%}
![Multiple Comparisons Meme](memes/multiple_comparisons.png)
:::
::::

::: {.notes}
The core issue is the standard test statistics have the right Type I/Type II properties for each individual test, but multiple tests distort these frequencies, sometimes quite dramatically.

For example, suppose each individual test has a 5% Type I error rate. If you test 100 different models, and the errors are independent, you would expect 5 false positives, one of which would be selected by minimizing the p-value. The probability of at least one type I error is >99%.

There are a number of corrections (Bonferroni being the most common), but sometimes stepwise tests are subtle, including cases of model selection followed by model fitting. You can also use simulation to estimate the false-positive rate for the procedure under a null data-generating process, which ties into the broader methods we'll discuss.
:::

## Results Are Flashy, But Meaningless Without Methods

![Elton John Results Section Meme](memes/elton_john_results.jpg)

::: {.caption}
Source: Richard McElreath
:::

## Interpretability of p-Values and Significance

:::: {.columns}
::: {.column width=50%}
- p-values are often confused with hypothesis probabilities or Type I error rates
- p-values are a continuous measure of "surprise" at seeing a dataset given the null, but **"significance" is binary**.
:::

::: {.column width=50%}
![XKCD #1478 ](https://imgs.xkcd.com/comics/p_values_2x.png){width=50%}

::: {.caption}
*Source*: [XKCD](https://xkcd.com/1478/)
:::
:::

::::

## Practical Results of NHST

:::: {.columns}
::: {.column width=50%}

Perhaps most damningly:

The null hypothesis approach, **as described here and typically practiced** has empirically failed to maintain rigor and credibility in the scientific literature [@Ioannidis2005-zb; @Szucs2017-of].

  
:::
::: {.column width=50%}
![How Could Stats Do This Meme](memes/how_could_stats_do_this.png){width=75%}

::: {.caption}
Source: Richard McElreath
:::
:::
::::

## Practical Results of NHST

:::: {.columns}
::: {.column width=50%}

- Overconfident confidence intervals;
- Strawman null hypotheses;
- Biased sampling;
- Lack of replications;
- p-hacking.
  
:::
::: {.column width=50%}
![How Could Stats Do This Meme](memes/how_could_stats_do_this.png){width=75%}

::: {.caption}
Source: Richard McElreath
:::
:::
::::


## What Might Be More Satisfying?

::: {.incremental}
- Consideration of multiple plausible (possibly more nuanced) hypotheses.
- Assessment/quantification of evidence consistent with different hypotheses.
- Identification of opportunities to design experiments/learn.
- Insight into the effect size.
:::

## Note: This Does Not Mean Null Hypothesis Testing Is Useless!

:::: {.columns}
::: {.column width=50%}
Examining and testing the implications of competing models is important, including "null" models!

:::
::: {.column width=50%}
![Null Hypothesis Selection Good Vs. Bad](memes/geordi_null_choice.png){width=75%}
:::
::::

::: {.notes}
Deborah Mayo discusses this interpretation as "severe testing": apply different levels of scrutiny to a scientific model to see what level of severity breaks the model.

The idea of a p-value (not necessarily "significance") being a piece of inductive evidence rather than a threshold for validity (especially for an individual experiment) reflects the original views of Fisher.
:::

# Key Points

## Hypothesis Testing

- Classical framework: Compare a null hypothesis (no effect) to an alternative (some effect)
- $p$-value: probability (under $H_0$) of more extreme test statistic than observed.
- "Significant" if $p$-value is below a significance level reflecting acceptable Type I error rate.

## Problems with NHST framework

- $p$-values are often over-interpreted and are often be incorrectly calculated, with negative outcomes!
- **Important**: "Big" data can make things worse, as NHST is highly sensitive to small but evidence effects.


# Upcoming Schedule

## Next Classes

**Wednesday**: What is a **generative** model?

**Next Week+**: Prob/Stats "Review" and Fundamentals

## Assessments

**Homework 1** available; due *next* Friday (2/7).

# References

## References (Scroll for Full List)
