---
title: "Bayesian Computation"
subtitle: "Lecture 13"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "March 13, 2024"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: 
            method: mathjax
            url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        touch: false
        controls: true
execute:
    freeze: auto
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using Distributions
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using LaTeXStrings
using Measures
using Dates
using DataFrames
using DataFramesMeta
using CSV

Random.seed!(1)
```

# Last Class

## Sampling Distributions

:::: {.columns}
::: {.column width=40%}
The **sampling distribution** of a statistic captures the uncertainty associated with random samples.
:::
::: {.column width=60%}
![Sampling Distribution](figures/true-sampling.png)
:::
::::

## The Bootstrap Principle

:::: {.columns}
::: {.column width=60%}
@Efron1979-zv suggested combining estimation with simulation: the **bootstrap**.

**Key idea**: use the data to simulate a data-generating mechanism.
:::
::: {.column width=40%}
::: {.center}
![Baron von Munchhausen Pulling Himself By His Hair](https://upload.wikimedia.org/wikipedia/commons/3/3b/Muenchhausen_Herrfurth_7_500x789.jpg){width=60%}

::: {.caption}
Source: [Wikipedia](https://en.wikipedia.org/wiki/M%C3%BCnchhausen_trilemma)
:::
:::
:::
::::

## Why Does The Bootstrap Work?

Let $t_0$ the "true" value of a statistic, $\hat{t}$ the estimate of the statistic from the sample, and $(\tilde{t}_i)$ the bootstrap estimates.

- Variance: $\text{Var}[\hat{t}] \approx \text{Var}[\tilde{t}]$
- Then the bootstrap error distribution approximates the sampling distribution
  $$(\tilde{t}_i - \hat{t}) \overset{\mathcal{D}}{\sim} \hat{t} - t_0$$


## The Non-Parametric Bootstrap

:::: {.columns}
::: {.column width=40%}
The non-parametric bootstrap is the most "naive" approach to the bootstrap: **resample-then-estimate**.
:::
::: {.column width=60%}
![Non-Parametric Bootstrap](figures/npboot-sampling.png)
:::
::::

## Why Use The Bootstrap?

- Do not need to rely on variance asymptotics;
- Can obtain non-symmetric CIs.


## Approaches to Bootstrapping Structured Data

- **Correlations**: Transform to uncorrelated data (principal components, etc.), sample, transform back.
- **Time Series**: Block bootstrap

## Generalizing the Block Bootstrap

The rough transitions in the block bootstrap can really degrade estimator quality.

- Improve transitions between blocks
- Moving blocks (allow overlaps)


## Sources of Non-Parametric Bootstrap Error

1. **Sampling error**: error from using finitely many replications
2. **Statistical error**: error in the bootstrap sampling distribution approximation

## When To Use The Non-Parametric Bootstrap

- Sample is representative of the data distribution
- Doesn't work well for extreme values!

# Bayesian Computation

## Reminder: Bayesian Modeling

Probability is the *degree of belief in a "proposition"*.

Then it makes sense to discuss the *probability* of

- model parameters $\mathbf{\theta}$
- unobserved data $\tilde{\mathbf{y}}$ 

conditional on the observations $\mathbf{y}$, which we can denote:
$$p(\mathbf{\theta} | \mathbf{y}) \text{ or } p(\tilde{\mathbf{y}} | \mathbf{y})$$

## Conditioning on Observations

This fundamental conditioning on observations $\mathbf{y}$ is a distinguishing feature of Bayesian inference.

**Compare**: frequentist approaches are based on re-estimated over the distribution of possible $\mathbf{y}$ conditional on the "true" parameter value.

## Bayes' Rule

Update priors with Bayes' Rule:

$$\underbrace{{p(\theta | y)}}_{\text{posterior}} = \frac{\overbrace{p(y | \theta)}^{\text{likelihood}}}{\underbrace{p(y)}_\text{normalization}} \overbrace{p(\theta)}^\text{prior}$$

## Goals of Bayesian Computation

1. Sampling from the *posterior* distribution
  $$p(\theta | \mathbf{y})$$
2. Sampling from the *posterior predictive* distribution
  $$p(\tilde{y} | \mathbf{y})$$
  by generating data.

## Bayesian Computation and Monte Carlo

Bayesian computation involves Monte Carlo simulation from the posterior (predictive) distribution.

These samples can then be analyzed to identify estimators, credible intervals, etc.

## Posterior Sampling

Trivial for *extremely* simple problems: 

1. low-dimensional.
2. with "conjugate" priors (which make the posterior a closed-form distribution).

::: {.fragment .fade-in}
But in general these can be high-dimensional distributions that don't have closed form from which we can conveniently sample.
:::

## A First Algorithm: Rejection Sampling

Idea: 

1. Generate proposed samples from another distribution $g(\theta)$ which covers the target $p(\theta | \mathbf{y})$;
2. Accept those proposals based on the ratio of the two distributions.

## Rejection Sampling Algorithm

Suppose $p(\theta | \mathbf{y}) \leq M g(\theta)$ for some $1 < M < \infty$.

1. Simulate $u \sim \text{Unif}(0, 1)$.
2. Simulate a proposal $\hat{\theta} \sim g(\theta)$.
3. If 
  $$u < \frac{p(\hat{\theta} | \mathbf{y})}{Mg(\hat{\theta})},$$
  accept $\hat{\theta}$. Otherwise reject.

## Rejection Sampling Challenges

1. Probability of accepting a sample is $1/M$, so the "tighter" the proposal distribution coverage the more efficient the sampler.
2. Need to be able to compute $M$.

Finding a good proposal and computing $M$ may not be easy (or possible) for complex posteriors!

**How can we do better?**

## How Can We Do Better?

The fundamental problem with rejection sampling is that we don't know the properties of the posterior. So we don't know that we have the appropriate coverage. But...

What if we could construct an proposal/acceptance/rejection scheme that necessarily converged to the target distribution, even without *a priori* knowledge of its properties?

# Markov Chain Basics

## What Is A Markov Chain?

Consider a stochastic process $\\{X\_t\\}\_{t \in \mathcal{T}}$, where 

- $X\_t \in \mathcal{S}$ is the state at time $t$, and 
- $\mathcal{T}$ is a time-index set (can be discrete or continuous)
- $\mathbb{P}(s\_i \to s\_j) = p\_{ij}$. 

## Markovian Property

This stochastic process is a **Markov chain** if it satisfies the **Markovian (or memoryless) property**:
$$
\mathbb{P}(X_{T+1} = s_i | X_1=x_1, \ldots, X_T=x_T) = \mathbb{P}(X_{T+1} = s_i| X_T=x_T)
$$

## Example: "Drunkard's Walk"

Consider a process where we can "stumble" to the left or right with equal probability.

The *unconditional* probability $\mathbb{P}(X\_T = s\_i)$ can be modeled by a sum of coin flips from the initial state $X\_0$, but the *conditional* probability $\mathbb{P}(X\_T = s\_i | X\_{T-1} = x\_{T-1})$ only depends on the current node, not how we got there.

## Example: Weather

Let's look at a more interesting example. Suppose the weather can be foggy, sunny, or rainy.

Based on past experience, we know that:

1. There are never two sunny days in a row;
2. Even chance of two foggy or two rainy days in a row;
3. A sunny day occurs 1/4 of the time after a foggy or rainy day.

## Aside: Higher Order Markov Chains

Suppose that today's weather depends on the prior *two* days. 

::: {.incremental}
1. Can we write this as a Markov chain?
2. What are the states?
:::

## Weather Transition Matrix

We can summarize these probabilities in a **transition matrix** $P$:
$$
P = 
\begin{array}{cc} 
\begin{array}{ccc}
\phantom{i}\color{red}{F}\phantom{i} & \phantom{i}\color{red}{S}\phantom{i} & \phantom{i}\color{red}{R}\phantom{i}
\end{array}
\\\\
\begin{pmatrix}
      1/2 & 1/4 & 1/4 \\\\
      1/2 & 0 & 1/2 \\\\
      1/4 & 1/4 & 1/2
      \end{pmatrix}
&
\begin{array}{ccc}
\color{red}F  \\\\ \color{red}S  \\\\ \color{red}R
\end{array}   
\end{array}
$$

Rows are the current state, columns are the next step, so $\sum\_i p\_{ij} = 1$.

## Weather Example: State Probabilities

Denote by $\lambda^t$ a probability distribution over the states at time $t$.

Then $\lambda^t = \lambda^{t-1}P$:

$$\begin{pmatrix}\lambda^t_F & \lambda^t_S & \lambda^t_R \end{pmatrix} =  
\begin{pmatrix}\lambda^{t-1}_F & \lambda^{t-1}_S & \lambda^{t-1}_R \end{pmatrix} 
      \begin{pmatrix}
      1/2 & 1/4 & 1/4 \\\\
      1/2 & 0 & 1/2 \\\\
      1/4 & 1/4 & 1/2
      \end{pmatrix}
$$

## Multi-Transition Probabilities

Notice that $$\lambda^{t+i} = \lambda^t P^i,$$ so multiple transition probabilities are $P$-exponentials. 

$$P^3 =
\begin{array}{cc} 
\begin{array}{ccc}
\phantom{iii}\color{red}{F}\phantom{ii} & \phantom{iii}\color{red}{S}\phantom{iii} & \phantom{ii}\color{red}{R}\phantom{iii}
\end{array}
\\\\
\begin{pmatrix}
      26/64 & 13/64 & 25/64 \\\\
      26/64 & 12/64 & 26/64 \\\\
      26/64 & 13/64 & 26/64
      \end{pmatrix}
&
\begin{array}{ccc}
\color{red}F  \\\\ \color{red}S  \\\\ \color{red}R
\end{array}   
\end{array}
$$

## Long Run Probabilities

What happens if we let the system run for a while starting from an initial sunny day? 

## Stationary Distributions

This stabilization always occurs when the probability distribution is an eigenvector of $P$ with eigenvalue 1:

$$\pi = \pi P.$$

This is called an *invariant* or a *stationary* distribution.

## What Markov Chains Have Stationary Distributions?


Not necessarily! The key is two properties:

- Irreducible
- Aperiodicity

## Irreducibility

A Markov chain is **irreducible** if every state is accessible from every other state, *e.g.* for every pair of states $s\_i$ and $s\_j$ there is some $k > 0$ such that $P\_{ij}^k > 0.$

## Aperiodicity

The period of a state $s_i$ is the greatest common divisor $k$ of all $t$ such that $P^t_{ii} > 0$. 

In other words, if a state $s_i$ has period $k$, all returns must occur after time steps which are multiples of $k$.

A Markov chain is **aperiodic** if all states have period 1.

## Ergodicity

A Markov chain is **ergodic** if it is aperiodic and irreducible.

Ergodic Markov chains have a  *limiting* distribution which is the limit of the time-evolution of the chain dynamics, *e.g.*
$$\pi\_j = \lim\_{t \to \infty} \mathbb{P}(X\_t = s_j).$$

**Key**: this limit is *independent* of the initial state probability.

**Intuition**: Ergodicity means we can exchange thinking about *time-averages* and *ensemble-averages*.

## Limiting Distributions are Stationary

For an ergodic chain, the limiting distribution is the unique stationary distribution (we won't prove uniqueness):

\begin{align}
\pi\_j &= \lim\_{t \to \infty} \mathbb{P}(X\_t = s\_j | X\_0 = s\_i) \\\\
&= \lim\_{t \to \infty} (P^{t+1})\_{ij} = \lim\_{t \to \infty} (P^tP)\_{ij} \\\\
&= \lim\_{t \to \infty} \sum\_d (P^n)\_{id} P\_{dj} \\\\
&= \sum\_d \pi\_d P\_{dj}
\end{align}

## Transient Portion of the Chain

The portion of the chain prior to convergence to the stationary distribution is called the **transient** portion. 

This will be important next week!

## Detailed Balance

The last important concept is **detailed balance**.

Let $\{X\_t\}$ be a Markov chain and let $\pi$ be a probability distribution over the states. Then the chain is in detailed balance with respect to $\pi$ if
$$\pi_i P\_{ij} = \pi_j P_{ji}.$$

::: {.fragment .fade-in}
Detailed balance implies **reversibility**: the chain's dynamics are the same when viewed forwards or backwards in time.
:::

## Detailed Balance and Stationary Distributions

Detailed balance is a sufficient but not necessary condition for the existence of a stationary distribution (namely $\pi$):

$$\begin{align*}
(\pi P)_i &= \sum_j \pi_j P_{ji} \\
&= \sum_j \pi_i P_{ij} \\
&= \pi_i \sum_j P_{ij} = \pi_i
\end{align*}$$


## Idea of Sampling Algorithm

The idea of our sampling algorithm (which we will discuss next time) is to construct an ergodic Markov chain from the detailed balance equation for the target distribution. 

- Detailed balance implies that the target distribution is the stationary distribution.
- Ergodicity implies that this distribution is unique and can be obtained as the limiting distribution of the chain's dynamics.

## Idea of Sampling Algorithm

In other words: 

- Generate an appropriate Markov chain so that its stationary distribution of the target distribution $\pi$;
- Run its dynamics long enough to converge to the stationary distribution;
- Use the resulting ensemble of states as Monte Carlo samples from $\pi$ .

## Sampling Algorithm

Any algorithm which follows this procedure is a Markov chain Monte Carlo algorithm.

**Good news**: These algorithms are designed to work quite generally, without (*usually*) having to worry about technical details like detailed balance and ergodicity.

**Bad news**: They *can* involve quite a bit of tuning for computational efficiency. Some algorithms or implementations are faster/adaptive to reduce this need.

## Sampling Algorithm

**Annoying news**: Convergence to the stationary distribution is only guaranteed asymptotically; evaluating if the chain has been run long enough requires lots of heuristics.


# Key Points and Upcoming Schedule

## Key Points

- **Bootstrap Principle**: Use the data as a proxy for the population.
- **Key**: Bootstrap gives idea of sampling error in statistics (including model parameters)
- Distribution of $\tilde{t} - \hat{t}$ approximates distribution around estimate $\hat{t} - t_0$.
- Allows us to estimate uncertainty of estimates (confidence intervals, bias, etc).
- Parametric bootstrap introduces model specification error

## Bootstrap Variants

  - Resample Cases (Non-Parametric)
  - Resample Residuals (Semi-Parametric)
  - Simulate from Fitted Model (Parametric)

## Which Bootstrap To Use?

- **Bias-Variance Tradeoff**: Parametric Bootstrap has narrowest intervals, Resampling Cases widest (*Exercise 8*)
- Depends on trust in model "correctness": 
  - Do we trust the model parameters to be "correct"?
  - Do we trust the shape of the regression model?
  - Do we trust the data-generating process?

## Next Classes

**Wednesday**: Bayesian Computation and Markov chains

**Next Week**: Markov chain Monte Carlo

## Assessments

- **Reading**: @Rahmstorf2011-oj
- **Exercise 8**: Due Friday
- **Homework 3**: Due 3/22

# References

## References