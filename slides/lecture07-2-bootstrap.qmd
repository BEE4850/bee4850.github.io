---
title: "Uncertainty Quantification and The Bootstrap"
subtitle: "Lecture 12"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "March 05, 2024"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: mathjax
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        mermaid: 
            theme: dark
engine: julia
filters:
  - code-fullscreen
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using DataFrames
using DataFramesMeta
using CSV
using Dates
using Distributions
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using Optim
using LaTeXStrings
using Measures

Random.seed!(1)

plot_font = "Computer Modern"
default(
    fontfamily=plot_font,
    linewidth=3, 
    framestyle=:box, 
    label=nothing, 
    grid=false,
    guidefontsize=18,
    legendfontsize=16,
    tickfontsize=16,
    titlefontsize=20,
    bottom_margin=10mm,
    left_margin=5mm
)
```

# Review

## Monte Carlo Simulation

- **Monte Carlo Principle**: Approximate $$\mathbb{E}_f[h(X)] = \int h(x) f(x) dx \approx \frac{1}{n} \sum_{i=1}^n h(X_i)$$
- Monte Carlo is an unbiased estimator, but beware (and always report) the standard error $\sigma_n = \sigma_Y / \sqrt{n}$ or confidence intervals.
- More advanced methods can reduce variance, but may be difficult to implement in practice.

## Uses of Monte Carlo

- Estimate expectations / quantiles;
- Calculate deterministic quantities (framed as stochastic expectations);
- Optimization (**problem 3 on HW3**).

# Sampling Distributions

## So Far...

We're 7 weeks in and haven't said anything about how to **quantify uncertainties**.

**Let's talk about that**.

## Sampling Distributions

:::: {.columns}
::: {.column width=40%}
The **sampling distribution** of a statistic captures the uncertainty associated with random samples.
:::
::: {.column width=60%}
![Sampling Distribution](figures/true-sampling.png)
:::
::::

## Estimating Sampling Distributions

- **Special Cases**: can derive closed-form representations of sampling distributions (think statistical tests)
- **Asymptotics**: Central Limit Theorem or Fisher Information

## Fisher Information

:::: {.columns}
::: {.column width=50%}

**Fisher Information**: $$\mathcal{I}_x(\theta) = -\mathbb{E}\left[\frac{\partial^2}{\partial \theta_i \partial \theta_j} \log \mathcal{L}(\theta | x)\right]$$

***Observed* Fisher Information** (uses observed data and calculated at the MLE): $$\mathcal{I}_\tilde{x}(\hat{\theta})$$
:::
::: {.column width=50%}
```{julia}
#| label: fig-fisher-information
#| fig-cap: Examples of Fisher Information

θ = -0.5:0.01:5
fi1(θ) = log(-(θ - 2)^4 + 3 * (θ + 2) + 80)
p1 = plot(θ, fi1.(θ), lw=3, label="Log-Likelihood", size=(500, 400))
pts = [2 4.5]
labels = ["Small FI" "High FI"]
scatter!(p1, pts, fi1.(pts), label=labels, markersize=5)
```
:::
::::

## Fisher Information and Standard Errors

Asymptotic result: $$\sqrt{n}(\theta_\text{MLE} - \theta^*) \to N(0, \left(\mathcal{I}_x(\hat{\theta^*})\right)^{-1}$$

Sampling distribution based on observed data: $$\theta \sim N\left(\hat{\theta}_\text{MLE}, \left(n\mathcal{I}_\tilde{x}(\theta_\text{MLE})\right)^{-1}\right)$$

## Estimating Fisher Information

- Can be done with automatic differentiation or (sometimes hard) calculations;
- May be singular (no inverse \Rightarrow; undefined standard errors) for complex models;
- May not be a good approximation of the variance for finite samples!

# The Bootstrap

## The Bootstrap Principle

:::: {.columns}
::: {.column width=50%}
@Efron1979-zv suggested combining estimation with simulation: the **bootstrap**.

**Key idea**: use the data to simulate a data-generating mechanism.
:::
::: {.column width=50%}
::: {.center}
![Xxibit Bootstrap Meme](memes/xxibit_bootstrap.png)

:::
:::
::::

## Bootstrap Principle


:::: {.columns}
::: {.column width=40%}

- Assume the existing data is representative of the "true" population, 
- Simulate based on properties of the data itself
- Re-estimate statistics from re-samples.

:::
::: {.column width=60%}
![Bootstrap Sampling Distribution](figures/npboot-sampling.png)
:::
::::



## Why Does The Bootstrap Work?

Efron's key insight: due to the Central Limit Theorem, the bootstrap distribution $\mathcal{D}(\tilde{t}_i)$ has the same relationship to the observed estimate $\hat{t}$ as the sampling distribution $\mathcal{D}(\hat{t})$ has to the "true" value $t_0$:

$$\mathcal{D}(\tilde{t} - \hat{t}) \approx \mathcal{D}(\hat{t} - t_0)$$

where $t_0$ the "true" value of a statistic, $\hat{t}$ the sample estimate, and $(\tilde{t}_i)$ the bootstrap estimates.

## What Can We Do With The Bootstrap?

Let $t_0$ the "true" value of a statistic, $\hat{t}$ the estimate of the statistic from the sample, and $(\tilde{t}_i)$ the bootstrap estimates.

::: {.incremental}
- Estimate Variance: $\text{Var}[\hat{t}] \approx \text{Var}[\tilde{t}]$
- Bias Correction: $\mathbb{E}[\hat{t}] - t_0 \approx \mathbb{E}[\tilde{t}] - \hat{t}$
:::

::: {.fragment .fade-in}
Notice that bias correction "shifts" away from the bootstrapped samples.
:::

## "Simple" Bootstrap Confidence Intervals

- **Basic Bootstrap CIs** (based on CLT for error distribution $\tilde{t} - \hat{t}$): $$\left(\hat{t} - (Q_{\tilde{t}}(1-\alpha/2) - \hat{t}), \hat{t} - (Q_{\tilde{t}}(\alpha/2) - \hat{t})\right)$$
- **Percentile Bootstrap CIs** (simplest, often wrong coverage): $$(Q_{\tilde{t}}(1-\alpha/2), Q_{\tilde{t}}(1-\alpha/2))$$

# The Non-Parametric Bootstrap

## The Non-Parametric Bootstrap

:::: {.columns}
::: {.column width=40%}
The non-parametric bootstrap is the most "naive" approach to the bootstrap: **resample-then-estimate**.
:::
::: {.column width=60%}
![Non-Parametric Bootstrap](figures/npboot-sampling.png)
:::
::::

## Sources of Non-Parametric Bootstrap Error

1. **Sampling error**: error from using finitely many replications
2. **Statistical error**: error in the bootstrap sampling distribution approximation

## Simple Example: Is A Coin Fair?

Suppose we have observed twenty flips with a coin, and want to know if it is weighted.

```{julia}
#| echo: true
#| code-fold: true

# define coin-flip model
p_true = 0.6
n_flips = 20
coin_dist = Bernoulli(p_true)
# generate data set
dat = rand(coin_dist, n_flips)
freq_dat = sum(dat) / length(dat)
dat'
```

The frequency of heads is `{julia} round(freq_dat, digits=2)`. 

## Is The Coin Fair?

```{julia}
#| echo: true
#| code-fold: true
#| label: fig-boot-20
#| fig-cap: "Bootstrap heads frequencies for 20 resamples."

# bootstrap: draw new samples
function coin_boot_sample(dat)
    boot_sample = sample(dat, length(dat); replace=true)
    return boot_sample
end

function coin_boot_freq(dat, nsamp)
    boot_freq = [sum(coin_boot_sample(dat)) for _ in 1:nsamp]
    return boot_freq / length(dat)
end

boot_out = coin_boot_freq(dat, 1000)
q_boot = 2 * freq_dat .- quantile(boot_out, [0.975, 0.025])

p = histogram(boot_out, xlabel="Heads Frequency", ylabel="Count", title="1000 Bootstrap Samples", label=false, right_margin=5mm)
vline!(p, [p_true], linewidth=3, color=:orange, linestyle=:dash, label="True Probability")
vline!(p, [mean(boot_out) ], linewidth=3, color=:red, linestyle=:dash, label="Bootstrap Mean")
vline!(p, [freq_dat], linewidth=3, color=:purple, linestyle=:dot, label="Observed Frequency")
vspan!(p, q_boot, linecolor=:grey, fillcolor=:grey, alpha=0.3, fillalpha=0.3, label="95% CI")
plot!(p, size=(1000, 450))
```

## Larger Sample Example

```{julia}
#| echo: true
#| code-fold: true
#| label: fig-boot-50
#| fig-cap: "Bootstrap heads frequencies for 1000 resamples."

n_flips = 50
dat = rand(coin_dist, n_flips)
freq_dat = sum(dat) / length(dat)

boot_out = coin_boot_freq(dat, 1000)
q_boot = 2 * freq_dat .- quantile(boot_out, [0.975, 0.025])

p = histogram(boot_out, xlabel="Heads Frequency", ylabel="Count", title="1000 Bootstrap Samples", titlefontsize=20, guidefontsize=18, tickfontsize=16, legendfontsize=16, label=false, bottom_margin=7mm, left_margin=5mm, right_margin=5mm)
vline!(p, [p_true], linewidth=3, color=:orange, linestyle=:dash, label="True Probability")
vline!(p, [mean(boot_out) ], linewidth=3, color=:red, linestyle=:dash, label="Bootstrap Mean")
vline!(p, [freq_dat], linewidth=3, color=:purple, linestyle=:dot, label="Observed Frequency")
vspan!(p, q_boot, linecolor=:grey, fillcolor=:grey, alpha=0.3, fillalpha=0.3, label="95% CI")
plot!(p, size=(1000, 450))
```


## Why Use The Non-Parametric Bootstrap?

- Do not need to rely on variance asymptotics;
- Can obtain non-symmetric CIs.
- Embarrassingly parallel to simulate new replicates and generate statistics.

## When Can't You Use The Non-Parametric Bootstrap

- Maxima/minima
- Very extreme values.

Generally, anything very sensitive to outliers which might not be re-sampled.


## Bootstrapping with Structured Data

The naive non-parametric bootstrap that we just saw doesn't work if data has structure, e.g. spatial or temporal dependence.

## Simple Bootstrapping Fails with Structured Data

```{julia}
#| echo: true
#| code-fold: true
#| label: fig-ts-bootstrap
#| fig-cap: Simple bootstrap with time series data.
#| layout-ncol: 2

tide_dat = CSV.read(joinpath("data", "surge", "norfolk-hourly-surge-2015.csv"), DataFrame)
surge_resids = tide_dat[:, 5] - tide_dat[:, 3]

p1 = plot(surge_resids, xlabel="Hour", ylabel="(m)", title="Tide Gauge Residuals", label=:false, linewidth=3)
plot!(p1, size=(600, 450))

resample_index = sample(1:length(surge_resids), length(surge_resids); replace=true)
p2 = plot(surge_resids[resample_index], xlabel="Hour", ylabel="(m)", title="Tide Gauge Resample", label=:false, linewidth=3)
plot!(p2, size=(600, 450))

display(p1)
display(p2)
```

## Block Bootstraps

Clever idea from @Kunsch1989-ll: Divide time series $y_{1:T}$ into overlapping blocks of length $k$.

$$\{y_{1:k}, y_{2:k+1}, \ldots y_{n-k+1:n}\}$$

Then draw $m = n / k$ of these blocks with replacement and construct replicate time series:

$$\hat{y}_{1:n} = (y_{b_1}, \ldots, y_{b_m}) $$

**Note**: Your series must not have a trend!

## Block Bootstrap Example

:::: {.columns}
::: {.column width=50%}

```{julia}
#| echo: true
#| code-fold: true

k = 20
n_blocks = length(surge_resids) - k + 1
blocks = zeros(Float64, (k, n_blocks))
for i = 1:n_blocks
    blocks[:, i] = surge_resids[i:(k+i-1)]
end
blocks[:, 1:5]
```
:::
::: {.column width=50%}
```{julia}
#| echo: true
#| code-fold: true
#| label: fig-ts-block-replicate
#| fig-cap: Comparison of original data with block bootstrap replicate.

m = Int64(ceil(length(surge_resids) / k))
n_boot = 1_000
surge_bootstrap = zeros(length(surge_resids), n_boot)
for i = 1:n_boot
    block_sample_idx = sample(1:n_blocks, m; replace=true)
    surge_bootstrap[:, i] = reduce(vcat, blocks[:, block_sample_idx])
end

p = plot(surge_resids, color=:black, lw=3, label="Data", xlabel="Hour", ylabel="(m)", title="Tide Gauge Residuals", alpha=0.5)
plot!(p, surge_bootstrap[:, 1], color=:blue, lw=3, label="Replicate", alpha=0.5)
plot!(p, size=(600, 500))
```
:::
::::

## Block Bootstrap Replicates

```{julia}
#| echo: true
#| code-fold: true
#| label: fig-ts-block-bootstrap
#| fig-cap: Block bootstrap with time series data.

p = plot(xlabel="Hour", ylabel="(m)", title="Tide Gauge Residuals")
for i = 1:10
    label = i == 1 ? "Replicate" : false
    plot!(p, surge_bootstrap[:, i], label=label, color=:gray, alpha=0.2, lw=2)
end
plot!(p, surge_resids, label="Data", color=:black, lw=3)
plot!(p, size=(1200, 500))
```

## Generalizing the Block Bootstrap

- **Circular Bootstrap**: "Wrap" the time series in a circle, $y_1, y_2, \ldots, y_n, y_1, y_2, \ldots$ then divide into blocks and resample.
- **Stationary Bootstrap**: Use random block lengths to avoid systematic boundary transitions.
- **Block of Blocks Bootstrap**: Divide series into blocks of length $k_2$, then subdivide into blocks of length $k_1$. Sample blocks with replacement then sample sub-blocks within each block with replacement.

# Key Points and Upcoming Schedule

## Key Points

- Bootstrap: Approximate sampling distribution by re-simulating data
- Non-Parametric Bootstrap: Treat data as representative of population and re-sample.
- More complicated for structured data: block bootstrap for time series, analogues for spatial data.

## Sources of Non-Parametric Bootstrap Error

1. **Sampling error**: error from using finitely many replications
2. **Statistical error**: error in the bootstrap sampling distribution approximation

## When To Use The Non-Parametric Bootstrap

- Sample is representative of the sampling distribution
- Doesn't work well for extreme values!
- Does not work *at all* for max/mins (or any other case where the CLT fails).


## Next Classes

**Monday**: The Parametric Bootstrap

**Wednesday**: What is a Markov Chain?

## Assessments

**Homework 3**: Due next Friday (3/14)

**Project Proposals**: Due 3/21.

# References

## References (Scroll for Full List)
