---
title: "Predictive Model Assessments"
subtitle: "Lecture 16"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "March 19, 2024"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: mathjax
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        mermaid: 
            theme: dark
engine: julia
filters:
  - code-fullscreen
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using DataFrames
using DataFramesMeta
using CSV
using Dates
using Distributions
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using Optim
using LaTeXStrings
using Measures

Random.seed!(1)
ENV["GKSwstype"] = "nul"

plot_font = "Computer Modern"
default(
    fontfamily=plot_font,
    linewidth=3, 
    framestyle=:box, 
    label=nothing, 
    grid=false,
    guidefontsize=18,
    legendfontsize=16,
    tickfontsize=16,
    titlefontsize=20,
    bottom_margin=10mm,
    left_margin=5mm
)
```

# Last Class

## Markov Chain Monte Carlo

- Family of methods for simulating from hard-to-sample from distributions $\pi$;
- Rely on ergodic Markov chains for simulation;
- By construction, chains converge to **limiting distribution** which is the target distribution $\pi$.

## Markov Chain Monte Carlo (Convergence)

- Assessment of convergence relies on heuristics.
- Examples: Stabilility of distribution, $\hat{R}$.
- Poor mixing can result in slow convergence; look at ESS.
- **Folk Theorem of Statistical Computing** (Gelman): Poor performance likely means something is wrong with your model.

# Bias-Variance Tradeoff

## Impact of Increasing Model Complexity

```{julia}
#| label: fig-true-data
#| fig-cap: True data and the data-generating curve.
#| echo: true
#| code-fold: true
#| warning: false
#| error: false

ntrain = 20
x = rand(Uniform(-2, 2), ntrain)
f(x) = x.^3 .- 5x.^2 .+ 1
y = f(x) + rand(Normal(0, 2), length(x))
p0 = scatter(x, y, label="Data", markersize=5)
xrange = -2:0.01:2
plot!(p0, xrange, f.(xrange), lw=3, color=:gray, label="True Curve")
plot!(p0, size=(1000, 450))
```

## Impact of Increasing Model Complexity

```{julia}
#| label: fig-true-data-1
#| fig-cap: Impact of increasing model complexity on model fits
#| echo: true
#| code-fold: true
#| layout-ncol: 2
#| warning: false
#| error: false

function polyfit(d, x, y)
    function m(d, θ, x)
        mout = zeros(length(x), d + 1)
        for j in eachindex(x)
            for i = 0:d
                mout[j, i + 1] = θ[i + 1] * x[j]^i
            end
        end
        return sum(mout; dims=2)
    end
    θ₀ = [zeros(d+1); 1.0]
    lb = [-10.0 .+ zeros(d+1); 0.01]
    ub = [10.0 .+ zeros(d+1); 20.0]
    optim_out = optimize(θ -> -sum(logpdf.(Normal.(m(d, θ[1:end-1], x), θ[end]), y)), lb, ub, θ₀)
    θmin = optim_out.minimizer
    mfit(x) = sum([θmin[i + 1] * x^i for i in 0:d])
    return (mfit, θmin[end])
end

function plot_polyfit(d, x, y)
    m, σ = polyfit(d, x, y)
    p = scatter(x, y, label="Data", markersize=5, ylabel=L"$y$", xlabel=L"$x$", title="Degree $d")
    plot!(p, xrange, m.(xrange), ribbon = 1.96 * σ, fillalpha=0.2, lw=3, label="Fit")
    ylims!(p, (-30, 15))
    plot!(p, size=(600, 450))
    return p
end

p1 = plot_polyfit(1, x, y)
p2 = plot_polyfit(2, x, y)

display(p1)
display(p2)
```

## Impact of Increasing Model Complexity

```{julia}
#| label: fig-true-data-2
#| fig-cap: Impact of increasing model complexity on model fits
#| echo: true
#| code-fold: true
#| layout-ncol: 2
#| warning: false
#| error: false

p3 = plot_polyfit(3, x, y)
p4 = plot_polyfit(4, x, y)

display(p3)
display(p4)
```

## Impact of Increasing Model Complexity

```{julia}
#| label: fig-true-data-3
#| fig-cap: Impact of increasing model complexity on model fits
#| echo: true
#| code-fold: true
#| layout-ncol: 2
#| warning: false
#| error: false

p5 = plot_polyfit(6, x, y)
p6 = plot_polyfit(10, x, y)

display(p5)
display(p6)
```

## What Is Happening?

We can think of a model as a form of **data compression**.

Instead of storing coordinates of individual points, project onto parameters of functional form.

The degree to which we can "tune" the model by adjusting parameters are called the **model degrees of freedom** (DOF), which is one measure of model complexity.

## Implications of Model DOF

Higher DOF &Rightarrow; more ability to represent complex patterns.

::: {.fragment .fade-in}
If DOF is too low, the model can't capture meaningful data-generating signals (**underfitting**).
:::

::: {.fragment .fade-in}
But if DOF is too high, the model will "learn" the noise rather than the signal, resulting in poor generalization (**overfitting**).
:::

## In- Vs. Out-Of-Sample Error

```{julia}
#| label: fig-sample-error
#| fig-cap: Impact of increasing model complexity on in and out of sample error
#| echo: true
#| code-fold: true
#| warning: false
#| error: false

ntest = 20
xtest = rand(Uniform(-2, 2), ntest)
ytest = f(xtest) + rand(Normal(0, 2), length(xtest))

in_error = zeros(11)
out_error = zeros(11)
for d = 0:10
    m, σ = polyfit(d, x, y)
    in_error[d+1] = mean((m.(x) .- y).^2)
    out_error[d+1] = mean((m.(xtest) .- ytest).^2)
end

plot(0:10, in_error, markersize=5, color=:blue, lw=3, label="In-Sample Error", xlabel="Polynomial Degree", ylabel="Mean Squared Error", legend=:topleft)
plot!(0:10, out_error, markersize=5, color=:red, lw=3, label="Out-of-Sample Error")
plot!(yaxis=:log)
```

## Why Is Overfitting A Problem?

Example from *The Signal and the Noise* by Nate Silver:

- Engineers at Fukushima used a non-linear regression on historical earthquake data (theory suggests a linear model).
- This model predicted a >9 Richter earthquake would only happen once every 13,000 years; engineers therefore designed nuclear plant to withstand an 8.6 Richter earthquake.
- Theoretical linear relationship suggests that a >9 Richter earthquake would happen every 300 years.

## Bias vs. Variance

The difference between low and high DOFs can be formalized using **bias** and **variance**.

Suppose we have a data-generating model $$y = f(x) + \varepsilon, \varepsilon \sim N(0, \sigma).$$ We want to fit a model $\hat{y} \approx \hat{f}(x)$.

## Bias

**Bias** is error from mismatches between the model predictions and the data ($\text{Bias}[\hat{f}] = \mathbb{E}[\hat{f}] - y$).

Bias comes from under-fitting meaningful relationships between inputs and outputs:

- too few degrees of freedom ("too simple")
- neglected processes.

## Variance

**Variance** is error from over-sensitivity to small fluctuations in training inputs $D$ ($\text{Variance} = \text{Var}_D(\hat{f}(x; D)$).

Variance can come from over-fitting noise in the data:

- too many degrees of freedom ("too complex")
- poor identifiability

## "Bias-Variance Tradeoff"

Can decompose MSE into bias and variance terms:

$$
\begin{align*}
\text{MSE} &= \mathbb{E}[y - \hat{f}^2] \\
&= \mathbb{E}[y^2 - 2y\hat{f}(x) + \hat{f}^2] \\
&= \mathbb{E}[y^2] - 2\mathbb{E}[y\hat{f}] + E[\hat{f}^2] \\
&= \mathbb{E}[(f + \varepsilon)^2] - \mathbb{E}[(f + \varepsilon)\hat{f}] + E[\hat{f}^2] \\
&= \vdots \\
&= \text{Bias}(\hat{f})^2 + \text{Var}(\hat{f}) + \sigma^2
\end{align*}
$$

## "Bias-Variance Tradeoff"

This means that **for a fixed error level**, you can reduce bias (increasing model complexity) or decrease variance (simplifying model) but one comes at the cost of the other.

This is the so-called "bias-variance tradeoff."

## More General B-V Tradeoff

This decomposition is for MSE, but the **principle holds more generally**.

- Models which perform better "on average" over the training data (low bias) are more likely to overfit (high variance);
- Models which have less uncertainty for training data (low variance) are more likely to do worse "on average" (high bias).

## Common Story: Complexity and Bias-Variance

::: {.center}
![Bias-Variance Tradeoff](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Bias_and_variance_contributing_to_total_error.svg/1920px-Bias_and_variance_contributing_to_total_error.svg.png){width=55%}

::: {.caption}
Source: [Wikipedia](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)
:::
:::


## Is "Bias-Variance Tradeoff" Useful?

::: {.incremental}

- **Tautology**: total error is the sum of bias, variance, and irreducible error.
- But error isn't actually "conserved": changing models changes all three terms.
- **Bias and variance do not directly tell us anything about generalizability** (prediction error is not necessarily monotonic; see *e.g.*  @Belkin2019-xy, @Mei2019-hb).
- Instead, think about **approximation** vs. **estimation** error.
:::


# Measuring Model Skill

## What Are Models For?

- Data summaries;
- Predictors of new data;
- Simulators of counterfactuals.

## Point Prediction Skill: $R^2$

$$R^2 = 1 - (\sigma^2_\text{residuals}/\sigma^2_\text{data})$$ 

- Probably most common, also only meaningful for linear Gaussian models $y \sim N(\beta_0 + \beta_1 x, \sigma^2)$.
- Increasing predictors **always** increases $R^2$, making it useless for model selection. 
- Can adjust for increasing complexity, substitute $$R^2_\text{adj} = 1 - ((n / (n-2)) \sigma^2_\text{residuals} / \sigma^2_\text{data})$$

## Myths and Problems With $R^2$

1. It (and its adjusted value) doesn't measure goodness of fit! If we knew the "true" regression slope $\beta_1$, not hard to derive $$R^2 = \frac{\beta_1^2 \text{Var}(x)}{\beta_1^2 \text{Var}(x) + \sigma^2}.$$ This can be made arbitrarily small if $\text{Var}(x)$ is small or $\sigma^2$ is large **even if the model is true**. And when the model is wrong, $R^2$ can be made arbitrarily large.

## Myths and Problems With $R^2$

2.  It says nothing about prediction error.
3.  Cannot be compared across different datasets (since it's impacted by $\text{Var}(x)$).
4.  Is not preserved under transformations.
5.  It doesn't **explain** anything about variance: $x$ regressed on $y$ gives the same $R^2$ as $y$ regressed on $x$ (measure of correlation).


## Anscombe's Quartet Illustrates Problems With $R^2$

:::: {.columns}
::: {.column width=40%}
Anscombe's Quartet [@Anscombe1973-pp] consists of datasets which have the same summary statistics (including $R^2$) but very different graphs.
:::
::: {.column width=60%}

![Anscombe's Quartet](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Anscombe%27s_quartet_3.svg/1920px-Anscombe%27s_quartet_3.svg.png){width=80%}

::: {.caption}
Source: [Wikipedia](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)
:::
:::
::::

## Alternatives to $R^2$

In pretty much every case where $R^2$ *might* be useful, mean squared error (MSE) is better. (To be honest, I have never, ever, ever found a use where $R^2$ was valuable). 

More generally, we want to think about measures which capture predictive fit given a **probabilistic forecast**.

These are commonly called **scoring rules** [@Gneiting2014-wh].

## What Makes A Good Prediction?

What do we want to see in a probabilistic projection $F$? 

::: {.fragment .fade-in}

- **Calibration**: Does the predicted CDF $F(y)$ align with the "true" distribution of observations $y$? 
  $$\mathbb{P}(y \leq F^{-1}(\tau)) = \tau \qquad \forall \tau \in [0, 1]$$
- **Dispersion**: Is the concentration (variance) of $F$ aligned with the concentration of observations?
- **Sharpness**: How concentrated are the forecasts $F$?
:::

## Probability Integral Transform (PIT)

Common to use the PIT to make these more concrete: $Z_F = F(y)$.

The forecast is **probabilistically calibrated** if $Z_F \sim Uniform(0, 1)$.

The forecast is **properly dispersed** if $\text{Var}(Z_F) = 1/12$.

**Sharpness** can be measured by the width of a particular prediction interval. **A good forecast is a sharp as possible subject to calibration** [@Gneiting2007-my].

## PIT Example: Well-Calibrated

```{julia}
#| label: fig-pit-2
#| fig-cap: Comparison of "proper" and overdispersed PIT.
#| echo: true
#| code-fold: true
#| layout-ncol: 2

# "true" observation distribution is N(2, 0.5)
obs = rand(Normal(2, 0.5), 50)
# forecast according to the "correct" distribution and obtain PIT
pit_corr = cdf(Normal(2, 0.45), obs)
p_corr = histogram(pit_corr, bins=10, label=false, xlabel=L"$y$", ylabel="Count", size=(500, 500))

xrange = 0:0.01:5
p_cdf1 = plot(xrange, cdf.(Normal(2, 0.4), xrange), xlabel=L"$y$", ylabel="Cumulative Density", label="Forecast", size=(500, 500))
plot!(p_cdf1, xrange, cdf.(Normal(2, 0.5), xrange), label="Truth")

display(p_cdf1)
display(p_corr)
```

## PIT Example: Underdispersed

```{julia}
#| label: fig-pit-3
#| fig-cap: Comparison of "proper" and overdispersed PIT.
#| echo: true
#| code-fold: true
#| layout-ncol: 2

# forecast according to an underdispersed distribution and obtain PIT
pit_under = cdf(Normal(2, 0.1), obs)
p_under = histogram(pit_under, bins=10, label=false, xlabel=L"$y$", ylabel="Count", size=(500, 500))

xrange = 0:0.01:5
p_cdf2 = plot(xrange, cdf.(Normal(2, 0.1), xrange), xlabel=L"$y$", ylabel="Cumulative Density", label="Forecast", size=(500, 500))
plot!(p_cdf2, xrange, cdf.(Normal(2, 0.5), xrange), label="Truth")

display(p_cdf2)
display(p_under)
```


## PIT Example: Overdispersed

```{julia}
#| label: fig-pit-1
#| fig-cap: Comparison of "proper" and overdispersed PIT.
#| echo: true
#| code-fold: true
#| layout-ncol: 2

# forecast according to an overdispersed distribution and obtain PIT
pit_over = cdf(Normal(2, 1), obs)
p_over = histogram(pit_over, bins=10, label=false, xlabel=L"$y$", ylabel="Count", size=(500, 500))

xrange = 0:0.01:5
p_cdf3 = plot(xrange, cdf.(Normal(2, 1), xrange), xlabel=L"$y$", ylabel="Cumulative Density", label="Forecast", size=(500, 500))
plot!(p_cdf3, xrange, cdf.(Normal(2, 0.5), xrange), label="Truth")

display(p_cdf3)
display(p_over)
```


## Scoring Rules

A scoring rule $S(F, y)$ measures the "loss" of a predicted probability distribution $F$ once an observation $y$ is obtained. 

Typically oriented so smaller = better.

## Scoring Rule Examples


1. Logarithmic: $S(F, y) = -\log F(y)$
2. Quadratic (Brier): $S(F, y) = -2F(y) - \int_{-\infty}^\infty F^2(z) dz$
3. Continous Ranked Probability Score (CRPS): 
   $$\begin{align*}
   S(F, y) &= -\int (F(z) - \mathbb{I}(y \leq z))^2 dz \\
   &= \mathbb{E}_F |Y -y| - \frac{1}{2} E_F | Y - Y'|
   \end{align*}$$

## Proper Scoring Rules

**Proper** scoring rules are intended to encourage forecasters to provide their full (and honest) forecasts.

Minimized when the forecasted distribution matches the observed distribution:

$$\mathbb{E}_Y(S(G, G)) \leq \mathbb{E}_Y(S(F, G)) \qquad \forall F.$$

It is **strictly proper** if equality holds only if $F = G$.

## Logarithmic Score As Scoring Rule

The **logarithmic score** $S(F, y) = -\log F(y)$ is (up to equivalence) the only **local** strictly proper scoring rule (locality &Rightarrow; score depends only on the observation).

This is the negative log-probability: straightforward to use for the likelihood (frequentist forecasts) or posterior (Bayesian forecasts).

We will focus on the logarithmic score as our scoring function going forward, but everything will hold with other scores.

## Important Caveat

**A model can predict well without being "correct"!**

For example, model selection (next two class weeks) using predictive criteria **does not mean** you are selecting the "true" model.

**The causes of the data cannot be found in the data alone.**

# Cross-Validation

## Can We Drive Model Error to Zero?

Effectively, no. **Why**?

::: {.fragment .fade-in}

- Inherent noise: even a perfect model wouldn't perfectly predict observations.
- Model mis-specification (the cause of bias)
- Model estimation is never "right" (the cause of variance) 

:::

## Quantifying Generalization Error

The goal is then to minimize the generalized (expected) error:

$$\mathbb{E}\left[L(X, \theta)\right] = \int_X L(x, \theta) \pi(x)dx$$

where $L(x, \theta)$ is an error function capturing the discrepancy between $\hat{f}(x, \theta)$ and $y$.

## In-Sample Error

Since we don't know the "true" distribution of $y$, we could try to approximate it using the training data:

$$\hat{L} = \min_{\theta \in \Theta} L(x_n, \theta)$$

But: **This is minimizing in-sample error and is likely to result an optimistic score.**

## Held Out Data

Instead, let's divide our data into a training dataset $y_k$ and testing dataset $\tilde{y}_l$.

1. Fit the model to $y_k$;
2. Evaluate error on $\tilde{y}_l$.

This results in an unbiased estimate of $\hat{L}$ but is noisy.

## $k$-Fold Cross-Validation

What if we repeated this procedure for multiple held-out sets?

1. Randomly split data into $k = n / m$ equally-sized subsets.
2. For each $i = 1, \ldots, k$, fit model to $y_{-i}$ and test on $y_i$.

If data are large, this is a good approximation.

## Leave-One-Out Cross-Validation (LOOCV)

The problem with $k$-fold CV, when data is scarce, is withholding $n/k$ points.

**LOO-CV**: Set $k=n$

**The trouble**: estimates of $L$ are highly correlated since every two datasets share $n-2$ points.

**The benefit**: LOO-CV approximates seeing "the next datum".

## LOO-CV Algorithm

1. Drop one value $y_i$.
2. Refit model on rest of data $y_{-i}$.
3. Evaluate $L(y_i | y_{-i})$.
4. Repeat on rest of data set.

# Key Points and Upcoming Schedule

## Key Points (Bias)

- Bias: Discrepancy between expected model output and observations. 
- Related to approximation error and underfitting.
- "Simpler" models tend to have higher bias.
- High bias suggests less sensitivity to specifics of dataset.

## Key Points (Variance)

- Variance: How much the model predictions vary around the mean. 
- Related to estimation error and overfitting.
- "More complex" models tend to have higher variance.
- High variance suggests high sensitivity to specifics of dataset.

## Key Points (Bias-Variance Tradeoff)

- Error can be decomposed into bias, variance, and irreducible error.
- "Tradeoff" reflects this decomposition (but it's not really a tradeoff).
- Useful conceptual to think about the balance between not picking up on meaningful signals (underfitting) and modeling noise (overfitting).

## Key Points (Model Skill)

- $R^2$ is not a good measure of point prediction skill even for linear models! Use MSE instead.
- Probabilistic forecasts should be assessed based on both calibration and sharpness.
- Scoring rules as measures of probabilistic forecast skill.
- Logarithmic score (negative log-probability) is the unique locally proper scoring rule.

## Next Classes

**Next Week**: Model Comparison Methods: Cross-Validation and Information Criteria

## Assessments

**Project Proposal**: Due Friday.

**HW4**: Will release this week, due 4/11 (after break).

**Literature Critique**: Talk to Prof. Srikrishnan if you want help finding a paper.

**No quiz this week so you can focus on your project proposal**.

# References

## Refernences (Scroll for Full List)