---
title: "Predictive Model Assessment"
subtitle: "Lecture 18"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "April 17, 2024"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: 
            method: mathjax
            url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        touch: false
        controls: true
execute:
    freeze: auto
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using DataFrames
using DataFramesMeta
using CSV
using Dates
using Distributions
using Turing
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using LaTeXStrings
using Measures
using Optim

Random.seed!(1)
```

# Review of Last Class

## Simulation as Hypothesis Testing

::: {.incremental}
- Define models for hypotheses (including baseline/null).
- Fit models to data.
- Simulate alternative datasets.
- Compare output/test statistics to data.
- *Maybe*: Convert metrics to probabilities (condition on $\mathcal{M}$).
:::

## Metrics for Model Adequacy/Performance

- Explanatory Metrics
  - Based on same training/calibration data
  - RMSE, R^2^, $\log p(y | M)$
- **Predictive Metrics**
  - Held-out data/cross-validation

# Bias-Variance Tradeoff

## What Is The Goal of Model Selection?

**Key Idea**: Model selection consists of navigating the bias-variance tradeoff.

Model error (*e.g.* RMSE) is a combination of *irreducible error*, *bias*, and *variance*.

## Setting for Model Selection

Suppose we have a data-generating model $$y = f(x) + \varepsilon, \varepsilon \sim N(0, \sigma)$$. We want to fit a model $\hat{y} \approx \hat{f}(x)$.

## Bias

**Bias** is error from mismatches between the model predictions and the data ($\text{Bias}[\hat{f}] = \mathbb{E}[\hat{f}] - y$).

Bias comes from under-fitting meaningful relationships between inputs and outputs.

- too few degrees of freedom ("too simple")
- neglected processes.

## Variance

**Variance** is error from over-sensitivity to small fluctuations in inputs ($\text{Variance} = \text{Var}(\hat{f})$).

Variance can come from over-fitting noise in the data.

- too many degrees of freedom ("too complex")
- poor identifiability

## Decomposition of MSE
$$
\begin{align*}
\text{MSE} &= \mathbb{E}[y - \hat{f}^2] \\
&= \mathbb{E}[y^2 - 2y\hat{f}(x) + \hat{f}^2] \\
&= \mathbb{E}[y^2] - 2\mathbb{E}[y\hat{f}] + E[\hat{f}^2] \\
&= \mathbb{E}[(f + \varepsilon)^2] - \mathbb{E}[(f + \varepsilon)\hat{f}] + E[\hat{f}^2] \\
&= \vdots \\
&= \text{Bias}(\hat{f})^2 + \text{Var}(\hat{f}) + \sigma^2
\end{align*}
$$

## Bias-Variance Tradeoff

**Upshot**: For achieve a fixed error level, you can reduce bias (more "complex" model) or you can reduce variance (more "simple" model) but there's a tradeoff.

## Bias-Variance Tradeoff More Generally 

This decomposition is for MSE, but the **principle holds more generally**.

- Models which perform better "on average" over the training data (low bias) are more likely to overfit (high variance);
- Models which have less uncertainty for training data (low variance) will do worse "on average".

## Model Complexity

Model complexity is *not* necessarily the same as the number of parameters.

Sometimes processes in the model can compensate for each other ("**reducing degrees of freedom**")

This can help improve the representation of the dynamics and reduce error/uncertainty even when additional parameters are included.

## Bias-Variance and Complexity

::: {.center}
![Bias-Variance Tradeoff](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Bias_and_variance_contributing_to_total_error.svg/1920px-Bias_and_variance_contributing_to_total_error.svg.png){width=60%}

::: {.caption}
Source: [Wikipedia](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)
:::
:::


## Occam's Razor

::: {.quote}
> Entities are not to be multiplied without necessity.

::: {.cite}
Credited to William of Ockham, appears much earlier in the works of Maimonides, Ptolemy, and Aristotle, first formulated as such by John Punch (1639)
:::
:::

## "Zebra Principle"

More colloquially:

::: {.quote}
> When you hear hoofbeats, think of horses, not zebras.

::: {.cite}
--- Theodore Woodward
:::
:::

# Cross-Validation and Predictive Fit

## Log-Likelihood as Predictive Fit Measure

The measure of predictive fit that we will use:

The **log predictive density** or **log-likelihood** of a replicated data point/set, $$p(y^{rep} | \theta).$$ 

## Why Use Log-Likelihood?

Why use the log-likelihood density instead of the log-posterior? 

- The likelihood captures the data-generating process; 
- The posterior includes the prior, which is only relevant for parameter estimation.

**Important**: This means that the prior is still relevant in predictive model assessment, and should be thought of as part of the model structure!

## Cross-Validation

The "gold standard" way to test for predictive performance is **cross-validation**:

1. Split data into training/testing sets;
2. Calibrate model to training set;
3. Check for predictive ability on testing set.

## Leave-One-Out Cross-Validation

1. Drop one value $y_i$.
2. Refit model on rest of data $y_{-i}$.
3. Evaluate $\log p(y_i | y_{-i})$.
4. Repeat on rest of data set.

## LOO-CV Example

```{julia}
#| output: false
#| echo: false

# load SF tide gauge data
# read in data and get annual maxima
function load_data(fname)
    date_format = DateFormat("yyyy-mm-dd HH:MM:SS")
    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data
    df = @chain fname begin
        CSV.read(DataFrame; header=false)
        rename("Column1" => "year", "Column2" => "month", "Column3" => "day", "Column4" => "hour", "Column5" => "gauge")
        # need to reformat the decimal date in the data file
        @transform :datetime = DateTime.(:year, :month, :day, :hour)
        # replace -99999 with missing
        @transform :gauge = ifelse.(abs.(:gauge) .>= 9999, missing, :gauge)
        select(:datetime, :gauge)
    end
    return df
end

dat = load_data("data/surge/h551.csv")

# detrend the data to remove the effects of sea-level rise and seasonal dynamics
ma_length = 366
ma_offset = Int(floor(ma_length/2))
moving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]
dat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))

# group data by year and compute the annual maxima
dat_ma = dropmissing(dat_ma) # drop missing data
dat_annmax = combine(dat_ma -> dat_ma[argmax(dat_ma.residual), :], groupby(DataFrames.transform(dat_ma, :datetime => x->year.(x)), :datetime_function))
delete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet
rename!(dat_annmax, :datetime_function => :Year)
select!(dat_annmax, [:Year, :residual])
```

```{julia}
#| output: true
#| echo: true
#| code-fold: true
#| label: fig-loo-cv
#| 

drop_idx = rand(1:nrow(dat_annmax), 1)
p1 = plot(
    dat_annmax.Year[setdiff(1:end, drop_idx)],
    dat_annmax.residual[setdiff(1:end, drop_idx)];
    xlabel="Year",
    ylabel="Annual Max Tide Level (m)",
    label=false,
    marker=:circle,
    markersize=5,
    tickfontsize=16,
    guidefontsize=18,
    left_margin=5mm, 
    bottom_margin=5mm
)
scatter!(p1,
    dat_annmax.Year[drop_idx],
    dat_annmax.residual[drop_idx],
    color=:red,
    label=false,
    markersize=5,
    marker=:circle
)
```

## LOO-CV Example

:::: {.columns}
::: {.column width=60%}
```{julia}
#| output: true
#| echo: true
#| code-fold: true
#| label: fig-loo-cv-fit
#| fig-cap: MLE for GEV fit without held out data point.

# fit held-out data
stat_lb = [1000.0, 0.0, -1.0]
stat_ub = [2000.0, 100.0, 1.0]
stat_p0 = [1200.0, 50.0, 0.01]
nonstat_lb = [1000.0, -20.0, 0.0, -5.0]
nonstat_ub = [2000.0, 20.0, 100.0, 5.0]
nonstat_p0 = [1200.0, 0.5, 50.0, 0.05]

stat_cv = Optim.optimize(p -> -sum(logpdf.(GeneralizedExtremeValue(p[1], p[2], p[3]), dat_annmax.residual[setdiff(1:end, drop_idx)])), stat_lb, stat_ub, stat_p0)
stat_cv_mle = stat_cv.minimizer
stat_cv_logp = logpdf(GeneralizedExtremeValue(stat_cv_mle[1], stat_cv_mle[2], stat_cv_mle[3]), dat_annmax.residual[drop_idx])

p = plot(GeneralizedExtremeValue(stat_cv_mle[1], stat_cv_mle[2], stat_cv_mle[3]),
    xlabel="Storm Surge Extreme (mm)",
    ylabel="Probability Density",
    xlims=(1000, 1800),
    linewidth=3,
    label=false,
    tickfontsize=16,
    guidefontsize=18,
    legendfontsize=16,
    left_margin=5mm, 
    bottom_margin=5mm,
    right_margin=10mm
)
vline!(p, [dat_annmax.residual[drop_idx]], color=:red, linestyle=:dash, label="Held-out Value", linewidth=2)
plot!(p, size=(600, 400))
```
:::
::: {.column width=40%}
$\log p(y_i | y_{-i})$: 

`{julia} round(stat_cv_logp[1]; digits=1)`.
:::
::::

## LOO-CV Example

```{julia}
stat_cv_lp = zeros(nrow(dat_annmax))
for i = 1:nrow(dat_annmax)
    stat_cv = Optim.optimize(p -> -sum(logpdf.(GeneralizedExtremeValue(p[1], p[2], p[3]), dat_annmax.residual[setdiff(1:end, i)])), stat_lb, stat_ub, stat_p0).minimizer
    stat_cv_lp[i] = logpdf(GeneralizedExtremeValue(stat_cv[1], stat_cv[2], stat_cv[3]), dat_annmax.residual[i])
end

nonstat_cv_lp = zeros(nrow(dat_annmax))
for i = 1:nrow(dat_annmax)
    nonstat_cv = Optim.optimize(p -> -sum(logpdf.(GeneralizedExtremeValue.(p[1] .+ p[2] * setdiff(1:nrow(dat_annmax), i), p[3], p[4]), dat_annmax.residual[setdiff(1:end, i)])), nonstat_lb, nonstat_ub, nonstat_p0).minimizer
    nonstat_cv_lp[i] = logpdf(GeneralizedExtremeValue(nonstat_cv[1] +  nonstat_cv[2] * i, nonstat_cv[3], nonstat_cv[4]), dat_annmax.residual[i])
end

DataFrame(Model=["Stationary", "Nonstationary"], LOOCV=[sum(stat_cv_lp), sum(nonstat_cv_lp)])
```

## Bayesian LOO-CV

By default, Bayesian LOO-CV is extremely expensive:

$$\text{loo-cv} = \sum_{i=1}^n \log p_{\text{post}(-i)}(y_i),$$

which requires refitting the model without $y_i$ for every data point.


## Leave-$k$-Out Cross-Validation

Drop $k$ values, refit model on rest of data, check for predictive skill.

As $k \to n$, this reduces to the prior predictive distribution
$$p(y^{\text{rep}}) = \int_{\theta} p(y^{\text{rep}} | \theta) p(\theta) d\theta.$$

## Challenges with Cross-Validation

::: {.incremental}
- This can be very computationally expensive!
- We often don't have a lot of data for calibration, so holding some back can be a problem.
- How to divide data with spatial or temporal structure? This can be addressed by partitioning the data more cleverly:
  $$y = \{y_{1:t}, y_{-((t+1):T)}\}$$
  but this makes the data problem worse.
::: 

## Expected Out-Of-Sample Predictive Accuracy

The out-of-sample predictive fit of a new data point $\tilde{y}_i$ is

$$
\begin{align}
\log p_\text{post}(\tilde{y}_i) &= \log \mathbb{E}_\text{post}\left[p(\tilde{y}_i | \theta)\right] \\
&= \log \int p(\tilde{y_i} | \theta) p_\text{post}(\theta)\,d\theta.
\end{align}
$$

## Expected Out-Of-Sample Predictive Accuracy

However, the out-of-sample data $\tilde{y}_i$ is itself unknown, so we need to compute the *expected out-of-sample log-predictive density*

$$
\begin{align}
\text{elpd} &= \text{expected log-predictive density for } \tilde{y}_i \\
&= \mathbb{E}_P \left[\log p_\text{post}(\tilde{y}_i)\right] \\
&= \int \log\left(p_\text{post}(\tilde{y}_i)\right) P(\tilde{y}_i)\,d\tilde{y}.
\end{align}
$$


## Expected Out-Of-Sample Predictive Accuracy

What is the challenge?

::: {.fragment .fade-in}
We don't know $P$ (the distribution of new data)!

We need some measure of the error induced by using an approximating distribution $Q$ from some model.
:::

# Information Criteria

## Information Criteria

"Information criteria" refers to a category of estimators of prediction error.

The idea: estimate predictive error using the fitted model.

## Information Criteria Overview

There is a common framework for all of these:

If we compute the expected log-predictive density for the existing data $p(y | \theta)$, this will be too good of a fit and will overestimate the predictive skill for new data.

## Information Criteria Corrections

We can adjust for that bias by correcting for the *effective number of parameters*, which can be thought of as the expected degrees of freedom in a model contributing to overfitting.



## Akaike Information Criterion (AIC)

The "first" information criterion that most people see.

Uses a point estimate (the maximum-likelihood estimate $\hat{\theta}_\text{MLE}$) to compute the log-predictive density for the data, corrected by the number of parameters $k$:

$$\widehat{\text{elpd}}_\text{AIC} = \log p(y | \hat{\theta}_\text{MLE}) - k.$$

## AIC Formula

The AIC is defined as $-2\widehat{\text{elpd}}_\text{AIC}$.

Due to this convention, lower AICs are better (they correspond to a higher predictive skill).

## AIC Correction Term

In the case of a normal model with independent and identically-distributed data and uniform priors, $k$ is the asymptotically "correct" bias term (there are modified corrections for small sample sizes).

However, with more informative priors and/or hierarchical models, the bias correction $k$ is no longer appropriate, as there is less "freedom" associated with each parameter.

## AIC: Storm Surge Example

```{julia}
#| output: false
#| echo: false

function load_pdo(fname)
    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data
    df = CSV.read(fname, DataFrame, delim=" ", ignorerepeated=true, header=2)
    # take yearly average
    @transform!(df, :PDO = mean(AsTable(names(df)[2:13])))
    @select!(df, $[:Year, :PDO])
    @rsubset!(df, :Year != 2023)
    return df
end

pdo = load_pdo("data/surge/ersst.v5.pdo.dat")
# subset for years that match the tide gauge data
years = dat_annmax[!, :Year]
@rsubset!(pdo, :Year in years)

@model function sf_stat(y)
    μ ~ truncated(Normal(1500, 200), lower=0)
    σ ~ truncated(Normal(100, 25), lower=0)
    ξ ~ Normal(0, 1)

    for i in 1:length(y)
        y[i] ~ GeneralizedExtremeValue(μ, σ, ξ)
    end
end

stat_mod = sf_stat(dat_annmax.residual)


@model function sf_nonstat(y)
    a ~ truncated(Normal(1500, 200), lower=0)
    b ~ Normal(0, 5)
    σ ~ truncated(Normal(100, 25), lower=0)
    ξ ~ Normal(0, 1)

    T = length(y)
    for i in 1:T
        y[i] ~ GeneralizedExtremeValue.(a .+ b * i, σ, ξ)
    end
end

nonstat_mod = sf_nonstat(dat_annmax.residual)

@model function sf_pdo(y, pdo)
    a ~ truncated(Normal(1500, 200), lower=0)
    b ~ Normal(0, 5)
    σ ~ truncated(Normal(100, 25), lower=0)
    ξ ~ Normal(0, 1)

    for i in 1:length(y)
        y[i] ~ GeneralizedExtremeValue.(a + b * pdo[i], σ, ξ)
    end
end

pdo_mod = sf_pdo(dat_annmax.residual, pdo.PDO)
```

Models:

1. Stationary ("null") model, $y_t \sim \text{GEV}(\mu, \sigma, \xi);$
2. Time nonstationary ("null-ish") model, $y_t \sim \text{GEV}(\mu_0 + \mu_1 t, \sigma, \xi);$
3. PDO nonstationary model, $y_t \sim \text{GEV}(\mu_0 + \mu_1 \text{PDO}_t, \sigma, \xi)$

## AIC Example

```{julia}
#| output: true
#| echo: true
#| code-fold: true

stat_mle = [1258.71, 56.27, 0.017]
stat_ll = -707.67
nonstat_mle = [1231.58, 0.42, 52.07, 0.075]
nonstat_ll = -702.45
pdo_mle = [1255.87, -12.39, 54.73, 0.033]
pdo_ll = -705.24

# compute AIC values
stat_aic = stat_ll - 3
nonstat_aic = nonstat_ll - 4
pdo_aic = pdo_ll - 4

model_aic = DataFrame(Model=["Stationary", "Time", "PDO"], LogLik=trunc.(Int64, round.([stat_ll, nonstat_ll, pdo_ll]; digits=0)), AIC=trunc.(Int64, round.(-2 * [stat_aic, nonstat_aic, pdo_aic]; digits=0)))
```

## AIC Interpretation

Absolute AIC values have **no meaning**, only the differences $\Delta_i = \text{AIC}_i - \text{AIC}_\text{min}$.

Some basic rules of thumb (from @Burnham2004-do):

- $\Delta_i < 2$ means the model has "strong" support across $\mathcal{M}$;
- $4 < \Delta_i < 7$ suggests "less" support;
- $\Delta_i > 10$ suggests "weak" or "no" support.

## AIC and Model Evidence

$\exp(-\Delta_i/2)$ can be thought of as a measure of the likelihood of the model given the data $y$. 

The ratio $$\exp(-\Delta_i/2) / \exp(-\Delta_j/2)$$ can approximate the relative evidence for  $M_i$ versus $M_j$.

## AIC and Model Averaging

This gives rise to the idea of *Akaike weights*:
$$w_i = \frac{\exp(-\Delta_i/2)}{\sum_{m=1}^M \exp(-\Delta_m/2)}.$$

Model projections can then be weighted based on $w_i$, which can be interpreted as the probability that $M_i$ is the best (in the sense of approximating the "true" predictive distribution) model in $\mathcal{M}$.

## Model Averaging vs. Selection

Model averaging can sometimes be beneficial vs. model selection.

Model selection can introduce bias from the selection process (this is particularly acute for stepwise selection due to path-dependence).

## Deviance Information Criterion (DIC)

The Deviance Information Criterion (DIC) is a more Bayesian generalization of AIC which uses the posterior mean 
$$\hat{\theta}_\text{Bayes} = \mathbb{E}\left[\theta | y\right]$$
and a bias correction derived from the data.

## DIC

$$\widehat{\text{elpd}}_\text{DIC} = \log p(y | \hat{\theta}_\text{Bayes}) - p_{\text{DIC}},$$
where
$$p_\text{DIC} = 2\left(\log p(y | \hat{\theta}_\text{Bayes}) - \mathbb{E}_\text{post}\left[\log p(y | \theta)\right]\right).$$

Then, as with AIC, $$\text{DIC} = -2\widehat{\text{elpd}}_\text{DIC}.$$

##  DIC: Effective Number of Parameters

**What is the meaning of $p_\text{DIC}$?**

- The difference between the average log-likelihood (across parameters) and the log-likelihood at a parameter average measures "degrees of freedom".
- The DIC adjustment assumes independence of residuals for fixed $\theta$.

## AIC vs. DIC

AIC and DIC often give similar results, but don't have to. 

The key difference is the impact of priors on parameter estimation and model degrees of freedom.

## AIC vs. DIC Example

```{julia}
#| output: true
#| echo: true
#| code-fold: true

# fit models
stat_chain = sample(stat_mod, NUTS(), MCMCThreads(), 10_000, 4)
nonstat_chain = sample(nonstat_mod, NUTS(), MCMCThreads(), 10_000, 4)
pdo_chain = sample(pdo_mod, NUTS(), MCMCThreads(), 10_000, 4)

stat_est = mean(stat_chain)[:, 2]
nonstat_est = mean(nonstat_chain)[:, 2]
pdo_est = mean(pdo_chain)[:, 2]

stat_bayes = loglikelihood(stat_mod, (μ = stat_est[1], σ = stat_est[2], ξ = stat_est[3]))
nonstat_bayes = loglikelihood(nonstat_mod, (a = nonstat_est[1], b = nonstat_est[2], σ = nonstat_est[3], ξ = nonstat_est[4]))
pdo_bayes = loglikelihood(pdo_mod, (a = pdo_est[1], b = pdo_est[2], σ = pdo_est[3], ξ = pdo_est[4]))

stat_ll = mean([loglikelihood(stat_mod, row) for row in eachrow(DataFrame(stat_chain))]) 
nonstat_ll = mean([loglikelihood(nonstat_mod, row) for row in eachrow(DataFrame(nonstat_chain))])
pdo_ll = mean([loglikelihood(pdo_mod, row) for row in eachrow(DataFrame(pdo_chain))])

stat_dic = 2 * stat_ll - stat_bayes
nonstat_dic = 2 * nonstat_ll - nonstat_bayes
pdo_dic = 2 * pdo_ll - pdo_bayes

model_dic = DataFrame(Model=["Stationary", "Time", "PDO"], AIC=trunc.(Int64, round.(-2 * [stat_aic, nonstat_aic, pdo_aic]; digits=0)), DIC=trunc.(Int64, round.(-2 * [stat_dic, nonstat_dic, pdo_dic]; digits=0)))
```

## Convergence of AIC and DIC

Both AIC and DIC converge (as $n \to \infty$) to expected leave-one-out CV.

The question is how well they do under limited sample sizes!

## Watanabe-Akaike Information Criterion (WAIC)

$$\widehat{\text{elpd}}_\text{WAIC} = \sum_{i=1}^n \log \int p(y_i | \theta) p_\text{post}(\theta)\,d\theta - p_{\text{WAIC}},$$

where
$$p_\text{WAIC} = \sum_{i=1}^n \text{Var}_\text{post}\left(\log p(y_i | \theta)\right).$$

## WAIC Correction Factor

$p_\text{WAIC}$ is an estimate of the number of "unconstrained" parameters in the model.

- A parameter counts as 1 if its estimate is "independent" of the prior;
- A parameter counts as 0 if it is fully constrained by the prior.
- A parameter gives a partial value if both the data and prior are informative.

## WAIC vs. AIC and DIC

- WAIC can be viewed as an approximation to leave-one-out CV, and averages over the entire posterior, vs. AIC and DIC which use point estimates.
- But it doesn't work well with highly structured data; no real alternative to more clever uses of Bayesian cross-validation.

## "Bayesian" "Information" Criterion (BIC)

$$\text{BIC} = -2\log p(y | \hat{\theta}_\text{MLE}) + k\log n.$$

BIC:

- Is not Bayesian (it relies on the MLE);
- Has no relationship to information theory (unlike AIC/DIC);
- Assumes $\mathcal{M}$-closed (e.g. that the true model is under consideration).

## BIC

BIC approximates the *prior* log-predictive likelihood and leave-$k$-out cross-validation (hence the extra penalization for additional parameters).

**This is why it's odd when model selection consists of examining both AIC and BIC: these are different quantities with different purposes!**


# Key Takeaways and Upcoming Schedule

## Key Takeaways

- Model selection is a balance between bias (underfitting) and variance (overfitting).
- For predictive assessment, leave-one-out cross-validation is an ideal, but hard to implement in practice (particularly for time series).

## Key Takeaways

- Information Criteria are an approximation to LOO-CV based on "correcting" for model complexity.
- AIC and DIC can be used to approximate LOO-CV.
- BIC is an entirely different measure, approximating the prior predictive distribution (leave-$k$-out CV).

## An Important Caveat

**Model selection can result in significant overfitting when separated from hypothesis-driven model development** [@Freedman1983-xq; @Smith2018-wt]

## An Important Caveat

- Better off thinking about the scientific or engineering problem you want to solve and use domain knowledge/checks rather than throwing a large number of possible models into the selection machinery.
- Regularizing priors reduce potential for overfitting.
- Model averaging [@Hoeting2021-vx] and stacking [@Yao2018-rr] can combine multiple models as an alternative to selection.


## Next Classes

**Next Week**: Model Simplicity and Emulation

# References

## References

