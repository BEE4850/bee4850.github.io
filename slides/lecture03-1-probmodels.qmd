---
title: "Probability Models"
subtitle: "Lecture 04"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "February 03, 2024"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: mathjax
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        mermaid: 
            theme: dark
engine: julia
filters:
  - code-fullscreen
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using DataFrames
using DataFramesMeta
using CSV
using Dates
using Distributions
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using Optim
using LaTeXStrings
using Measures

Random.seed!(1)

plot_font = "Computer Modern"
default(
    fontfamily=plot_font,
    linewidth=3, 
    framestyle=:box, 
    label=nothing, 
    grid=false,
    guidefontsize=18,
    legendfontsize=16,
    tickfontsize=16,
    titlefontsize=20,
    bottom_margin=10mm,
    left_margin=5mm
)
```

# Review

## Probability Fundamentals

- Bayesian vs. Frequentist Interpretations
- Distributions reflect assumptions on probability of data.
- Normal distributions: "least informative" distribution for a given mean/variance.
- Fit distributions by maximizing likelihood.
- Communicating uncertainty: confidence vs. predictive intervals.

# Linear Regression

## How Does River Flow Affect TDS?

:::: {.columns}
::: {.column width=50%}

**Question**: Does river flow affect the concentration of total dissolved solids?

**Data**: Cuyahoga River (1969 -- 1973), from @Helsel2020-nq [Chapter 9].

:::
::: {.column width=50%}
```{julia}
tds = let
	fname = "data/tds/cuyaTDS.csv" # CHANGE THIS!
	tds = DataFrame(CSV.File(fname))
	tds[!, [:date, :discharge_cms, :tds_mgL]]
end
p = scatter(
	tds.discharge_cms,
	tds.tds_mgL,
	xlabel=L"Discharge (m$^3$/s)",
	ylabel="Total dissolved solids (mg/L)",
    markersize=5,
	label="Observations"
)
plot!(p, size=(600, 600))
```
:::
::::

## How Does TDS Affect River Flow?

:::: {.columns}

::: {.column width=50%}
**Question**: Does river flow affect the concentration of total dissolved solids?

**Model**: 
$$D \rightarrow S \ {\color{purple}\leftarrow U}$$
$$S = f(D, U)$$
:::
::: {.column width=50%}
```{julia}
p
```
:::
::::


## Log-Linear Relationships

:::: {.columns}
::: {.column width=50%}
$$
\begin{align*}
S &= \beta_0 + \beta_1 \log(D) + U\\
U &\sim \mathcal{N}(0, \sigma^2)
\end{align*}
$$

:::
::: {.column width=50%}
```{julia}
xx = [2, 5, 10, 20, 50, 100]
p1 = plot(p, xaxis=:log, xticks=(xx, string.(xx)))
```
:::
::::

## Likelihood

How do we find $\beta_i$ and $\sigma$?

**Likelihood** of data to have come from distribution $f(\mathbf{x} | \theta)$:
$$\mathcal{L}(\theta | \mathbf{x}) = \underbrace{f(\mathbf{x} | \theta)}_{\text{PDF}}$$

Here the randomness comes from $U$:
$$S \sim \mathcal{N}(\beta_0 + \beta_1 \log(D), \sigma^2)$$


## Maximizing Gaussian Likelihood &hArr; Least Squares

$$y_i \sim \mathcal{N}(F(x_i), \sigma^2)$$

::: {.fragment .fade-in}
$$\mathcal{L}(\theta | \mathbf{y}; F) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}} \exp(-\frac{y_i - F(x_i)^2}{2\sigma^2})$$
:::

::: {.fragment .fade-in}

$$\log \mathcal{L}(\theta | \mathbf{y}; F) = \sum_{i=1}^n \left[\log \frac{1}{\sqrt{2\pi}} - \frac{1}{2\sigma^2}(y_i - F(x_i))^2 \right]$$

:::

## {#simplifying-log-likelihood data-menu-title="Simplifying the Log Likelihood"}

$$
\begin{align}
\log \mathcal{L}(\theta | \mathbf{y}, F) &= \sum_{i=1}^n \left[\log \frac{1}{\sqrt{2\pi}} - \frac{1}{2\sigma^2}(y_i - F(x_i))  ^2 \right] \\
&= n \log \frac{1}{\sqrt{2\pi}} - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - F(x_i))^2
\end{align}
$$

## {#simplifying-constants-ignore data-menu-title="Simplifying by Ignoring Constants"}

Ignoring constants (including $\sigma$):

$$\log \mathcal{L}(\theta | \mathbf{y}, F) \propto -\sum_{i=1}^n (y_i - F(x_i))^2.$$

::: {.fragment .fade-in}
Maximizing $f(x)$ is equivalent to minimizing $-f(x)$:

$$
-\log \mathcal{L}(\theta | \mathbf{y}, F) \propto \sum_{i=1}^n (y_i - F(x_i))^2 = \text{MSE}
$$

**But note**: Don't get an estimate of $\sigma^2$ directly through least squares.
:::

## Back to the Problem...

```{julia}
#| echo: true
#| code-line-numbers: "|4-9|11-14"
#| output-location: fragment

# tds_riverflow_loglik: function to compute the log-likelihood for the tds model
# θ: vector of model parameters (coefficients β₀ and β₁ and stdev σ)
# tds, flow: vectors of data
function tds_riverflow_loglik(θ, tds, flow)
    β₀, β₁, σ = θ # unpack parameter vector
    μ = β₀ .+ β₁ * log.(flow) # find mean
    ll = sum(logpdf.(Normal.(μ, σ), tds)) # compute log-likelihood
    return ll
end

lb = [0.0, -1000.0, 1.0]
ub = [1000.0, 1000.0, 100.0]
θ₀ = [500.0, 0.0, 50.0]
optim_out = Optim.optimize(θ -> -tds_riverflow_loglik(θ, tds.tds_mgL, tds.discharge_cms), lb, ub, θ₀)
θ_mle = round.(optim_out.minimizer; digits=0)
@show θ_mle;
```

## Maximum Likelihood Results

```{julia}
#| echo: true
#| output-location: column
#| code-line-numbers: "|2-8|10"

# simulate 10,000 predictions
x = 1:0.1:60
μ = θ_mle[1] .+ θ_mle[2] * log.(x)

y_pred = zeros(length(x), 10_000)
for i = 1:length(x)
    y_pred[i, :] = rand(Normal(μ[i], θ_mle[3]), 10_000)
end
# take quantiles to find prediction intervals
y_q = mapslices(v -> quantile(v, [0.05, 0.5, 0.95]), y_pred; dims=2)

plot(p1, 
    x,
    y_q[:, 2],
    ribbon = (y_q[:, 2] - y_q[:, 1], y_q[:, 3] - y_q[:, 2]),
    linewidth=3, 
    fillalpha=0.2, 
    label="Best Fit",
    size=(600, 550))
```

## Residual Analysis

```{julia}
#| layout-ncol: 2
#| label: fig-residuals
#| fig-cap: Residuals for the TDS-Riverflow model.

pred = θ_mle[1] .+ θ_mle[2] * log.(tds.discharge_cms)
resids = pred - tds.tds_mgL
p1 = histogram(resids, xlabel="Model Residuals (mg/L)", ylabel="Count", title="Residual Histogram", size=(600, 500))
p2 = qqnorm(resids, xlabel="Theoretical Values", ylabel="Empirical Values", title="Residual Q-Q- Plot", size=(600, 500))
display(p1)
display(p2)
```

# More General Models

## Structure of a Probability Model

Unpacking linear regression:

$$\begin{align*}
y_i &\sim N(\mu_i, \sigma^2) \\
\mu_i &= \sum_j \beta_j x^j_i
\end{align*}
$$

::: {.fragment .fade-in}
**Note**: Can handle heteroskedastic errors with a regression $$\sigma_i^2 = \sum_j \alpha_j z^j_i$$
:::


## Think Generatively

:::: {.columns}
::: {.column width=50%}
Components of probability model for observations:

1. Model for "system state" (LR: $\sum_j \beta_j x^j_i$)
2. Choice of "error" distribution (LR: $N(0, \sigma^2)$)

:::

::: {.column width=50%}
![Brain on Regression Meme](memes/brain_on_regression.jpg){width=50%}

::: {.caption}
Source: Richard McElreath
:::
:::
::::

## You Can Always Fit Models...

:::: {.columns}
::: {.column width=50%}
But not all models **are theoretically justifiable**.
:::
::: {.column width=50%}
![Ian Malcolm meme](memes/ian_malcolm_should_model.png)
:::
::::

## How Do We Choose What To Model?

![XKCD 2620](https://imgs.xkcd.com/comics/health_data.png)

::: {.caption}
Source: [XKCD 2620](https://xkcd.com/2620/)
:::

## Example: Modeling Counts



```{julia}
#| echo: true
#| code-fold: true
#| output: true

fish = CSV.File("data/ecology/Fish.csv") |> DataFrame
fish[1:3, :]
```

## What Distribution?

::: {.fragment .fade-in}
Count data can be modeled using a Poisson or negative binomial distribution.

```{julia}
#| label: fig-count-dists
#| fig-cap: 
#|  - "Poisson"
#|  - "Negative Binomial"
#| layout-ncol: 2

p1 = plot(Poisson(2), size=(500, 450), title="Poisson(2)", xlabel="Value")
p2 = plot(NegativeBinomial(2, 0.5), size=(500, 450), title="Negative Binomial(2, 0.5)", xlabel="Value")
display(p1)
display(p2)
```
:::

## Exploring the Data

```{julia}
#| label: fig-fishing-data
#| fig-cap: Fishing Data

histogram(fish.fish_caught, xlabel="Caught Fish", ylabel="Count", size=(1000, 400))
```

- Mean: `{julia} round(mean(fish.fish_caught); digits=1)`
- Variance: `{julia} round(var(fish.fish_caught); digits=1)`

## Model Specification

We might hypothesize that more people fishing for more hours results in a greater chance of catching fish.

$$\begin{align*}
y_i &\sim Poisson(\lambda_i) \\
f(\lambda_i) &= \beta_0 + \beta_1 P_i + \beta_2 H_i
\end{align*}
$$

::: {.fragment .fade-in}
$\lambda_i$: positive "rate"

$f(\cdot)$: maps positive reals (rate scale) to all reals (linear model scale)
:::

## Link Functions

:::: {.columns}
::: {.column width=50%}
$\color{brown}f$ is the **link function**.

$\color{royalblue}f^{-1}$ is the **inverse link**.
:::
::: {.column width=50%}
$$\begin{align*}
y_i &\sim Poisson(\lambda_i) \\
{\color{brown}f}(\lambda_i) &= \beta_0 + \beta_1 P_i + \beta_2 H_i
\end{align*}
$$

$$
\lambda_i = {\color{royalblue}f^{-1}}\left(\beta_0 + \beta_1 P_i + \beta_2 H_i\right)
$$
:::
::::

## Choosing a Link Function

Link functions are typically linked to distributions.

- Poisson models usually use the **log link** (positives &rarr; reals).
- Binomial/Bernoulli models use the **logit** link ($[0, 1]$ &rarr; reals) $$\text{logit}(p) = \log\left(\frac{p}{1-p}\right)$$

## Fitting the Model

```{julia}
#| echo: true
#| code-line-numbers: "|1-5|7-11"

function fish_model(params, persons, hours, fish_caught)
    β₀, β₁, β₂ = params
    λ = exp.(β₀ .+ β₁ * persons + β₂ * hours)
    loglik = sum(logpdf.(Poisson.(λ), fish_caught))
end

lb = [-100.0, -100.0, -100.0]
ub = [100.0, 100.0, 100.0]
init = [0.0, 0.0, 0.0]

optim_out = optimize(θ -> -fish_model(θ, fish.persons, fish.hours, fish.fish_caught), lb, ub, init)
θ_mle = optim_out.minimizer
@show round.(θ_mle; digits=1);
```

## Evaluating Model Fit

```{julia}
#| label: fig-fish-skill
#| fig-cap: Predictive distribution for fitted fish model.
#| output-location: column
#| code-line-numbers: "|1-3|5-8"

P = 1:4 # number of persons
# simulate samples for each P
λ = exp.(θ_mle[1] .+ θ_mle[2] * P)
# draw 10,000 samples for each
fish_sim = zeros(10_000, length(P)) # initialize matrix to store simulations
for i = 1:length(P)
    fish_sim[:, i] = rand(Poisson(λ[i]), 10_000)
end

# plot predictive interval against return periods
boxplot(fish_sim, fillalpha=0.3, size=(900, 600), label=:none, color=:blue, ylabel="Fish Caught", xlabel="Persons")
scatter!(fish.persons .+ 0.5, fish.fish_caught, label="Observations", color=:black)
```

## What Was The Problem?

- Model neglected other plausible contributors, *e.g.* live bait.
- Data is **over-dispersed**: higher variance than mean.
- Might be better described by a zero-inflated Poisson model: 
  - Visitors who spent little time are likely to catch no fish.
  - Visitors who spent more time are likely to catch positive fish.

# Key Points

## Probability Models

- Think of regression as modeling system state (or expectation).
- Full probability model includes distribution of "errors" from expected state.
  - Error models can be complex! More on this later...
- Generalized linear models may require a link to map regression to parameters.
- Fit models by maximizing likelihood: no problem using optimization routines.

# Upcoming Schedule

## Next Classes

**Wednesday**: Modeling Time Series

**Next Week**: Bayesian Statistics and Inference

## Assessments

**Homework 1** Due Friday (2/7).

# References

## References (Scroll for Full List)
