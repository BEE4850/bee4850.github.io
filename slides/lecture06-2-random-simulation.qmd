---
title: "Simulating Random Variables"
subtitle: "Lecture 10"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "February 26, 2024"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: mathjax
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        mermaid: 
            theme: dark
engine: julia
filters:
  - code-fullscreen
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using DataFrames
using DataFramesMeta
using CSV
using Dates
using Distributions
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using Optim
using LaTeXStrings
using Measures
using Animations
using Logging

Random.seed!(1)

plot_font = "Computer Modern"
default(
    fontfamily=plot_font,
    linewidth=3, 
    framestyle=:box, 
    label=nothing, 
    grid=false,
    guidefontsize=18,
    legendfontsize=16,
    tickfontsize=16,
    titlefontsize=20,
    bottom_margin=10mm,
    left_margin=5mm
)
```

# Review

## Bayesian Statistics

- Focused on conditional probability (conditioned on the data)
- Bayes' Theorem to update priors using likelihood.
- Priors can be finicky: use predictive simulations.
- Outstanding question: how do we sample from the posterior (versus just taking the MAP?)

# Random Variable Simulation

## Why Simulate?

- We want to see implications of a probability model.
- We want to test statistical procedures (synthetic data simulation).
- Easier than computing integrals (Monte Carlo).
- Computational efficiency (*e.g.* stochastic gradient descent).

**Generally*: Turns calculus/analytical problems into data summary problems.

## Example: Posterior Sampling

:::: {.columns}
::: {.column width=50%}
$p(\theta)$ and $p(y | \theta)$ are often "nice" distributions, but nothing says $$p(\theta | y) \propto p(\theta) \times p(y | \theta)$$

has to be!
:::
::: {.column width=50%}
```{julia}
#| echo: true
#| code-fold: true
#| label: fig-posterior
#| fig-cap: Comparison of prior, posterior, and likelihood.

normal_samples = rand(Normal(2, 0.5), 10)

lik(σ) = prod(pdf(Normal.(2, σ), normal_samples))
prior(σ) = pdf(LogNormal(log(0.25), 0.25), σ)
posterior(σ) = lik(σ) * prior(σ)

σ_range = 0.1:0.01:1
plot(σ_range, posterior.(σ_range) / maximum(posterior.(σ_range)), color=:black, label="Posterior", linewidth=3)
plot!(σ_range, lik.(σ_range) / maximum(lik.(σ_range)), color=:blue, label="Likelihood", linewidth=3, linestyle=:dash)
plot!(σ_range, prior.(σ_range) / maximum(prior.(σ_range)), color=:orange, label="Prior", linewidth=3, linestyle=:dot)
xlabel!(L"$\sigma$")
ylabel!("Scaled Log-Density")
plot!(size=(550, 500))
```

:::
::::

## Simulation Workflow

## Built-In Sampling Functions

**R**: `sample`, `rnorm`, `rbinom`, etc.

**Julia**: `rand`, `Distributions.rand`

**Python**: `numpy.random.rand`, `scipy.xx.rand`

## What Are These Functions Doing?

Think of a **biased coin with probability of heads $\theta$**.

Want to obtain a Bernoulli random variable.

**What can we do without using a built-in Bernoulli function**?

## Coin Flip Simulation

Given heads probability $\theta$:

1. Draw $u \sim Uniform(0, 1)$.
2. If $u < \theta$, return `H`.
3. Else return `T`.


## What About Discrete Variables?

How can we generalize this strategy for discrete distributions with category probabilities $\theta_1, \theta_2, \ldots, \theta_n$?

## Discrete Variable Simulation

Given category probabilities $\theta_1, \theta_2, \ldots, \theta_n$:

1. Draw $u \sim Uniform(0, 1)$.
2. If $u < \theta_1$, return 1.
3. If $u < \theta_1 + \theta_2$, return 2,
4. $\vdots$
5. Else return $n$.

## Generalization: Quantile Transform Method

Given $U \sim Uniform(0, 1)$, target CDF $F$, **$X = F^{-1}(U)$ has CDF $F$.

Why?

$$\mathbb{P}(X \leq a) = \mathbb{P}(F^{-1}(U) \leq a) = \mathbb{P}(U \leq a) = F(a)$$

::: {.fragment .fade-in}
In other words, if we can generate uniform variables and calculate quantiles, can generate non-uniform variables.
:::

## Problem Solved...Right?

- Often don't have quantile functions in closed form.
- They also often don't have nice numerical solutions.

## Using the PDF

:::: {.columns}
::: {.column width=50%}
Suppose the PDF $f$ has support on $[a, b]$ and $f(x) \leq M$.

**What could we do to sample $X \sim f(\cdot)$?
:::
::: {.column width=50%}
```{julia}
#| label: fig-beta-dist
#| fig-cap: Beta Distribution

plot(Beta(5, 10), xlabel=L"$\theta$", ylabel="Density")
ylims!((0, 3.5))
plot!(size=(500, 500))
```
:::
::::

## Rejection Sampling Algorithm

Given pdf $f$ for $X$, upper bound $M$ on $f$ in $[a, b]$:

1. Simulate uniformly: $y \sim [a, b]$, $u \sim [0, 1]$.
2. If $Mu < f(y)$, keep $y$ as a sample of $X$.
3. Otherwise reject.

## Rejection Sampling Visualization

```{julia}
#| label: fig-rejection-sample
#| fig-cap: Example of rejection sampling for a Beta distribution

nsamp = 500
M = 3.5
u = rand(Uniform(0, 1), nsamp)
y = rand(Uniform(0, 1), nsamp)
f = pdf.(Beta(5, 10), y)

keep_samp = (M * u) .< f

plt = plot(
    1,
    xlim = (0, 1),
    ylim = (0, 3.5),
    legend = false,
    markersize = 4,
    linewidth=3,
    xlabel=L"$X$",
    ylabel="Density",
    rightmargin=5mm
    )

plot!(plt, Beta(5, 10), linecolor=:black, lw=3)

Logging.disable_logging(Logging.Info)

mc_anim = @animate for i = 1:nsamp
    if keep_samp[i]
        scatter!(plt, (y[i], M * u[i]), color=:blue, markershape=:o)
    else
        scatter!(plt, (y[i], M * u[i]), color=:red, markershape=:x)
    end
end every 25

gif(mc_anim, "figures/mc_beta.gif", fps=2)    

```

## Rejection Sampling Efficiency

**Important**: Only kept `{julia} Int64(round(100 * sum(keep_samp) / nsamp; digits=0))`% of proposals.

```{julia}
#| label: fig-rejection-sample-results
#| fig-cap: Results from uniform rejection sampling
#| layout-ncol: 2

plt2 = plot(density(y[keep_samp]), xlabel=L"$X$", ylabel="Sample Density Estimate", linewidth=3, size=(500, 500), legend=false, xlims=(0, 1), rightmargin=5mm)
plot!(plt, size=(500, 500))

display(plt)
display(plt2)
```

## More General Rejection Sampling

:::: {.columns}
::: {.column width=50%}
Use a **proposal density** $g(\cdot)$ which "covers" target $f(\cdot)$ and is easy to sample from.

Sample from $g$, reject based on $f$.
:::

::: {.column width=50%}
```{julia}
#| label: fig-normal-t
#| fig-cap: Using a t distribution as proposal density for normal.

x = -5:0.01:5
gauss_x = pdf.(Normal(0, 2), x)
t_x = pdf.(TDist(2), x)
plot(x, gauss_x, linewidth=3, color=:black, label="N(0, 4)")
plot!(x, 3 * t_x, linewidth=3, color=:blue, label="3 * t(2)")
plot!(size=(500, 500))
```

:::
::::

## General Rejection Sampling Algorithm

Suppose $f(x) \leq M g(x)$ for some $1 < M < \infty$.

1. Simulate a proposal $y \sim g(x)$ (*e.g.* by quantile method).
2. Simulate $u \sim \text{Unif}(0, 1)$
3. If $$u < \frac{f(y)}{M g(y)},$$ accept $y$; otherwise reject.

## Rejection Sampling Example

:::: {.columns}
::: {.column width=50%}
```{julia}
#| label: fig-rejection-dist
#| fig-cap: Example of rejection sampling for a Normal distribution

nsamp = 1_000
M = 3
u = rand(Uniform(0, 1), nsamp)
y = rand(TDist(2), nsamp)
g = pdf.(TDist(2), y)
f = pdf.(Normal(0, 2), y)
keep_samp = u .< f ./ (M * g)
histogram(y[keep_samp], normalize=:pdf, label="Kept Samples")
plot!(Normal(0, 2), linewidth=3, color=:black, label="N(0, 4)")
plot!(size=(500, 500))
```
:::
::: {.column width=50%}
This time we kept `{julia} Int64(round(100 * sum(keep_samp) / nsamp; digits=0))`% of the proposals.
:::
::::

## Rejection Sampling Efficiency

1. Probability of accepting a sample is $1/M$, so the "tighter" the proposal distribution coverage the more efficient the sampler.
2. Need to be able to compute $M$ and sample from the proposal.

Finding a good proposal and computing $M$ may not be easy (or possible) for complex distributions!

# Random Number Generators

## Where Do "Random" Samples Come From?

:::: {.columns}

::: {.column width=50%}
There's no such thing as a truly random number generator!

Need to generate samples in a deterministic way that have random-like properties.
:::
::: {.column width=50%}
![XKCD Cartoon 221](https://imgs.xkcd.com/comics/random_number.png)

::: {.caption}
Source: [XKCD 221](https://xkcd.com/221){width=100%}
:::
:::
::::

## Pseudorandom Number Generators

We want:

- Number of $U_i \in [a, b] \subset [0, 1]$ is $\propto b-a$
- No correlation between successive $U_i$.
- No detectable dependencies in longer series.

There are several of these implemented in modern programming languages: typical default is the **Mersenne Twister**.

## Example: Rotations

$$U_{t+1} = U_t + \alpha \mod 1$$

- If $\alpha \neq k/n$ is irrational, this never repeats (no $m$ such that $m\alpha = 1$.)
- If $\alpha = k/n$ is rational, this repeats, but with a long period for large $n$.


## (P)RNGs and Chaos

We can get similar dynamics from area-preserving **chaotic** dynamical systems:

- Long periods with dense orbits (well-mixing);
- Area-perserving (uniformly distributed);
- Rapid divergence of nearby points (sensitivity to initial conditions);

## Example: Arnold Cat Map

:::: {.columns}
::: {.column width=50%}

$$
\begin{align*}
\phi_{t+1} &= 2U_t + \phi_t \mod  1 \\
U_{t+1} &= \phi_t + U_t \mod  1
\end{align*}
$$

Report only $(U_t)$: get hard to predict uniformly distributed data.

:::
::: {.column width=50%}
![Arnold Cat Map](https://upload.wikimedia.org/wikipedia/commons/a/a6/Arnold_cat.png){width=50%}

::: {.caption}
Source: [Wikipedia](https://en.wikipedia.org/wiki/Arnold%27s_cat_map#The_discrete_cat_map)
:::
:::
::::

## Cat Map vs Mersenne Twister

```{julia}
#| label: fig-prng-compare
#| fig-cap: Comparison of the Arnold cat map and output from the Mersenne Twister
#| layout-ncol: 2
#| code-fold: true
#| echo: true

function cat_map(N)
    U = zeros(N)
    ϕ = zeros(N)
    U[1], ϕ[1] = rand(Uniform(0, 1), 2)
    for i = 2:N
        ϕ[i] = rem(U[i-1] + 2 * ϕ[i-1], 1.0)
        U[i] = rem(U[i-1] + ϕ[i-1], 1.0)
    end
    return U
end

N = 1_000
Z = cat_map(N)
U = rand(MersenneTwister(1), N) # Mersenne Twister is not the default in Julia, but is in other languages, so using it here by explicitly setting it as the generator

p1 = scatter(Z[1:end-1], Z[2:end], xlabel=L"$U_t$", ylabel=L"$U_{t+1}$", title="Cat Map", label=false, size=(500, 500))
p2 = scatter(U[1:end-1], U[2:end], xlabel=L"$U_t$", ylabel=L"$U_{t+1}$", title="Mersenne Twister", label=false, size=(500, 500))

display(p1)
display(p2)

```

## Random Seeds

Pseudo-random numbers are deterministic, so can be repeated. The sequence depends on a **seed**.

- If you don't set a seed explicitly, the computer will choose one (usually based on the date/time stamp when you execute the script or program).
- Setting a seed resets the sequence.

**Set seeds to ensure reproducibility**.

## But...Be Careful With Seeds

- Possible for a statistical procedure to appear to work with just one seed; **test across several**.
- Seeds may match, but RNG algorithm may be different across different languages/versions.

# Key Points and Upcoming Schedule

## Key Points: Bayesian Statistics

- Use prior predictive simulations to refine priors.
- Priors matter less when likelihood is highly informative.

## Key Points: Random Numbers

- Can generate uniform distributions using pseudorandom number generators (unstable dynamical systems).
- Transform into other distributions using:
  - Quantile method;
  - Rejection method.
- Default functions work well; only really have to worry about any of this when sampling from "non-nice" distributions (*e.g.* most posteriors)

# Upcoming Schedule

## Next Classes

**Wednesday**: Monte Carlo

**Next Week**: The Bootstrap

## Term Project

- Can work in groups of 1-2
- Proposal due 3/21.
- Max three pages, should include background and problem statement, data overview, proposed probability models, and research plan.
- Deliverables include presentations (in class, last 2-3 sessions) and written report (during finals week).

# References

## References (Scroll for Full List)
