---
title: "Model Complexity and Emulation"
subtitle: "Lecture 21"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "April 29, 2024"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: 
            method: mathjax
            url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        touch: false
        controls: true
execute:
    freeze: auto
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using DataFrames
using DataFramesMeta
using CSV
using Dates
using Distributions
using Turing
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using LaTeXStrings
using Measures
using Optim

Random.seed!(1)
```

# Review of Model Selection

## What Is The Goal of Model Selection?

**Key Idea**: Model selection consists of navigating the bias-variance tradeoff.

Model error (*e.g.* MSE) is a combination of *irreducible error*, *bias*, and *variance*.

- Bias can come from under-dispersion (too little complexity) or neglected processes;
- Variance can come from over-dispersion (too much complexity) or poor identifiability.

## Cross-Validation

Cross-validation is the gold standard for predictive accuracy: how well does the fitted model predict out of sample data?

The problems:

- Leave-one-out CV can be very computationally expensive!
- We often don't have a lot of data for calibration, so holding some back can be a problem.
- How to divide data with spatial or temporal structure? This can be addressed by partitioning the data more cleverly (*e.g.* leaving out future observations), but makes the data problem worse.

## Information Criteria

Approximate LOO-CV by computing fit on calibration/training data and "correcting" for expected overfitting.

Examples (differ in the parameter values used and correction factor(s)):

- AIC
- DIC
- WAIC

## Model Weighting

Information Criteria can be used to get averaging weights:

$$w\_i = \frac{\exp(-\Delta\_i/2)}{\sum\_{m=1}^M \exp(-\Delta\_m/2)}.$$

## Bayesian LOO-CV: Advanced Methods

There are approximations to Leave-One-Out Cross-Validation which use *importance sampling* to avoid this, and these can be extended to time series. 

See:

- @Vehtari2017-oi on approximations to LOO-CV;
- @Burkner2020-lo on "leave-future-out" time series CV;
- @Yao2018-rr on modeling stacking for averaging.

# Model Complexity

## Parsimony as A Modeling Virtue

Parsimony (fewer model terms/components) can reduce the chance of overfitting and increased variance, all else being equal.

Model simplicity has another advantange: **simpler models are less computationally expensive**.

## Benefits of Model Simplicity

:::: {.columns}
::: {.column width=50%}
- More thorough representation of uncertainties
- Can focus on "important" characteristics for problem at hand
- Potential increase in generalizability
:::
::: {.column width=50%}
![Computational Complexity](figures/simplicity-calibration.png)
::: {.caption}
Source: @Helgeson2021-ok
:::
::::

## Downsides of Model Simplicity

- Potential loss of salience
- May miss important dynamics (creating bias)
- Parameter/dynamical compensation can result in loss of interpretability

## Simplicity Tradeoffs

Simple models can be epistemically and practically valuable.

**But**:

Need to carefully select which processes/parameters are included in the simplified representation, and at what resolution.

# Emulation

## Approximating Complex Models

**Challenge**: How do we simplify complex models to keep key dynamics but reduce computational expense?

::: {.fragment .fade-in}
Approximate (or **emulate**) the model response surface.

1. Evaluate original model at an ensemble of points (design of experiment)
2. Calibrate emulator against those points.
3. Use emulator for UQ with MCMC or other methods.
:::

## Emulation of a 1-D Toy Model

::: {.center}
![Emulation of a Toy Model](figures/toy-model.png)
::: {.caption}
Source: @Haran2017-vz
:::
:::

## Emulator of a Spatial (Ice Sheet) Model

::: {.center}
![Emulation of a Toy Model](figures/emulator-ice-sheet.png)
::: {.caption}
Source: @Haran2017-vz
:::
:::

## Is Emulation Always The Right Choice?

::: {.center}
![Emulation of a Toy Model](figures/calibration-emulation.png)
::: {.caption}
Source: @Lee2020-ws
:::
:::

## Impacts of "Poor" Emulation

This error can have large knock-on effects for risk analysis:

::: {.center}
![Emulation of a Toy Model](figures/calibration-ice-sheet.png)
::: {.caption}
Source: @Lee2020-ws
:::
:::

# Key Takeaways and Upcoming Schedule

## Key Takeaways

- Model simplicity can be valuable.
- Tradeoff between computational expense and fidelity of approximation.
- Emulation can "simplify" complex models by approximating response surfaces
- Emulator methods have different pros and cons which can make them more or less important.
- Emulator error can strongly influence resulting risk estimates.

## Upcoming Schedule

**Wednesday**: Emulation Methods

**Friday**: HW4 due

**Next Monday**: Project Presentations, email slides by Saturday.

# References

## References

