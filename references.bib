@ARTICLE{Lloyd2018-oa,
  title = {{Climate change attribution: When is it appropriate to accept new
  methods?}},
  author = {Lloyd, Elisabeth A and Oreskes, Naomi},
  journaltitle = {Earths Future},
  publisher = {American Geophysical Union (AGU)},
  volume = {6},
  issue = {3},
  pages = {311--325},
  date = {2018-03-01},
  doi = {10.1002/2017ef000665},
  issn = {2328-4277},
  abstract = {AbstractThe most common approaches to detection and attribution
  (D\&A) of extreme weather events using fraction of attributable risk or risk
  ratio answer a particular form of research question, namely “What is the
  probability of a certain class of weather events, given global climate change,
  relative to a world without?” In a set of recent papers, Trenberth et al.
  (2015,https://doi.org/10.1038/nclimate2657) and Shepherd
  (2016,https://doi.org/10.1007/s40641‐016‐0033‐y) have argued that this is not
  always the best tool for analyzing causes, or for communicating with the
  public about climate events and extremes. Instead, they promote the idea of a
  “storyline” approach, which asks complementary questions, such as “How much
  did climate change affect the severity of a given storm?” From the vantage of
  history and philosophy of science, a proposal to introduce a new approach or
  to answer different research questions—especially those of public
  interest—does not appear particularly controversial. However, the proposal
  proved highly controversial, with the majority of D\&A scientists reacting in
  a very negative and even personal manner. Some suggested the proposed
  alternatives amount to a weakening of standards, or an abandonment of
  scientific method. Here, we address the question: Why is this such a
  controversial proposition? We argue that there is no “right” or “wrong”
  approach to D\&A in any absolute sense, but rather that in different contexts,
  society may have a greater or lesser concern with errors of a particular type.
  How we view the relative risk of overestimation versus underestimation of harm
  is context‐dependent.},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/2017EF000665},
  urldate = {2024-12-26},
  file = {All_Papers/L/Lloyd_and_Oreskes_2018_-_Climate_change_attribution_-_When_is_it_appropriate_to_accept_new_methods.pdf},
  keywords = {detection and attribution; extreme event; framing questions; Type
  I and Type II errors; null hypothesis; logic of research questions},
  language = {en}
}

@INBOOK{Fox-Kemper2021-ni,
  title = {{Ocean, Cryosphere and Sea Level Change}},
  author = {Fox-Kemper, B and Hewitt, H T and Xiao, C and Aðalgeirsdóttir, G and
  Drijfhout, S S and Edwards, T L and Golledge, N R and Hemer, M and Kopp, R E
  and Krinner, G and Mix, A and Notz, D and Nowicki, S and Nurhati, I S and
  Ruiz, L and Sallée, J-B and Slangen, A B A and and Yu, Y},
  editor = {Pirani, A and Connors, S L and Péan, C and Berger, S and Caud, N and
  Chen, Y and Goldfarb, L and Gomis, M I and Huang, M and Leitzell, K and
  Lonnoy, E and Matthews, J B R and Maycock, T K and Waterfield, T and Yelekçi,
  O and Yu, R and and Zhou, B},
  booktitle = {{Climate Change 2021: The Physical Science Basis. Contribution of
  Working Group I to the Sixth Assessment Report of the Intergovernmental Panel
  on Climate Change}},
  publisher = {Cambridge University Press},
  location = {Cambridge, United Kingdom and New York, NY, USA},
  pages = {1211--1362},
  date = {2021},
  doi = {10.1017/9781009157896.011},
  url = {https://www.ipcc.ch/report/ar6/wg1/downloads/report/IPCC_AR6_WGI_Chapter09.pdf},
  file = {All_Papers/F/Fox-Kemper_et_al._2021_-_Ocean,_Cryosphere_and_Sea_Level_Change.pdf}
}


@ARTICLE{Milne2009-um,
  title = {{Identifying the causes of sea-level change}},
  author = {Milne, Glenn A and Gehrels, W Roland and Hughes, Chris W and
  Tamisiea, Mark E},
  journaltitle = {Nat. Geosci.},
  volume = {2},
  issue = {7},
  pages = {471--478},
  date = {2009-07},
  doi = {10.1038/ngeo544},
  issn = {1752-0894,1752-0908},
  abstract = {Global mean sea-level change has increased from a few centimetres
  per century over recent millennia to a few tens of centimetres per century in
  recent decades. This tenfold increase in the rate of rise can be attributed to
  climate change through the melting of land ice and the thermal expansion of
  ocean water. As the present warming trend is expected to continue, global mean
  sea level will continue to rise. Here we review recent insights into past
  sea-level changes on decadal to millennial timescales and how they may help
  constrain future changes. We find that most studies constrain global mean
  sea-level rise to less than one metre over the twenty-first century, but
  departures from this global mean could reach several decimetres in many areas.
  We conclude that improving estimates of the spatial variability in future
  sea-level change is an important research target in coming years.},
  url = {https://www.nature.com/articles/ngeo544},
  urldate = {2023-02-12},
  language = {en}
}



@MISC{Caldwell2015-kc,
  title     = "Sea level measured by tide gauges from global oceans --- the
               Joint Archive for Sea Level holdings ({NCEI} Accession 0019568)",
  author    = "Caldwell, P C and Merrifield, M A and Thompson, P R",
  publisher = "NOAA National Centers for Environmental Information (NCEI)",
  year      =  2015,
  doi       = "10.7289/V5V40S7W"
}

@ARTICLE{Hoffman2014-gv,
  title    = "The {No-U-Turn} Sampler: Adaptively Setting Path Lengths in
              {H}amiltonian {M}onte {C}arlo",
  author   = "Hoffman, Matthew D and Gelman, Andrew",
  journal  = "J. Mach. Learn. Res.",
  volume   =  15,
  number   =  47,
  pages    = "1593--1623",
  year     =  2014,
  issn     = "1532-4435, 1533-7928"
}


@ARTICLE{Arns2013-tl,
  title    = "Estimating extreme water level probabilities: A comparison of the
              direct methods and recommendations for best practise",
  author   = "Arns, A and Wahl, T and Haigh, I D and Jensen, J and
              Pattiaratchi, C",
  abstract = "Over the past five decades, several approaches for estimating
              probabilities of extreme still water levels have been developed.
              Currently, different methods are applied not only on
              transnational, but also on national scales, resulting in a
              heterogeneous level of protection. Applying different statistical
              methods can yield significantly different estimates of return
              water levels, but even the use of the same technique can produce
              large discrepancies, because there is subjective parameter choice
              at several steps in the model setup. In this paper, we compare
              probabilities of extreme still water levels estimated using the
              main direct methods (i.e. the block maxima method and the peaks
              over threshold method) considering a wide range of strategies to
              create extreme value dataset and a range of different model
              setups. We primarily use tide gauge records from the German Bight
              but also consider data from sites around the UK and Australia for
              comparison. The focus is on testing the influence of the
              following three main factors, which can affect the estimates of
              extreme value statistics: (1) detrending the original data sets;
              (2) building samples of extreme values from the original data
              sets; and (3) the record lengths of the original data sets. We
              find that using different detrending techniques biases the
              results from extreme value statistics. Hence, we recommend using
              a 1-year moving average of high waters (or hourly records if
              these are available) to correct the original data sets for
              seasonal and long-term sea level changes. Our results highlight
              that the peaks over threshold method yields more reliable and
              more stable (i.e. using short records leads to the same results
              as when using long records) estimates of probabilities of extreme
              still water levels than the block maxima method. In analysing a
              variety of threshold selection methods we find that using the
              99.7th percentile water level leads to the most stable return
              water level estimates along the German Bight. This is also valid
              for the international stations considered. Finally, to provide
              guidance for coastal engineers and operators, we recommend the
              peaks over threshold method and define an objective approach for
              setting up the model. If this is applied routinely around a
              country, it will help overcome the problem of heterogeneous
              levels of protection resulting from different methods and varying
              model setups.",
  journal  = "Coast. Eng.",
  volume   =  81,
  pages    = "51--66",
  month    =  nov,
  year     =  2013,
  keywords = "Extreme water levels; Coastal flooding; Return levels and
              periods; Extreme value distribution; German Bight",
  issn     = "0378-3839",
  doi      = "10.1016/j.coastaleng.2013.07.003"
}

@REPORT{Helsel2020-nq,
  type = {resreport},
  title = {{Statistical methods in water resources}},
  author = {Helsel, Dennis R and Hirsch, Robert M and Ryberg, Karen R and
  Archfield, Stacey A and Gilroy, Edward J},
  institution = {U.S. Geological Survey},
  location = {Reston, VA},
  number = {4-A3},
  pages = {484},
  date = {2020},
  abstract = {This text began as a collection of class notes for a course on
  applied statistical methods for hydrologists taught at the U.S. Geological
  Survey (USGS) National Training Center. Course material was formalized and
  organized into a textbook, first published in 1992 by Elsevier as part of
  their Studies in Environmental Science series. In 2002, the work was made
  available online as a USGS report.The text has now been updated as a USGS
  Techniques and Methods Report. It is intended to be a text in applied
  statistics for hydrology, environmental science, environmental engineering,
  geology, or biology that addresses distinctive features of environmental data.
  For example, water resources data tend to have many variables with a lower
  bound of zero, tend to be more skewed than data from many other disciplines,
  commonly contain censored data (less than values), and assumptions that the
  data are normally distributed are not appropriate. Computer-intensive methods
  (bootstrapping and permutation tests) now improve upon and replace the
  dependence on t-intervals, t-tests, and analysis of variance. A new chapter on
  sampling design addresses questions such as “How many observations do I need?”
  The chapter also presents distribution-free methods to help plan sampling
  efforts. The trends chapter has been updated to include the WRTDS (Weighted
  Regressions on Time, Discharge, and Season) method for analysis of
  water-quality data. This new version contains updated graphics and updated
  guidance on the use of statistical techniques. The text utilizes R, a
  programming language and open-source software environment, for all exercises
  and most graphics, and the R code used to generate figures and examples is
  provided for download.},
  series = {Techniques and Methods},
  url = {http://pubs.er.usgs.gov/publication/tm4A3},
  urldate = {2023-02-19},
  file = {All_Papers/H/Helsel_et_al._2020_-_Statistical_methods_in_water_resources.pdf}
}

@ARTICLE{Szucs2017-of,
  title = {{When null hypothesis significance testing is unsuitable for
  research: A reassessment}},
  author = {Szucs, Denes and Ioannidis, John P A},
  journaltitle = {Front. Hum. Neurosci.},
  publisher = {Frontiers Media SA},
  volume = {11},
  pages = {390},
  date = {2017-08-03},
  doi = {10.3389/fnhum.2017.00390},
  pmc = {PMC5540883},
  pmid = {28824397},
  issn = {1662-5161},
  abstract = {Null hypothesis significance testing (NHST) has several
  shortcomings that are likely contributing factors behind the widely debated
  replication crisis of (cognitive) neuroscience, psychology, and biomedical
  science in general. We review these shortcomings and suggest that, after
  sustained negative experience, NHST should no longer be the default, dominant
  statistical practice of all biomedical and psychological research. If
  theoretical predictions are weak we should not rely on all or nothing
  hypothesis tests. Different inferential methods may be most suitable for
  different types of research questions. Whenever researchers use NHST they
  should justify its use, and publish pre-study power calculations and effect
  sizes, including negative findings. Hypothesis-testing studies should be
  pre-registered and optimally raw data published. The current statistics lite
  educational approach for students that has sustained the widespread, spurious
  use of NHST should be phased out.},
  url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC5540883/},
  urldate = {2024-12-28},
  file = {All_Papers/S/Szucs_and_Ioannidis_2017_-_When_null_hypothesis_significance_testing_is_unsuitable_for_research_-_A_reassessment.pdf},
  keywords = {Bayesian methods; false positive findings; null hypothesis
  significance testing; replication crisis; research methodology},
  language = {en}
}

@ARTICLE{Ioannidis2005-zb,
  title = {{Why most published research findings are false}},
  author = {Ioannidis, John P A},
  journaltitle = {PLoS Med.},
  publisher = {Public Library of Science},
  volume = {2},
  issue = {8},
  pages = {e124},
  date = {2005-08-30},
  doi = {10.1371/journal.pmed.0020124},
  pmc = {PMC1182327},
  pmid = {16060722},
  issn = {1549-1676,1549-1277},
  abstract = {There is increasing concern that most current published research
  findings are false. The probability that a research claim is true may depend
  on study power and bias, the number of other studies on the same question,
  and, importantly, the ratio of true to no relationships among the
  relationships probed in each scientific field. In this framework, a research
  finding is less likely to be true when the studies conducted in a field are
  smaller; when effect sizes are smaller; when there is a greater number and
  lesser preselection of tested relationships; where there is greater
  flexibility in designs, definitions, outcomes, and analytical modes; when
  there is greater financial and other interest and prejudice; and when more
  teams are involved in a scientific field in chase of statistical significance.
  Simulations show that for most study designs and settings, it is more likely
  for a research claim to be false than true. Moreover, for many current
  scientific fields, claimed research findings may often be simply accurate
  measures of the prevailing bias. In this essay, I discuss the implications of
  these problems for the conduct and interpretation of research.},
  url = {https://journals.plos.org/plosmedicine/article/file?id=10.1371/journal.pmed.0020124&type=printable},
  urldate = {2024-12-28},
  file = {All_Papers/I/Ioannidis_2005_-_Why_most_published_research_findings_are_false.pdf},
  language = {en}
}

@book{mcelreath2020statistical,
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work.

The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding.

The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses.},
  added-at = {2024-08-14T05:34:26.000+0200},
  address = {Boca Raton, Florida},
  author = {McElreath, Richard},
  biburl = {https://www.bibsonomy.org/bibtex/2962aa91664e5ea168a72622d5f2dd8b9/gdmcbain},
  edition = {Second},
  interhash = {a3f35075150550e2b37f8818e0535fbf},
  intrahash = {962aa91664e5ea168a72622d5f2dd8b9},
  isbn = {036713991X 9780367139919},
  keywords = {62-01-statistics-instructional-exposition 62-08-computational-methods-for-problems-for-statistics 62f15-bayesian-inference 62m05-markov-processes-estimation-hidden-markov-models 62-04-statistics-software-source-code},
  publisher = {CRC},
  series = {Texts in Statistical Science},
  timestamp = {2024-12-10T01:16:55.000+0100},
  title = {Statistical Rethinking : A Bayesian Course with Examples in {R} and {Stan}},
  url = {https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919},
  year = 2020
}


@ARTICLE{Gelman2006-wv,
  title    = "Prior distributions for variance parameters in hierarchical
              models (comment on article by {B}rowne and {D}raper)",
  author   = "Gelman, Andrew",
  abstract = "Various noninformative prior distributions have been suggested
              for scale parameters in hierarchical models. We construct a new
              folded-noncentral-$t$ family of conditionally conjugate priors
              for hierarchical standard deviation parameters, and then consider
              noninformative and weakly informative priors in this family. We
              use an example to illustrate serious problems with the
              inverse-gamma family of ``noninformative'' prior distributions.
              We suggest instead to use a uniform prior on the hierarchical
              standard deviation, using the half-$t$ family when the number of
              groups is small and in other settings where a weakly informative
              prior is desired. We also illustrate the use of the half-$t$
              family for hierarchical modeling of multiple variance parameters
              such as arise in the analysis of variance.",
  journal  = "Bayesian Anal.",
  volume   =  1,
  number   =  3,
  pages    = "515--533",
  month    =  sep,
  year     =  2006,
  keywords = "Bayesian inference, conditional conjugacy, folded-; bution,
              half-t distribution, hierarchical model, m; prior distribution,
              weakly informative prior distr",
  issn     = "1936-0975, 1931-6690",
  doi      = "10.1214/06-BA117A"
}

@ARTICLE{Srikrishnan2022-yq,
  title = {{Probabilistic projections of baseline twenty-first century CO2
  emissions using a simple calibrated integrated assessment model}},
  author = {Srikrishnan, Vivek and Guan, Yawen and Tol, Richard S J and Keller,
  Klaus},
  journaltitle = {Clim. Change},
  volume = {170},
  issue = {3},
  pages = {37},
  date = {2022-02-24},
  doi = {10.1007/s10584-021-03279-7},
  issn = {0165-0009,1573-1480},
  url = {https://doi.org/10.1007/s10584-021-03279-7},
  urldate = {2022-02-24},
  language = {en}
}

@ARTICLE{Doss-Gollin2023-kl,
  title = {{A subjective Bayesian framework for synthesizing deep uncertainties
  in climate risk management}},
  author = {Doss-Gollin, James and Keller, Klaus},
  journaltitle = {Earths Future},
  publisher = {American Geophysical Union (AGU)},
  volume = {11},
  issue = {1},
  pages = {e2022EF003044},
  date = {2023-01-01},
  doi = {10.1029/2022ef003044},
  issn = {2328-4277},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2022EF003044},
  urldate = {2023-06-09},
  keywords = {decision making under deep uncertainty; climate adaptation;
  climate risk management; house elevation; Bayesian statistics},
  language = {en}
}

@ARTICLE{Ruckert2017-qg,
  title = {{The effects of time-varying observation errors on semi-empirical
  sea-level projections}},
  author = {Ruckert, Kelsey L and Guan, Yawen and Bakker, Alexander M R and
  Forest, Chris E and Keller, Klaus},
  journaltitle = {Clim. Change},
  publisher = {Climatic Change},
  volume = {140},
  issue = {3-4},
  pages = {349--360},
  date = {2017},
  doi = {10.1007/s10584-016-1858-z},
  issn = {0165-0009,1573-1480},
  url = {http://link.springer.com/10.1007/s10584-016-1858-z},
  urldate = {2021-06-14},
  language = {en}
}

@ARTICLE{Fagnant2020-xu,
  title = {{Characterizing spatiotemporal trends in extreme precipitation in
  Southeast Texas}},
  author = {Fagnant, Carlynn and Gori, Avantika and Sebastian, Antonia and
  Bedient, Philip B and Ensor, Katherine B},
  journaltitle = {Nat. Hazards},
  publisher = {Springer Science and Business Media LLC},
  volume = {104},
  issue = {2},
  pages = {1597--1621},
  date = {2020-11-01},
  doi = {10.1007/s11069-020-04235-x},
  issn = {0921-030X,1573-0840},
  urldate = {2024-01-14},
  language = {en}
}

@ARTICLE{Errickson2021-kr,
  title = {{Equity is more important for the social cost of methane than climate
  uncertainty}},
  author = {Errickson, Frank C and Keller, Klaus and Collins, William D and
  Srikrishnan, Vivek and Anthoff, David},
  journaltitle = {Nature},
  publisher = {Nature Publishing Group},
  volume = {592},
  issue = {7855},
  pages = {564--570},
  date = {2021-04-22},
  doi = {10.1038/s41586-021-03386-6},
  issn = {0028-0836,1476-4687},
  url = {http://www.nature.com/articles/s41586-021-03386-6},
  urldate = {2021-06-15},
  language = {en}
}

@MISC{Rockmore2024-ms,
  title = {{How Much of the World Is It Possible to Model?}},
  author = {Rockmore, Dan},
  journaltitle = {The New Yorker},
  publisher = {The New Yorker},
  date = {2024-01-15},
  issn = {0028-792X},
  abstract = {Dan Rockmore writes about mathematical models—including for
  COVID-19, climate change, and artificial intelligence—and whether they have
  limits.},
  url = {https://www.newyorker.com/culture/annals-of-inquiry/how-much-of-the-world-is-it-possible-to-model},
  urldate = {2024-01-16}
}

@ARTICLE{Walker2003-zi,
  title = {{Defining uncertainty: A conceptual basis for uncertainty management
  in model-based decision support}},
  shorttitle = {{Defining uncertainty}},
  author = {Walker, W E and Harremoës, P and Rotmans, J and van der Sluijs, J P
  and van Asselt, M B A and Janssen, P and Krayer von Krauss, M P},
  journaltitle = {Integrated Assessment},
  publisher = {Taylor \& Francis},
  volume = {4},
  issue = {1},
  pages = {5--17},
  date = {2003-03-01},
  doi = {10.1076/iaij.4.1.5.16466},
  issn = {1389-5176},
  abstract = {The aim of this paper is to provide a conceptual basis for the
  systematic treatment of uncertainty in model-based decision support activities
  such as policy analysis, integrated assessment and risk assessment. It focuses
  on the uncertainty perceived from the point of view of those providing
  information to support policy decisions (i.e., the modellers? view on
  uncertainty) ? uncertainty regarding the analytical outcomes and conclusions
  of the decision support exercise. Within the regulatory and management
  sciences, there is neither commonly shared terminology nor full agreement on a
  typology of uncertainties. Our aim is to synthesise a wide variety of
  contributions on uncertainty in model-based decision support in order to
  provide an interdisciplinary theoretical framework for systematic uncertainty
  analysis. To that end we adopt a general definition of uncertainty as being
  any deviation from the unachievable ideal of completely deterministic
  knowledge of the relevant system. We further propose to discriminate among
  three dimensions of uncertainty: location, level and nature of uncertainty,
  and we harmonise existing typologies to further detail the concepts behind
  these three dimensions of uncertainty. We propose an uncertainty matrix as a
  heuristic tool to classify and report the various dimensions of uncertainty,
  thereby providing a conceptual framework for better communication among
  analysts as well as between them and policymakers and stakeholders.
  Understanding the various dimensions of uncertainty helps in identifying,
  articulating, and prioritising critical uncertainties, which is a crucial step
  to more adequate acknowledgement and treatment of uncertainty in decision
  support endeavours and more focused research on complex, inherently uncertain,
  policy issues.},
  url = {http://www.tandfonline.com/doi/abs/10.1076/iaij.4.1.5.16466},
  urldate = {2021-06-14},
  file = {All_Papers/W/Walker_et_al._2003_-_Defining_uncertainty_-_A_conceptual_basis_for_uncertainty_management_in_model-based_decision_support.pdf},
  keywords = {notion},
  language = {en}
}

@ARTICLE{Bayes1763-at,
  title = {{An Essay towards Solving a Problem in the Doctrine of Chance}},
  author = {Bayes, Thomas},
  journaltitle = {Philosophical Transactions of the Royal Society of London},
  volume = {53},
  pages = {370--418},
  date = {1763}
}

@INBOOK{Laplace1774-nf,
  title = {{Mémoire sur la Probabilité des Causes par les évènemens}},
  author = {Laplace, P S},
  booktitle = {{Mémoires de Mathematique et de Physique, Presentés à l’Académie
  Royale des Sciences, Par Divers Savans \& Lus Dans ses Assemblées}},
  pages = {621--656},
  date = {1774}
}

@ARTICLE{Stein2020-rb,
  title = {{Some statistical issues in climate science}},
  author = {Stein, Michael L},
  journaltitle = {Stat. Sci.},
  publisher = {Institute of Mathematical Statistics},
  volume = {35},
  issue = {1},
  pages = {31--41},
  date = {2020-02-01},
  doi = {10.1214/19-sts730},
  issn = {0883-4237,2168-8745},
  url = {https://projecteuclid.org/journals/statistical-science/volume-35/issue-1/Some-Statistical-Issues-in-Climate-Science/10.1214/19-STS730.full},
  file = {All_Papers/S/Stein_2020_-_Some_statistical_issues_in_climate_science.pdf},
  language = {en}
}

@ARTICLE{Brynjarsdottir2014-ve,
  title = {{Learning about physical parameters: the importance of model
  discrepancy}},
  shorttitle = {{Learning about physical parameters}},
  author = {Brynjarsdóttir, Jenný and O'Hagan, Anthony},
  journaltitle = {Inverse Problems},
  volume = {30},
  issue = {11},
  pages = {114007},
  date = {2014-11-01},
  doi = {10.1088/0266-5611/30/11/114007},
  issn = {0266-5611,1361-6420},
  abstract = {Science-based simulation models are widely used to predict the
  behavior of complex physical systems. It is also common to use observations of
  the physical system to solve the inverse problem, that is, to learn about the
  values of parameters within the model, a process which is often called
  calibration. The main goal of calibration is usually to improve the predictive
  performance of the simulator but the values of the parameters in the model may
  also be of intrinsic scientiﬁc interest in their own right. In order to make
  appropriate use of observations of the physical system it is important to
  recognize model discrepancy, the difference between reality and the simulator
  output. We illustrate through a simple example that an analysis that does not
  account for model discrepancy may lead to biased and over-conﬁdent parameter
  estimates and predictions. The challenge with incorporating model discrepancy
  in statistical inverse problems is being confounded with calibration
  parameters, which will only be resolved with meaningful priors. For our simple
  example, we model the model-discrepancy via a Gaussian process and demonstrate
  that through accounting for model discrepancy our prediction within the range
  of data is correct. However, only with realistic priors on the model
  discrepancy do we uncover the true parameter values. Through theoretical
  arguments we show that these ﬁndings are typical of the general problem of
  learning about physical parameters and the underlying physical system using
  science-based mechanistic models.},
  url = {https://iopscience.iop.org/article/10.1088/0266-5611/30/11/114007},
  urldate = {2021-06-14},
  file = {All_Papers/B/Brynjarsdóttir_and_OʼHagan_2014_-_Brynjarsdóttir_and_OʼHagan_-_2014_-_Learning_about_physical_parameters_the_importance.pdf},
  keywords = {notion},
  language = {en}
}

@ARTICLE{Gelman2004-gh,
  title = {{Exploratory Data Analysis for Complex Models}},
  author = {Gelman, Andrew},
  journaltitle = {J. Comput. Graph. Stat.},
  volume = {13},
  issue = {4},
  pages = {755--779},
  date = {2004},
  doi = {10.1198/106186004X11435},
  issn = {1061-8600,1537-2715},
  abstract = {“Exploratory” and “confirmatory” data analysis can both be viewed
  as methods for comparing observed data to what would be obtained under an
  implicit or explicit statistical model. For example, many of Tukey's methods
  can be interpreted as checks against hypothetical linear models and Poisson
  distributions. In more complex situations, Bayesian methods can be useful for
  constructing reference distributions for various plots that are useful in
  exploratory data analysis. This article proposes an approach to unify
  exploratory data analysis with more formal statistical methods based on
  probability models. These ideas are developed in the context of examples from
  fields including psychology, medicine, and social science.},
  url = {http://www.stat.columbia.edu/~gelman/research/published/p755.pdf},
  file = {All_Papers/G/Gelman_2004_-_Exploratory_Data_Analysis_for_Complex_Models.pdf}
}

@BOOK{Jaynes2003-lx,
  title = {{Probability theory: the Logic of Science}},
  shorttitle = {{Probability theory}},
  author = {Jaynes, E T},
  editor = {Bretthorst, G Larry},
  publisher = {Cambridge University Press},
  location = {Cambridge, UK ; New York, NY},
  date = {2003-04-10},
  pagetotal = {727},
  isbn = {9780521592710},
  abstract = {The standard rules of probability can be interpreted as uniquely
  valid principles in logic. In this book, E. T. Jaynes dispels the imaginary
  distinction between 'probability theory' and 'statistical inference', leaving
  a logical unity and simplicity, which provides greater technical power and
  flexibility in applications. This book goes beyond the conventional
  mathematics of probability theory, viewing the subject in a wider context. New
  results are discussed, along with applications of probability theory to a wide
  variety of problems in physics, mathematics, economics, chemistry and biology.
  It contains many exercises and problems, and is suitable for use as a textbook
  on graduate level courses involving data analysis. The material is aimed at
  readers who are already familiar with applied mathematics at an advanced
  undergraduate level or higher. The book will be of interest to scientists
  working in any area where inference from incomplete information is necessary.},
  url = {https://market.android.com/details?id=book-tTN4HuUNXjgC},
  keywords = {notion},
  language = {en}
}

@ARTICLE{Gelman2017-zp,
  title = {{The prior can often only be understood in the context of the
  likelihood}},
  author = {Gelman, Andrew and Simpson, Daniel and Betancourt, Michael},
  journaltitle = {Entropy},
  publisher = {Multidisciplinary Digital Publishing Institute},
  volume = {19},
  issue = {10},
  pages = {555},
  date = {2017-10-19},
  doi = {10.3390/e19100555},
  issn = {1099-4300},
  abstract = {A key sticking point of Bayesian analysis is the choice of prior
  distribution, and there is a vast literature on potential defaults including
  uniform priors, Jeffreys’ priors, reference priors, maximum entropy priors,
  and weakly informative priors. These methods, however, often manifest a key
  conceptual tension in prior modeling: a model encoding true prior information
  should be chosen without reference to the model of the measurement process,
  but almost all common prior modeling techniques are implicitly motivated by a
  reference likelihood. In this paper we resolve this apparent paradox by
  placing the choice of prior into the context of the entire Bayesian analysis,
  from inference to prediction to model evaluation.},
  url = {http://www.mdpi.com/1099-4300/19/10/555},
  urldate = {2021-06-15},
  file = {All_Papers/G/Gelman_et_al._2017_-_The_prior_can_often_only_be_understood_in_the_context_of_the_likelihood.pdf},
  keywords = {notion},
  language = {en}
}

@ARTICLE{Gelman2013-dw,
  title = {{Philosophy and the practice of Bayesian statistics}},
  shorttitle = {{Philosophy and the practice of bayesian statistics}},
  author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
  journaltitle = {Br. J. Math. Stat. Psychol.},
  volume = {66},
  issue = {1},
  pages = {8--38},
  date = {2013-02},
  doi = {10.1111/j.2044-8317.2011.02037.x},
  pmc = {PMC4476974},
  pmid = {22364575},
  issn = {0007-1102,2044-8317},
  abstract = {A substantial school in the philosophy of science identifies
  Bayesian inference with inductive inference and even rationality as such, and
  seems to be strengthened by the rise and practical success of Bayesian
  statistics. We argue that the most successful forms of Bayesian statistics do
  not actually support that particular philosophy but rather accord much better
  with sophisticated forms of hypothetico-deductivism. We examine the actual
  role played by prior distributions in Bayesian models, and the crucial aspects
  of model checking and model revision, which fall outside the scope of Bayesian
  confirmation theory. We draw on the literature on the consistency of Bayesian
  updating and also on our experience of applied work in social science. Clarity
  about these matters should benefit not just philosophy of science, but also
  statistical practice. At best, the inductivist view has encouraged researchers
  to fit and compare models without checking them; at worst, theorists have
  actively discouraged practitioners from performing model checking because it
  does not fit into their framework.},
  url = {http://doi.wiley.com/10.1111/j.2044-8317.2011.02037.x},
  urldate = {2021-06-15},
  file = {All_Papers/G/Gelman_and_Shalizi_2013_-_Philosophy_and_the_practice_of_Bayesian_statistics.pdf},
  keywords = {notion},
  language = {en}
}

@ARTICLE{Doss-Gollin2021-kc,
  title = {{How unprecedented was the February 2021 Texas cold snap?}},
  author = {Doss-Gollin, James and Farnham, David J and Lall, Upmanu and Modi,
  Vijay},
  journaltitle = {Environ. Res. Lett.},
  volume = {16},
  issue = {6},
  pages = {064056},
  date = {2021-06},
  doi = {10/gnswvt},
  issn = {1748-9326},
  abstract = {Winter storm Uri brought severe cold to the southern United States
  in February 2021, causing a cascading failure of interdependent systems in
  Texas where infrastructure was not adequately prepared for such cold. In
  particular, the failure of interconnected energy systems restricted
  electricity supply just as demand for heating spiked, leaving millions of
  Texans without heat or electricity, many for several days. This motivates the
  question: did historical storms suggest that such temperatures were known to
  occur, and if so with what frequency? We compute a temperature-based proxy for
  heating demand and use this metric to answer the question ‘what would the
  aggregate demand for heating have been had historic cold snaps occurred with
  today’s population?’. We find that local temperatures and the inferred demand
  for heating per capita across the region served by the Texas Interconnection
  were more severe during a storm in December 1989 than during February 2021,
  and that cold snaps in 1951 and 1983 were nearly as severe. Given anticipated
  population growth, future storms may lead to even greater infrastructure
  failures if adaptive investments are not made. Further, electricity system
  managers should prepare for trends in electrification of heating to drive peak
  annual loads on the Texas Interconnection during severe winter storms.},
  url = {https://doi.org/10.1088/1748-9326/ac0278},
  urldate = {2021-12-17},
  file = {All_Papers/D/Doss-Gollin_et_al._2021_-_How_unprecedented_was_the_February_2021_Texas_cold_snap.pdf},
  keywords = {notion},
  language = {en}
}

@ARTICLE{Zarekarizi2020-ky,
  title = {{Neglecting uncertainties biases house-elevation decisions to manage
  riverine flood risks}},
  author = {Zarekarizi, Mahkameh and Srikrishnan, Vivek and Keller, Klaus},
  journaltitle = {Nat. Commun.},
  volume = {11},
  issue = {1},
  pages = {5361},
  date = {2020-10-26},
  doi = {10.1038/s41467-020-19188-9},
  pmc = {PMC7588474},
  pmid = {33106490},
  issn = {2041-1723},
  abstract = {Homeowners around the world elevate houses to manage flood risks.
  Deciding how high to elevate a house poses a nontrivial decision problem. The
  U.S. Federal Emergency Management Agency (FEMA) recommends elevating existing
  houses to the Base Flood Elevation (the elevation of the 100-year flood) plus
  a freeboard. This recommendation neglects many uncertainties. Here we analyze
  a case-study of riverine flood risk management using a multi-objective robust
  decision-making framework in the face of deep uncertainties. While the
  quantitative results are location-specific, the approach and overall insights
  are generalizable. We find strong interactions between the economic,
  engineering, and Earth science uncertainties, illustrating the need for
  expanding on previous integrated analyses to further understand the nature and
  strength of these connections. Considering deep uncertainties surrounding
  flood hazards, the discount rate, the house lifetime, and the fragility can
  increase the economically optimal house elevation to values well above FEMA's
  recommendation.},
  url = {http://dx.doi.org/10.1038/s41467-020-19188-9},
  urldate = {2021-06-14},
  file = {All_Papers/Z/Zarekarizi_et_al._2020_-_Zarekarizi_et_al._-_2020_-_Neglecting_uncertainties_biases_house-elevation_de.pdf;All_Papers/Z/Zarekarizi_et_al._2020_-_Neglecting_uncertainties_biases_house-elevation_decisions_to_manage_riverine_flood_risks.pdf},
  keywords = {for Nino;notion;my-papers},
  language = {en}
}

@ARTICLE{Arns2013-tl,
  title = {{Estimating extreme water level probabilities: A comparison of the
  direct methods and recommendations for best practise}},
  author = {Arns, A and Wahl, T and Haigh, I D and Jensen, J and Pattiaratchi, C},
  journaltitle = {Coast. Eng.},
  volume = {81},
  pages = {51--66},
  date = {2013-11-01},
  doi = {10.1016/j.coastaleng.2013.07.003},
  issn = {0378-3839},
  abstract = {Over the past five decades, several approaches for estimating
  probabilities of extreme still water levels have been developed. Currently,
  different methods are applied not only on transnational, but also on national
  scales, resulting in a heterogeneous level of protection. Applying different
  statistical methods can yield significantly different estimates of return
  water levels, but even the use of the same technique can produce large
  discrepancies, because there is subjective parameter choice at several steps
  in the model setup. In this paper, we compare probabilities of extreme still
  water levels estimated using the main direct methods (i.e. the block maxima
  method and the peaks over threshold method) considering a wide range of
  strategies to create extreme value dataset and a range of different model
  setups. We primarily use tide gauge records from the German Bight but also
  consider data from sites around the UK and Australia for comparison. The focus
  is on testing the influence of the following three main factors, which can
  affect the estimates of extreme value statistics: (1) detrending the original
  data sets; (2) building samples of extreme values from the original data sets;
  and (3) the record lengths of the original data sets. We find that using
  different detrending techniques biases the results from extreme value
  statistics. Hence, we recommend using a 1-year moving average of high waters
  (or hourly records if these are available) to correct the original data sets
  for seasonal and long-term sea level changes. Our results highlight that the
  peaks over threshold method yields more reliable and more stable (i.e. using
  short records leads to the same results as when using long records) estimates
  of probabilities of extreme still water levels than the block maxima method.
  In analysing a variety of threshold selection methods we find that using the
  99.7th percentile water level leads to the most stable return water level
  estimates along the German Bight. This is also valid for the international
  stations considered. Finally, to provide guidance for coastal engineers and
  operators, we recommend the peaks over threshold method and define an
  objective approach for setting up the model. If this is applied routinely
  around a country, it will help overcome the problem of heterogeneous levels of
  protection resulting from different methods and varying model setups.},
  url = {https://www.sciencedirect.com/science/article/pii/S0378383913001270},
  file = {All_Papers/A/Arns_et_al._2013_-_Estimating_extreme_water_level_probabilities_-_A_comparison_of_the_direct_methods_and_recommendations_for_best_practise.pdf},
  keywords = {Extreme water levels; Coastal flooding; Return levels and periods;
  Extreme value distribution; German Bight}
}

@ARTICLE{Ferro2003-jm,
  title = {{Inference for clusters of extreme values}},
  author = {Ferro, Christopher A T and Segers, Johan},
  journaltitle = {J. R. Stat. Soc. Series B Stat. Methodol.},
  publisher = {Oxford University Press (OUP)},
  volume = {65},
  issue = {2},
  pages = {545--556},
  date = {2003-05-01},
  doi = {10.1111/1467-9868.00401},
  issn = {1467-9868,1369-7412},
  abstract = {SummaryInference for clusters of extreme values of a time series
  typically requires the identification of independent clusters of exceedances
  over a high threshold. The choice of declustering scheme often has a
  significant effect on estimates of cluster characteristics. We propose an
  automatic declustering scheme that is justified by an asymptotic result for
  the times between threshold exceedances. The scheme relies on the extremal
  index, which we show may be estimated before declustering, and supports a
  bootstrap procedure for assessing the variability of estimates.},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00401},
  urldate = {2024-02-18},
  file = {All_Papers/F/Ferro_and_Segers_2003_-_Inference_for_clusters_of_extreme_values.pdf},
  keywords = {Bootstrap; Declustering; Extremal index; Extreme values;
  Interexceedance times},
  language = {en}
}

@BOOK{Coles2001-sg,
  title = {{An Introduction to Statistical Modeling of Extreme Values}},
  author = {Coles, Stuart},
  publisher = {Springer-Verlag London},
  location = {London},
  date = {2001},
  isbn = {9781852334598}
}


@BOOK{Munzner2014-pj,
  title     = "Visualization Analysis and Design",
  author    = "Munzner, Tamara",
  abstract  = "Learn How to Design Effective Visualization SystemsVisualization
               Analysis and Design provides a systematic, comprehensive
               framework for thinking about visualization in terms of
               principles and design choices. The book features a unified
               approach encompassing information visualization techniques for
               abstract data, scientific visualization techniques",
  publisher = "CRC Press",
  month     =  dec,
  year      =  2014,
  language  = "en",
  isbn      = "9781498759717"
}


@BOOK{Healy2018-zx,
  title     = "Data Visualization: A Practical Introduction",
  author    = "Healy, Kieran",
  abstract  = "An accessible primer on how to create effective graphics from
               dataThis book provides students and researchers a hands-on
               introduction to the principles and practice of data
               visualization. It explains what makes some graphs succeed while
               others fail, how to make high-quality figures from data using
               powerful and reproducible methods, and how to think about data
               visualization in an honest and effective way.Data Visualization
               builds the reader's expertise in ggplot2, a versatile
               visualization library for the R programming language. Through a
               series of worked examples, this accessible primer then
               demonstrates how to create plots piece by piece, beginning with
               summaries of single variables and moving on to more complex
               graphics. Topics include plotting continuous and categorical
               variables; layering information on graphics; producing effective
               ``small multiple'' plots; grouping, summarizing, and
               transforming data for plotting; creating maps; working with the
               output of statistical models; and refining plots to make them
               more comprehensible.Effective graphics are essential to
               communicating ideas and a great way to better understand data.
               This book provides the practical skills students and
               practitioners need to visualize quantitative data and get the
               most out of their research findings.Provides hands-on
               instruction using R and ggplot2Shows how the ``tidyverse'' of
               data analysis tools makes working with R easier and more
               consistentIncludes a library of data sets, code, and functions",
  publisher = "Princeton University Press",
  month     =  dec,
  year      =  2018,
  language  = "en",
  isbn      = "9780691181622"
}


@UNPUBLISHED{Shu2023-ht,
  title    = "Future population exposure to flood risk: A decomposition
              approach across {Shared-Socioeconomic} Pathways ({SSPs})",
  author   = "Shu, Evelyn and Hauer, Mathew and Porter, Jeremy",
  abstract = "Abstract Extreme weather events, such as flooding, are expected
              to become more severe due to climate change. These impacts are
              connected to impacts on human systems including economic, social,
              and political crises. Adding to the challenge, populations have
              concurrently settled in risky areas that were previously thought
              to have low, or no, exposure. As such, it is important to
              understand how population exposure to flooding may change over
              time. Here, we build on existing research to project future
              populations with their demographic components under all five
              Shared Socioeconomic Pathways (SSPs) at the block group level
              across the US. These projections are evaluated with future flood
              projections to characterize exposure changes over time. Broadly,
              across the five SSPs, the most increase in exposure will occur in
              SSP5 (+470,719), and the least will occur in SSP 3 (+57,189).
              Across all five SSPs, we find that there will be significant
              increases in expected annual exposure for populations 80 years
              and older, with an increase in exposure by approximately 216\%
              under SSP5. By decomposing the contributions from flood and
              population growth, we find that the population growth induced
              effect contributed to an increase in the population exposure for
              all of the SSPs except for SSP3. In SSP5, this effect was the
              largest (+72.7\% more exposure), while in SSP3, the population
              effect decreased the overall exposure by 83.5\%.",
  journal  = "Research Square",
  month    =  nov,
  year     =  2023,
  language = "en",
  doi      = "10.21203/rs.3.rs-3628132/v1"
}

@ARTICLE{Bakkensen2022-xj,
  title = {{Going Underwater? Flood Risk Belief Heterogeneity and Coastal Home
  Price Dynamics}},
  author = {Bakkensen, Laura A and Barrage, Lint},
  journaltitle = {Rev. Financ. Stud.},
  publisher = {Oxford Academic},
  volume = {35},
  issue = {8},
  pages = {3666--3709},
  date = {2022-07-18},
  doi = {10.1093/rfs/hhab122},
  issn = {0893-9454},
  abstract = {Abstract. How do climate risk beliefs affect coastal housing
  markets? This paper provides theoretical and empirical evidence. First, we
  build a dynamic housing},
  url = {https://academic.oup.com/rfs/article-pdf/35/8/3666/45018664/hhab122.pdf},
  urldate = {2023-05-16},
  file = {All_Papers/B/Bakkensen_and_Barrage_2022_-_Going_Underwater_-_Flood_Risk_Belief_Heterogeneity_and_Coastal_Home_Price_Dynamics.pdf}
}

@ARTICLE{Haer2020-cj,
  title = {{The safe development paradox: an agent-based model for flood risk
  under climate change in the european union}},
  shorttitle = {{The safe development paradox}},
  author = {Haer, Toon and Husby, Trond G and Botzen, W J Wouter and Aerts,
  Jeroen C J H},
  journaltitle = {Glob. Environ. Change},
  volume = {60},
  pages = {102009},
  date = {2020-01-01},
  doi = {10.1016/j.gloenvcha.2019.102009},
  issn = {0959-3780},
  abstract = {With increasing flood risk due to climate change and socioeconomic
  trends, governments are under pressure to continue implementing flood
  protection measures, such as dikes, to reduce flood risk. However, research
  suggests that a sole focus on government-funded flood protection leads to an
  adverse increase in exposure as people and economic activities tend to
  concentrate in protected areas. Moreover, governmental flood protection can
  reduce the incentive for autonomous adaptation by local households, which
  paradoxically results in more severe consequences if an extreme flood event
  occurs. This phenomenon is often referred to as the ‘safe development paradox’
  or ‘levee effect’ and is generally not accounted for in existing flood risk
  models used to assess developments in future flood risk under climate change.
  In this study we assess the impact of extreme flood events for the European
  Union using a large-scale agent-based model (ABM). We quantify how the safe
  development paradox affects (1) population growth and the increase in exposed
  property values, (2) the reduction in investments to flood-proof buildings as
  public protection increases, and (3) the increase in potential damage should a
  flood occur. For this analysis, we apply an ABM that integrates the dynamic
  behaviour of governments and residents into a large-scale flood risk
  assessment framework, in which we include estimates of changing population
  growth. We find that the impact of extreme flood events increases considerably
  when governments provide high protection levels, especially in large
  metropolitan areas. Moreover, we demonstrate how policy that stimulates the
  flood-proofing of buildings can largely counteract the effects of the safe
  development paradox.},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959378018314079},
  urldate = {2021-06-15},
  file = {All_Papers/H/Haer_et_al._2020_-_The_safe_development_paradox_-_an_agent-based_model_for_flood_risk_under_climate_change_in_the_european_union.pdf},
  keywords = {Adaptation policy; Agent-based model; Extreme events; Flood risk;
  Levee effect; Safe development paradox;notion},
  language = {en}
}

@ARTICLE{Kale2021-fv,
  title = {{Visual reasoning strategies for effect size judgments and decisions}},
  author = {Kale, Alex and Kay, Matthew and Hullman, Jessica},
  journaltitle = {IEEE Trans. Vis. Comput. Graph.},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume = {27},
  issue = {2},
  pages = {272--282},
  date = {2021-02},
  doi = {10.1109/TVCG.2020.3030335},
  pmid = {33048681},
  issn = {1077-2626,1941-0506},
  abstract = {Uncertainty visualizations often emphasize point estimates to
  support magnitude estimates or decisions through visual comparison. However,
  when design choices emphasize means, users may overlook uncertainty
  information and misinterpret visual distance as a proxy for effect size. We
  present findings from a mixed design experiment on Mechanical Turk which tests
  eight uncertainty visualization designs: 95\% containment intervals,
  hypothetical outcome plots, densities, and quantile dotplots, each with and
  without means added. We find that adding means to uncertainty visualizations
  has small biasing effects on both magnitude estimation and decision-making,
  consistent with discounting uncertainty. We also see that visualization
  designs that support the least biased effect size estimation do not support
  the best decision-making, suggesting that a chart user's sense of effect size
  may not necessarily be identical when they use the same information for
  different tasks. In a qualitative analysis of users' strategy descriptions, we
  find that many users switch strategies and do not employ an optimal strategy
  when one exists. Uncertainty visualizations which are optimally designed in
  theory may not be the most effective in practice because of the ways that
  users satisfice with heuristics, suggesting opportunities to better understand
  visualization effectiveness by modeling sets of potential strategies.},
  url = {https://mucollective.northwestern.edu/files/2020%20-%20Kale,%20Visual%20Reasoning%20Strategies%20for%20Effect%20Size%20Judgements.pdf},
  file = {All_Papers/K/Kale_et_al._2021_-_Visual_reasoning_strategies_for_effect_size_judgments_and_decisions.pdf},
  language = {en}
}

@ARTICLE{Efron1979-zv,
  title = {{Bootstrap methods: Another look at the jackknife}},
  author = {Efron, B},
  journaltitle = {Ann. Stat.},
  publisher = {Institute of Mathematical Statistics},
  volume = {7},
  issue = {1},
  pages = {1--26},
  date = {1979-01-01},
  doi = {10.1214/aos/1176344552},
  issn = {0090-5364,2168-8966},
  abstract = {We discuss the following problem: given a random sample
  $\mathbf{X} = (X_1, X_2, \cdots, X_n)$ from an unknown probability
  distribution $F$, estimate the sampling distribution of some prespecified
  random variable $R(\mathbf{X}, F)$, on the basis of the observed data
  $\mathbf{x}$. (Standard jackknife theory gives an approximate mean and
  variance in the case $R(\mathbf{X}, F) = \theta(\hat{F}) - \theta(F), \theta$
  some parameter of interest.) A general method, called the "bootstrap," is
  introduced, and shown to work satisfactorily on a variety of estimation
  problems. The jackknife is shown to be a linear approximation method for the
  bootstrap. The exposition proceeds by a series of examples: variance of the
  sample median, error rates in a linear discriminant analysis, ratio
  estimation, estimating regression parameters, etc.},
  url = {https://projecteuclid.org/journals/annals-of-statistics/volume-7/issue-1/Bootstrap-Methods-Another-Look-at-the-Jackknife/10.1214/aos/1176344552.short},
  urldate = {2024-03-05},
  file = {All_Papers/E/Efron_1979_-_Bootstrap_methods_-_Another_look_at_the_jackknife.pdf},
  keywords = {62G05; 62G15; 62H30; 62J05; bootstrap; discriminant analysis;
  error rate estimation; jackknife; Nonlinear regression; nonparametric variance
  estimation; Resampling; subsample values},
  language = {en}
}

@ARTICLE{Palmer2018-ha,
  title = {{Extending CMIP5 projections of global mean temperature change and
  sea level rise due to thermal expansion using a physically-based emulator}},
  author = {Palmer, Matthew D and Harris, Glen R and Gregory, Jonathan M},
  journaltitle = {Environ. Res. Lett.},
  publisher = {IOP Publishing},
  volume = {13},
  issue = {8},
  pages = {084003},
  date = {2018-07-23},
  doi = {10.1088/1748-9326/aad2e4},
  issn = {1748-9326},
  abstract = {Extending CMIP5 projections of global mean temperature change and
  sea level rise due to thermal expansion using a physically-based emulator,
  Palmer, Matthew D, Harris, Glen R, Gregory, Jonathan M},
  url = {https://iopscience.iop.org/article/10.1088/1748-9326/aad2e4/meta},
  urldate = {2025-02-08},
  file = {All_Papers/P/Palmer_et_al._2018_-_Extending_CMIP5_projections_of_global_mean_temperat_..._level_rise_due_to_thermal_expansion_using_a_physically-based_emulator.pdf},
  language = {en}
}


@ARTICLE{Vermeer2009-eo,
  title = {{Global sea level linked to global temperature}},
  author = {Vermeer, Martin and Rahmstorf, Stefan},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {106},
  issue = {51},
  pages = {21527--21532},
  date = {2009-12-22},
  doi = {10.1073/pnas.0907765106},
  pmid = {19995972},
  issn = {0027-8424,1091-6490},
  abstract = {We propose a simple relationship linking global sea-level
  variations on time scales of decades to centuries to global mean temperature.
  This relationship is tested on synthetic data from a global climate model for
  the past millennium and the next century. When applied to observed data of sea
  level and temperature for 1880–2000, and taking into account known
  anthropogenic hydrologic contributions to sea level, the correlation is >0.99,
  explaining 98\% of the variance. For future global temperature scenarios of
  the Intergovernmental Panel on Climate Change's Fourth Assessment Report, the
  relationship projects a sea-level rise ranging from 75 to 190 cm for the
  period 1990–2100.},
  url = {https://www.pnas.org/doi/10.1073/pnas.0907765106},
  urldate = {2023-02-07},
  file = {All_Papers/V/Vermeer_and_Rahmstorf_2009_-_Global_sea_level_linked_to_global_temperature.pdf;All_Papers/V/Vermeer_and_Rahmstorf_2009_-_Global_sea_level_linked_to_global_temperature.pdf},
  keywords = {Science \& Technology - Other Topics; climate; global warming;
  millennium; model; ocean; projections; rise}
}


@ARTICLE{Rahmstorf2011-oj,
  title = {{Increase of extreme events in a warming world}},
  author = {Rahmstorf, Stefan and Coumou, Dim},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {108},
  issue = {44},
  pages = {17905--17909},
  date = {2011-11-01},
  doi = {10.1073/pnas.1101766108},
  pmc = {PMC3207670},
  pmid = {22025683},
  issn = {0027-8424,1091-6490},
  abstract = {We develop a theoretical approach to quantify the effect of
  long-term trends on the expected number of extremes in generic time series,
  using analytical solutions and Monte Carlo simulations. We apply our method to
  study the effect of warming trends on heat records. We find that the number of
  record-breaking events increases approximately in proportion to the ratio of
  warming trend to short-term standard deviation. Short-term variability thus
  decreases the number of heat extremes, whereas a climatic warming increases
  it. For extremes exceeding a predefined threshold, the dependence on the
  warming trend is highly nonlinear. We further find that the sum of warm plus
  cold extremes increases with any climate change, whether warming or cooling.
  We estimate that climatic warming has increased the number of new global-mean
  temperature records expected in the last decade from 0.1 to 2.8. For July
  temperature in Moscow, we estimate that the local warming trend has increased
  the number of records expected in the past decade fivefold, which implies an
  approximate 80\% probability that the 2010 July heat record would not have
  occurred without climate warming.},
  url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1101766108},
  urldate = {2021-06-15},
  file = {All_Papers/R/Rahmstorf_and_Coumou_2011_-_Increase_of_extreme_events_in_a_warming_world.pdf},
  keywords = {notion},
  language = {en}
}

@ARTICLE{Metropolis1953-rv,
  title = {{Equation of State Calculations by Fast Computing Machines}},
  author = {Metropolis, Nicholas and Rosenbluth, Arianna W and Rosenbluth,
  Marshall N and Teller, Augusta H and Teller, Edward},
  journaltitle = {J. Chem. Phys.},
  volume = {21},
  issue = {6},
  pages = {1087--1092},
  date = {1953-06},
  doi = {10.1063/1.1699114},
  issn = {0021-9606,1089-7690},
  url = {http://aip.scitation.org/doi/10.1063/1.1699114},
  file = {All_Papers/M/Metropolis_et_al._1953_-_Equation_of_State_Calculations_by_Fast_Computing_Machines.pdf}
}

@ARTICLE{Gelman1992-da,
  title = {{Inference from Iterative Simulation Using Multiple Simulations}},
  author = {Gelman, Andrew and Rubin, Donald B},
  journaltitle = {Stat. Sci.},
  volume = {7},
  issue = {4},
  pages = {457--511},
  date = {1992},
  doi = {10.1214/ss/1177011136},
  issn = {0883-4237},
  url = {http://projecteuclid.org/euclid.ss/1177010123},
  file = {All_Papers/G/Gelman_and_Rubin_1992_-_Inference_from_Iterative_Simulation_Using_Multiple_Simulations.pdf}
}

@ARTICLE{Hoeting2021-vx,
  title = {{Bayesian model averaging: a tutorial}},
  author = {Hoeting, Jennifer A and Madigan, David and Raftery, Adrian E and
  Volinsky, Chris T},
  journaltitle = {Stat. Sci.},
  publisher = {Institute of Mathematical Statistics},
  volume = {14},
  issue = {4},
  pages = {382--401},
  date = {2021},
  doi = {10.1214/ss/1009212519},
  issn = {0883-4237},
  abstract = {[Standard statistical practice ignores model uncertainty. Data
  analysts typically select a model from some class of models and then proceed
  as if the selected model had generated the data. This approach ignores the
  uncertainty in model selection, leading to over-confident inferences and
  decisions that are more risky than one thinks they are. Bayesian model
  averaging (BMA) provides a coherent mechanism for accounting for this model
  uncertainty. Several methods for implementing BMA have recently emerged. We
  discuss these methods and present a number of examples. In these examples, BMA
  provides improved out-of-sample predictive performance. We also provide a
  catalogue of currently available BMA software.]},
  url = {http://www.jstor.org/stable/2676803},
  file = {All_Papers/H/Hoeting_et_al._2021_-_Bayesian_model_averaging_-_a_tutorial.pdf},
  keywords = {notion},
  language = {en}
}


@ARTICLE{Burkner2020-lo,
  title         = "Approximate leave-future-out cross-validation for Bayesian
                   time series models",
  author        = "B{\"u}rkner, Paul-Christian and Gabry, Jonah and Vehtari,
                   Aki",
  abstract      = "One of the common goals of time series analysis is to use
                   the observed series to inform predictions for future
                   observations. In the absence of any actual new data to
                   predict, cross-validation can be used to estimate a model's
                   future predictive accuracy, for instance, for the purpose of
                   model comparison or selection. Exact cross-validation for
                   Bayesian models is often computationally expensive, but
                   approximate cross-validation methods have been developed,
                   most notably methods for leave-one-out cross-validation
                   (LOO-CV). If the actual prediction task is to predict the
                   future given the past, LOO-CV provides an overly optimistic
                   estimate because the information from future observations is
                   available to influence predictions of the past. To properly
                   account for the time series structure, we can use
                   leave-future-out cross-validation (LFO-CV). Like exact
                   LOO-CV, exact LFO-CV requires refitting the model many times
                   to different subsets of the data. Using Pareto smoothed
                   importance sampling, we propose a method for approximating
                   exact LFO-CV that drastically reduces the computational
                   costs while also providing informative diagnostics about the
                   quality of the approximation.",
  journal       = "J. Stat. Comput. Simul.",
  number        =  14,
  pages         = "2499--2523",
  month         =  sep,
  year          =  2020,
  archivePrefix = "arXiv",
  eprint        = "1902.06281",
  primaryClass  = "stat.ME",
  arxivid       = "1902.06281",
  doi           = "10.1080/00949655.2020.1783262"
}

@ARTICLE{Yao2018-rr,
  title         = "Using stacking to average Bayesian predictive distributions",
  author        = "Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman,
                   Andrew",
  abstract      = "The widely recommended procedure of Bayesian model averaging
                   is flawed in the M-open setting in which the true
                   data-generating process is not one of the candidate models
                   being fit. We take the idea of stacking from the point
                   estimation literature and generalize to the combination of
                   predictive distributions, extending the utility function to
                   any proper scoring rule, using Pareto smoothed importance
                   sampling to efficiently compute the required leave-one-out
                   posterior distributions and regularization to get more
                   stability. We compare stacking of predictive distributions
                   to several alternatives: stacking of means, Bayesian model
                   averaging (BMA), pseudo-BMA using AIC-type weighting, and a
                   variant of pseudo-BMA that is stabilized using the Bayesian
                   bootstrap. Based on simulations and real-data applications,
                   we recommend stacking of predictive distributions, with
                   BB-pseudo-BMA as an approximate alternative when computation
                   cost is an issue.",
  number        =  3,
  month         =  sep,
  year          =  2018,
  language      = "en",
  archivePrefix = "arXiv",
  eprint        = "1704.02030",
  primaryClass  = "stat.ME",
  arxivid       = "1704.02030",
  doi           = "10.1214/17-BA1091"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Vehtari2017-oi,
  title     = "Practical Bayesian model evaluation using leave-one-out
               cross-validation and {WAIC}",
  author    = "Vehtari, Aki and Gelman, Andrew and Gabry, Jonah",
  abstract  = "Leave-one-out cross-validation (LOO) and the widely applicable
               information criterion (WAIC) are methods for estimating
               pointwise out-of-sample prediction accuracy from a ﬁtted
               Bayesian model using the log-likelihood evaluated at the
               posterior simulations of the parameter values. LOO and WAIC have
               various advantages over simpler estimates of predictive error
               such as AIC and DIC but are less used in practice because they
               involve additional computational steps. Here we lay out fast and
               stable computations for LOO and WAIC that can be performed using
               existing simulation draws. We introduce an efﬁcient computation
               of LOO using Pareto-smoothed importance sampling (PSIS), a new
               procedure for regularizing importance weights. Although WAIC is
               asymptotically equal to LOO, we demonstrate that PSIS-LOO is
               more robust in the ﬁnite case with weak priors or inﬂuential
               observations. As a byproduct of our calculations, we also obtain
               approximate standard errors for estimated predictive errors and
               for comparison of predictive errors between two models. We
               implement the computations in an R package called loo and
               demonstrate using models ﬁt with the Bayesian inference package
               Stan.",
  journal   = "Stat. Comput.",
  publisher = "Springer",
  volume    =  27,
  number    =  5,
  pages     = "1413--1432",
  year      =  2017,
  language  = "en",
  issn      = "0960-3174, 1573-1375",
  doi       = "10.1007/s11222-016-9696-4"
}


@ARTICLE{Haran2017-vz,
  title     = "Statistics and the Future of the Antarctic Ice Sheet",
  author    = "Haran, Murali and Chang, Won and Keller, Klaus and Nicholas,
               Robert and Pollard, David",
  journal   = "Chance",
  publisher = "Taylor \& Francis",
  volume    =  30,
  number    =  4,
  pages     = "37--44",
  month     =  oct,
  year      =  2017,
  issn      = "0933-2480",
  doi       = "10.1080/09332480.2017.1406758"
}

@BOOK{Reed2022-fm,
  title = {{Addressing uncertainty in multisector dynamics research}},
  author = {Reed, Patrick M and Hadjimichael, Antonia and Malek, Keyvan and
  Karimi, Tina and Vernon, Chris R and Srikrishnan, Vivek and Gupta, Rohini S
  and Gold, David F and Lee, Ben and Keller, Klaus and Rice, Jennie S and
  Thurber, Travis B},
  publisher = {Zenodo},
  date = {2022},
  url = {https://immm-sfa.github.io/msd_uncertainty_ebook/},
  keywords = {notion}
}


@ARTICLE{Lee2020-ws,
  title     = "A fast particle-based approach for calibrating a {3-D} model of
               the Antarctic ice sheet",
  author    = "Lee, Ben Seiyon and Haran, Murali and Fuller, Robert W and
               Pollard, David and Keller, Klaus",
  abstract  = "We consider the scientifically challenging and policy-relevant
               task of understanding the past and projecting the future
               dynamics of the Antarctic ice sheet. The Antarctic ice sheet has
               shown a highly nonlinear threshold response to past climate
               forcings. Triggering such a threshold response through
               anthropogenic greenhouse gas emissions would drive drastic and
               potentially fast sea level rise with important implications for
               coastal flood risks. Previous studies have combined information
               from ice sheet models and observations to calibrate model
               parameters. These studies have broken important new ground but
               have either adopted simple ice sheet models or have limited the
               number of parameters to allow for the use of more complex
               models. These limitations are largely due to the computational
               challenges posed by calibration as models become more
               computationally intensive or when the number of parameters
               increases. Here, we propose a method to alleviate this problem:
               a fast sequential Monte Carlo method that takes advantage of the
               massive parallelization afforded by modern high-performance
               computing systems. We use simulated examples to demonstrate how
               our sample-based approach provides accurate approximations to
               the posterior distributions of the calibrated parameters. The
               drastic reduction in computational times enables us to provide
               new insights into important scientific questions, for example,
               the impact of Pliocene era data and prior parameter information
               on sea level projections. These studies would be computationally
               prohibitive with other computational approaches for calibration
               such as Markov chain Monte Carlo or emulation-based methods. We
               also find considerable differences in the distributions of sea
               level projections when we account for a larger number of
               uncertain parameters. For example, based on the same ice sheet
               model and data set, the 99th percentile of the Antarctic ice
               sheet contribution to sea level rise in 2300 increases from 6.5
               m to 13.1 m when we increase the number of calibrated parameters
               from three to 11. With previous calibration methods, it would be
               challenging to go beyond five parameters. This work provides an
               important next step toward improving the uncertainty
               quantification of complex, computationally intensive and
               decision-relevant models.",
  journal   = "Ann. Appl. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  14,
  number    =  2,
  pages     = "605--634",
  month     =  jun,
  year      =  2020,
  keywords  = "Antarctic ice sheet model; computer model calibration;
               paleoclimate; sequential Monte Carlo; uncertainty quantification",
  language  = "en",
  issn      = "1932-6157, 1941-7330",
  doi       = "10.1214/19-AOAS1305"
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Helgeson2021-ok,
  title    = "Why simpler computer simulation models can be epistemically
              better for informing decisions",
  author   = "Helgeson, Casey and Srikrishnan, Vivek and Keller, Klaus and
              Tuana, Nancy",
  abstract = "For computer simulation models to usefully inform climate risk
              management decisions, uncertainties in model projections must be
              explored and characterized. Because doing so requires running the
              model many times over, and because computing resources are ﬁnite,
              uncertainty assessment is more feasible using models that need
              less computer processor time. Such models are generally simpler
              in the sense of being more idealized, or less realistic. So
              modelers face a trade-oﬀ between realism and extent of
              uncertainty quantiﬁcation. Seeing this trade-oﬀ for the important
              epistemic issue that it is requires a shift in perspective from
              the established simplicity literature in philosophy of science.",
  journal  = "Philos. Sci.",
  volume   =  88,
  number   =  2,
  pages    = "213--233",
  month    =  apr,
  year     =  2021,
  language = "en",
  issn     = "0031-8248, 1539-767X",
  doi      = "10.1086/711501"
}


@ARTICLE{Yao2018-rr,
  title = {{Using stacking to average Bayesian predictive distributions}},
  author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
  journaltitle = {arXiv [stat.ME]},
  issue = {3},
  date = {2018-09-01},
  eprint = {1704.02030},
  eprintclass = {stat.ME},
  doi = {10.1214/17-BA1091},
  abstract = {The widely recommended procedure of Bayesian model averaging is
  flawed in the M-open setting in which the true data-generating process is not
  one of the candidate models being fit. We take the idea of stacking from the
  point estimation literature and generalize to the combination of predictive
  distributions, extending the utility function to any proper scoring rule,
  using Pareto smoothed importance sampling to efficiently compute the required
  leave-one-out posterior distributions and regularization to get more
  stability. We compare stacking of predictive distributions to several
  alternatives: stacking of means, Bayesian model averaging (BMA), pseudo-BMA
  using AIC-type weighting, and a variant of pseudo-BMA that is stabilized using
  the Bayesian bootstrap. Based on simulations and real-data applications, we
  recommend stacking of predictive distributions, with BB-pseudo-BMA as an
  approximate alternative when computation cost is an issue.},
  url = {http://dx.doi.org/10/gddmx4},
  urldate = {2021-06-15},
  file = {All_Papers/Y/Yao_et_al._2018_-_Yao_et_al._-_2018_-_Using_stacking_to_average_Bayesian_predictive_dist.pdf},
  keywords = {notion},
  language = {en}
}

@ARTICLE{Freedman1983-xq,
  title = {{A note on screening regression equations}},
  author = {Freedman, David A},
  journaltitle = {Am. Stat.},
  publisher = {JSTOR},
  volume = {37},
  issue = {2},
  pages = {152},
  date = {1983-05},
  doi = {10.2307/2685877},
  issn = {0003-1305,1537-2731},
  abstract = {David A. Freedman, A Note on Screening Regression Equations, The
  American Statistician, Vol. 37, No. 2 (May, 1983), pp. 152-155},
  url = {https://www.jstor.org/stable/2685877},
  urldate = {2024-04-16},
  language = {en}
}


@ARTICLE{Smith2018-wt,
  title = {{Step away from stepwise}},
  author = {Smith, Gary},
  journaltitle = {J. Big Data},
  publisher = {Springer Science and Business Media LLC},
  volume = {5},
  issue = {1},
  pages = {1--12},
  date = {2018-12-15},
  doi = {10.1186/s40537-018-0143-6},
  issn = {2196-1115,2196-1115},
  abstract = {BackgroundStepwise regression is a popular data-mining tool that
  uses statistical significance to select the explanatory variables to be used
  in a multiple-regression model.FindingsA fundamental problem with stepwise
  regression is that some real explanatory variables that have causal effects on
  the dependent variable may happen to not be statistically significant, while
  nuisance variables may be coincidentally significant. As a result, the model
  may fit the data well in-sample, but do poorly out-of-sample.ConclusionMany
  Big-Data researchers believe that, the larger the number of possible
  explanatory variables, the more useful is stepwise regression for selecting
  explanatory variables. The reality is that stepwise regression is less
  effective the larger the number of potential explanatory variables. Stepwise
  regression does not solve the Big-Data problem of too many explanatory
  variables. Big Data exacerbates the failings of stepwise regression.},
  url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-018-0143-6},
  urldate = {2024-04-16},
  language = {en}
}


@ARTICLE{Burnham2004-do,
  title = {{Multimodel Inference: Understanding AIC and BIC in Model Selection}},
  author = {Burnham, Kenneth P and Anderson, David R},
  journaltitle = {Sociol. Methods Res.},
  publisher = {SAGE Publications Inc},
  volume = {33},
  issue = {2},
  pages = {261--304},
  date = {2004-11-01},
  doi = {10.1177/0049124104268644},
  issn = {0049-1241},
  abstract = {The model selection literature has been generally poor at
  reflecting the deep foundations of the Akaike information criterion (AIC) and
  at making appropriate comparisons to the Bayesian information criterion (BIC).
  There is a clear philosophy, a sound criterion based in information theory,
  and a rigorous statistical foundation for AIC. AIC can be justified as
  Bayesian using a ?savvy? prior on models that is a function of sample size and
  the number of model parameters. Furthermore, BIC can be derived as a
  non-Bayesian result. Therefore, arguments about using AIC versus BIC for model
  selection cannot be from a Bayes versus frequentist perspective. The
  philosophical context of what is assumed about reality, approximating models,
  and the intent of model-based inference should determine whether AIC or BIC is
  used. Various facets of such multimodel inference are presented here,
  particularly methods of model averaging.},
  url = {https://doi.org/10.1177/0049124104268644},
  note = {doi: 10.1177/0049124104268644},
  file = {All_Papers/B/Burnham_and_Anderson_2004_-_Multimodel_Inference_-_Understanding_AIC_and_BIC_in_Model_Selection.pdf}
}

@book{shumstof2025,
  author = {Shumway, Robert H. and Stoffer, David S.},
  publisher = {Springer},
  title = {Time Series Analysis and Its Applications},
  isbn = {978-3-031-70583-0},
  year = 2025
}

@BOOK{Hyndman2021-mw,
  title = {{Forecasting: Principles and Practice (3rd ed)}},
  author = {Hyndman, Rob J and Athanasopoulos, George},
  publisher = {OTexts},
  location = {Melbourne, Australia},
  date = {2021},
  abstract = {3rd edition},
  url = {https://otexts.com/fpp3/},
  urldate = {2025-02-04}
}

@BOOK{Durbin2012-pn,
  title = {{Time series analysis by state space methods}},
  author = {Durbin, James and Koopman, Siem Jan},
  publisher = {Oxford University Press},
  location = {London, England},
  edition = {2},
  date = {2012-05-03},
  pagetotal = {368},
  doi = {10.1093/acprof:oso/9780199641178.001.0001},
  isbn = {9780199641178},
  series = {Oxford Statistical Science Series},
  url = {http://dx.doi.org/10.1093/acprof:oso/9780199641178.001.0001},
  language = {en}
}

@BOOK{Cressie2011-pj,
  title = {{Statistics for Spatio-Temporal Data}},
  author = {Cressie, Noel and Wikle, Christopher K},
  publisher = {Wiley},
  location = {Hoboken, NJ},
  date = {2011}
}

@BOOK{Banerjee2011-dg,
  title = {{Hierarchical modeling and analysis for spatial data, second edition}},
  author = {Banerjee, Sudipto and Carlin, Bradley P and Gelfand, Alan E},
  publisher = {Chapman \& Hall/CRC},
  location = {Philadelphia, PA},
  edition = {2},
  date = {2011-06-15},
  pagetotal = {584},
  doi = {10.1201/b17115},
  isbn = {9781439819180},
  series = {Chapman \& Hall/CRC Monographs on Statistics \& Applied Probability},
  url = {http://dx.doi.org/10.1201/b17115}
}



@ARTICLE{Hannigan2024-cv,
  title = {{Beware of botshit: How to manage the epistemic risks of generative
  chatbots}},
  author = {Hannigan, Timothy R and McCarthy, Ian P and Spicer, André},
  journaltitle = {Bus. Horiz.},
  publisher = {Elsevier BV},
  volume = {67},
  issue = {5},
  pages = {471--486},
  date = {2024-03-01},
  doi = {10.1016/j.bushor.2024.03.001},
  issn = {0007-6813,1873-6068},
  abstract = {Advances in large language model (LLM) technology enable chatbots
  to generate and analyze content for our work. Generative chatbots do this work
  by pr…},
  url = {http://dx.doi.org/10.1016/j.bushor.2024.03.001},
  urldate = {2025-01-16},
  language = {en}
}


@ARTICLE{Hicks2024-tp,
  title = {{ChatGPT is bullshit}},
  author = {Hicks, Michael Townsen and Humphries, James and Slater, Joe},
  journaltitle = {Ethics Inf. Technol.},
  publisher = {Springer Science and Business Media LLC},
  volume = {26},
  issue = {2},
  pages = {1--10},
  date = {2024-06-08},
  doi = {10.1007/s10676-024-09775-5},
  issn = {1388-1957,1572-8439},
  abstract = {AbstractRecently, there has been considerable interest in large
  language models: machine learning systems which produce human-like text and
  dialogue. Applications of these systems have been plagued by persistent
  inaccuracies in their output; these are often called “AI hallucinations”. We
  argue that these falsehoods, and the overall activity of large language
  models, is better understood as bullshit in the sense explored by Frankfurt
  (On Bullshit, Princeton, 2005): the models are in an important way indifferent
  to the truth of their outputs. We distinguish two ways in which the models can
  be said to be bullshitters, and argue that they clearly meet at least one of
  these definitions. We further argue that describing AI misrepresentations as
  bullshit is both a more useful and more accurate way of predicting and
  discussing the behaviour of these systems.},
  url = {https://link.springer.com/article/10.1007/s10676-024-09775-5},
  urldate = {2025-01-16},
  file = {All_Papers/H/Hicks_et_al._2024_-_ChatGPT_is_bullshit.pdf},
  language = {en}
}

@book{frankfurt_bullshit_2005,
	address = {Princeton, NJ},
	title = {On bullshit},
	isbn = {978-0-691-12294-6},
	publisher = {Princeton University Press},
	author = {Frankfurt, Harry G.},
	year = {2005},
	keywords = {Truthfulness and falsehood},
}