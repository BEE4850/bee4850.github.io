@MISC{Caldwell2015-kc,
  title     = "Sea level measured by tide gauges from global oceans --- the
               Joint Archive for Sea Level holdings ({NCEI} Accession 0019568)",
  author    = "Caldwell, P C and Merrifield, M A and Thompson, P R",
  publisher = "NOAA National Centers for Environmental Information (NCEI)",
  year      =  2015,
  doi       = "10.7289/V5V40S7W"
}

@ARTICLE{Hoffman2014-gv,
  title    = "The {No-U-Turn} Sampler: Adaptively Setting Path Lengths in
              {H}amiltonian {M}onte {C}arlo",
  author   = "Hoffman, Matthew D and Gelman, Andrew",
  journal  = "J. Mach. Learn. Res.",
  volume   =  15,
  number   =  47,
  pages    = "1593--1623",
  year     =  2014,
  issn     = "1532-4435, 1533-7928"
}


@ARTICLE{Arns2013-tl,
  title    = "Estimating extreme water level probabilities: A comparison of the
              direct methods and recommendations for best practise",
  author   = "Arns, A and Wahl, T and Haigh, I D and Jensen, J and
              Pattiaratchi, C",
  abstract = "Over the past five decades, several approaches for estimating
              probabilities of extreme still water levels have been developed.
              Currently, different methods are applied not only on
              transnational, but also on national scales, resulting in a
              heterogeneous level of protection. Applying different statistical
              methods can yield significantly different estimates of return
              water levels, but even the use of the same technique can produce
              large discrepancies, because there is subjective parameter choice
              at several steps in the model setup. In this paper, we compare
              probabilities of extreme still water levels estimated using the
              main direct methods (i.e. the block maxima method and the peaks
              over threshold method) considering a wide range of strategies to
              create extreme value dataset and a range of different model
              setups. We primarily use tide gauge records from the German Bight
              but also consider data from sites around the UK and Australia for
              comparison. The focus is on testing the influence of the
              following three main factors, which can affect the estimates of
              extreme value statistics: (1) detrending the original data sets;
              (2) building samples of extreme values from the original data
              sets; and (3) the record lengths of the original data sets. We
              find that using different detrending techniques biases the
              results from extreme value statistics. Hence, we recommend using
              a 1-year moving average of high waters (or hourly records if
              these are available) to correct the original data sets for
              seasonal and long-term sea level changes. Our results highlight
              that the peaks over threshold method yields more reliable and
              more stable (i.e. using short records leads to the same results
              as when using long records) estimates of probabilities of extreme
              still water levels than the block maxima method. In analysing a
              variety of threshold selection methods we find that using the
              99.7th percentile water level leads to the most stable return
              water level estimates along the German Bight. This is also valid
              for the international stations considered. Finally, to provide
              guidance for coastal engineers and operators, we recommend the
              peaks over threshold method and define an objective approach for
              setting up the model. If this is applied routinely around a
              country, it will help overcome the problem of heterogeneous
              levels of protection resulting from different methods and varying
              model setups.",
  journal  = "Coast. Eng.",
  volume   =  81,
  pages    = "51--66",
  month    =  nov,
  year     =  2013,
  keywords = "Extreme water levels; Coastal flooding; Return levels and
              periods; Extreme value distribution; German Bight",
  issn     = "0378-3839",
  doi      = "10.1016/j.coastaleng.2013.07.003"
}


@ARTICLE{Gelman2006-wv,
  title    = "Prior distributions for variance parameters in hierarchical
              models (comment on article by {B}rowne and {D}raper)",
  author   = "Gelman, Andrew",
  abstract = "Various noninformative prior distributions have been suggested
              for scale parameters in hierarchical models. We construct a new
              folded-noncentral-$t$ family of conditionally conjugate priors
              for hierarchical standard deviation parameters, and then consider
              noninformative and weakly informative priors in this family. We
              use an example to illustrate serious problems with the
              inverse-gamma family of ``noninformative'' prior distributions.
              We suggest instead to use a uniform prior on the hierarchical
              standard deviation, using the half-$t$ family when the number of
              groups is small and in other settings where a weakly informative
              prior is desired. We also illustrate the use of the half-$t$
              family for hierarchical modeling of multiple variance parameters
              such as arise in the analysis of variance.",
  journal  = "Bayesian Anal.",
  volume   =  1,
  number   =  3,
  pages    = "515--533",
  month    =  sep,
  year     =  2006,
  keywords = "Bayesian inference, conditional conjugacy, folded-; bution,
              half-t distribution, hierarchical model, m; prior distribution,
              weakly informative prior distr",
  issn     = "1936-0975, 1931-6690",
  doi      = "10.1214/06-BA117A"
}

@ARTICLE{Srikrishnan2022-yq,
  title = {{Probabilistic projections of baseline twenty-first century CO2
  emissions using a simple calibrated integrated assessment model}},
  author = {Srikrishnan, Vivek and Guan, Yawen and Tol, Richard S J and Keller,
  Klaus},
  journaltitle = {Clim. Change},
  volume = {170},
  issue = {3},
  pages = {37},
  date = {2022-02-24},
  doi = {10.1007/s10584-021-03279-7},
  issn = {0165-0009,1573-1480},
  url = {https://doi.org/10.1007/s10584-021-03279-7},
  urldate = {2022-02-24},
  language = {en}
}

@ARTICLE{Doss-Gollin2023-kl,
  title = {{A subjective Bayesian framework for synthesizing deep uncertainties
  in climate risk management}},
  author = {Doss-Gollin, James and Keller, Klaus},
  journaltitle = {Earths Future},
  publisher = {American Geophysical Union (AGU)},
  volume = {11},
  issue = {1},
  pages = {e2022EF003044},
  date = {2023-01-01},
  doi = {10.1029/2022ef003044},
  issn = {2328-4277},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2022EF003044},
  urldate = {2023-06-09},
  keywords = {decision making under deep uncertainty; climate adaptation;
  climate risk management; house elevation; Bayesian statistics},
  language = {en}
}

@ARTICLE{Ruckert2017-qg,
  title = {{The effects of time-varying observation errors on semi-empirical
  sea-level projections}},
  author = {Ruckert, Kelsey L and Guan, Yawen and Bakker, Alexander M R and
  Forest, Chris E and Keller, Klaus},
  journaltitle = {Clim. Change},
  publisher = {Climatic Change},
  volume = {140},
  issue = {3-4},
  pages = {349--360},
  date = {2017},
  doi = {10.1007/s10584-016-1858-z},
  issn = {0165-0009,1573-1480},
  url = {http://link.springer.com/10.1007/s10584-016-1858-z},
  urldate = {2021-06-14},
  language = {en}
}

@ARTICLE{Fagnant2020-xu,
  title = {{Characterizing spatiotemporal trends in extreme precipitation in
  Southeast Texas}},
  author = {Fagnant, Carlynn and Gori, Avantika and Sebastian, Antonia and
  Bedient, Philip B and Ensor, Katherine B},
  journaltitle = {Nat. Hazards},
  publisher = {Springer Science and Business Media LLC},
  volume = {104},
  issue = {2},
  pages = {1597--1621},
  date = {2020-11-01},
  doi = {10.1007/s11069-020-04235-x},
  issn = {0921-030X,1573-0840},
  urldate = {2024-01-14},
  language = {en}
}

@ARTICLE{Errickson2021-kr,
  title = {{Equity is more important for the social cost of methane than climate
  uncertainty}},
  author = {Errickson, Frank C and Keller, Klaus and Collins, William D and
  Srikrishnan, Vivek and Anthoff, David},
  journaltitle = {Nature},
  publisher = {Nature Publishing Group},
  volume = {592},
  issue = {7855},
  pages = {564--570},
  date = {2021-04-22},
  doi = {10.1038/s41586-021-03386-6},
  issn = {0028-0836,1476-4687},
  url = {http://www.nature.com/articles/s41586-021-03386-6},
  urldate = {2021-06-15},
  language = {en}
}

@MISC{Rockmore2024-ms,
  title = {{How Much of the World Is It Possible to Model?}},
  author = {Rockmore, Dan},
  journaltitle = {The New Yorker},
  publisher = {The New Yorker},
  date = {2024-01-15},
  issn = {0028-792X},
  abstract = {Dan Rockmore writes about mathematical models—including for
  COVID-19, climate change, and artificial intelligence—and whether they have
  limits.},
  url = {https://www.newyorker.com/culture/annals-of-inquiry/how-much-of-the-world-is-it-possible-to-model},
  urldate = {2024-01-16}
}

@ARTICLE{Walker2003-zi,
  title = {{Defining uncertainty: A conceptual basis for uncertainty management
  in model-based decision support}},
  shorttitle = {{Defining uncertainty}},
  author = {Walker, W E and Harremoës, P and Rotmans, J and van der Sluijs, J P
  and van Asselt, M B A and Janssen, P and Krayer von Krauss, M P},
  journaltitle = {Integrated Assessment},
  publisher = {Taylor \& Francis},
  volume = {4},
  issue = {1},
  pages = {5--17},
  date = {2003-03-01},
  doi = {10.1076/iaij.4.1.5.16466},
  issn = {1389-5176},
  abstract = {The aim of this paper is to provide a conceptual basis for the
  systematic treatment of uncertainty in model-based decision support activities
  such as policy analysis, integrated assessment and risk assessment. It focuses
  on the uncertainty perceived from the point of view of those providing
  information to support policy decisions (i.e., the modellers? view on
  uncertainty) ? uncertainty regarding the analytical outcomes and conclusions
  of the decision support exercise. Within the regulatory and management
  sciences, there is neither commonly shared terminology nor full agreement on a
  typology of uncertainties. Our aim is to synthesise a wide variety of
  contributions on uncertainty in model-based decision support in order to
  provide an interdisciplinary theoretical framework for systematic uncertainty
  analysis. To that end we adopt a general definition of uncertainty as being
  any deviation from the unachievable ideal of completely deterministic
  knowledge of the relevant system. We further propose to discriminate among
  three dimensions of uncertainty: location, level and nature of uncertainty,
  and we harmonise existing typologies to further detail the concepts behind
  these three dimensions of uncertainty. We propose an uncertainty matrix as a
  heuristic tool to classify and report the various dimensions of uncertainty,
  thereby providing a conceptual framework for better communication among
  analysts as well as between them and policymakers and stakeholders.
  Understanding the various dimensions of uncertainty helps in identifying,
  articulating, and prioritising critical uncertainties, which is a crucial step
  to more adequate acknowledgement and treatment of uncertainty in decision
  support endeavours and more focused research on complex, inherently uncertain,
  policy issues.},
  url = {http://www.tandfonline.com/doi/abs/10.1076/iaij.4.1.5.16466},
  urldate = {2021-06-14},
  file = {All_Papers/W/Walker_et_al._2003_-_Defining_uncertainty_-_A_conceptual_basis_for_uncertainty_management_in_model-based_decision_support.pdf},
  keywords = {notion},
  language = {en}
}

@ARTICLE{Bayes1763-at,
  title = {{An Essay towards Solving a Problem in the Doctrine of Chance}},
  author = {Bayes, Thomas},
  journaltitle = {Philosophical Transactions of the Royal Society of London},
  volume = {53},
  pages = {370--418},
  date = {1763}
}

@INBOOK{Laplace1774-nf,
  title = {{Mémoire sur la Probabilité des Causes par les évènemens}},
  author = {Laplace, P S},
  booktitle = {{Mémoires de Mathematique et de Physique, Presentés à l’Académie
  Royale des Sciences, Par Divers Savans \& Lus Dans ses Assemblées}},
  pages = {621--656},
  date = {1774}
}

@ARTICLE{Stein2020-rb,
  title = {{Some statistical issues in climate science}},
  author = {Stein, Michael L},
  journaltitle = {Stat. Sci.},
  publisher = {Institute of Mathematical Statistics},
  volume = {35},
  issue = {1},
  pages = {31--41},
  date = {2020-02-01},
  doi = {10.1214/19-sts730},
  issn = {0883-4237,2168-8745},
  url = {https://projecteuclid.org/journals/statistical-science/volume-35/issue-1/Some-Statistical-Issues-in-Climate-Science/10.1214/19-STS730.full},
  file = {All_Papers/S/Stein_2020_-_Some_statistical_issues_in_climate_science.pdf},
  language = {en}
}

@ARTICLE{Brynjarsdottir2014-ve,
  title = {{Learning about physical parameters: the importance of model
  discrepancy}},
  shorttitle = {{Learning about physical parameters}},
  author = {Brynjarsdóttir, Jenný and O'Hagan, Anthony},
  journaltitle = {Inverse Problems},
  volume = {30},
  issue = {11},
  pages = {114007},
  date = {2014-11-01},
  doi = {10.1088/0266-5611/30/11/114007},
  issn = {0266-5611,1361-6420},
  abstract = {Science-based simulation models are widely used to predict the
  behavior of complex physical systems. It is also common to use observations of
  the physical system to solve the inverse problem, that is, to learn about the
  values of parameters within the model, a process which is often called
  calibration. The main goal of calibration is usually to improve the predictive
  performance of the simulator but the values of the parameters in the model may
  also be of intrinsic scientiﬁc interest in their own right. In order to make
  appropriate use of observations of the physical system it is important to
  recognize model discrepancy, the difference between reality and the simulator
  output. We illustrate through a simple example that an analysis that does not
  account for model discrepancy may lead to biased and over-conﬁdent parameter
  estimates and predictions. The challenge with incorporating model discrepancy
  in statistical inverse problems is being confounded with calibration
  parameters, which will only be resolved with meaningful priors. For our simple
  example, we model the model-discrepancy via a Gaussian process and demonstrate
  that through accounting for model discrepancy our prediction within the range
  of data is correct. However, only with realistic priors on the model
  discrepancy do we uncover the true parameter values. Through theoretical
  arguments we show that these ﬁndings are typical of the general problem of
  learning about physical parameters and the underlying physical system using
  science-based mechanistic models.},
  url = {https://iopscience.iop.org/article/10.1088/0266-5611/30/11/114007},
  urldate = {2021-06-14},
  file = {All_Papers/B/Brynjarsdóttir_and_OʼHagan_2014_-_Brynjarsdóttir_and_OʼHagan_-_2014_-_Learning_about_physical_parameters_the_importance.pdf},
  keywords = {notion},
  language = {en}
}

@ARTICLE{Gelman2004-gh,
  title = {{Exploratory Data Analysis for Complex Models}},
  author = {Gelman, Andrew},
  journaltitle = {J. Comput. Graph. Stat.},
  volume = {13},
  issue = {4},
  pages = {755--779},
  date = {2004},
  doi = {10.1198/106186004X11435},
  issn = {1061-8600,1537-2715},
  abstract = {“Exploratory” and “confirmatory” data analysis can both be viewed
  as methods for comparing observed data to what would be obtained under an
  implicit or explicit statistical model. For example, many of Tukey's methods
  can be interpreted as checks against hypothetical linear models and Poisson
  distributions. In more complex situations, Bayesian methods can be useful for
  constructing reference distributions for various plots that are useful in
  exploratory data analysis. This article proposes an approach to unify
  exploratory data analysis with more formal statistical methods based on
  probability models. These ideas are developed in the context of examples from
  fields including psychology, medicine, and social science.},
  url = {http://www.stat.columbia.edu/~gelman/research/published/p755.pdf},
  file = {All_Papers/G/Gelman_2004_-_Exploratory_Data_Analysis_for_Complex_Models.pdf}
}

@BOOK{Jaynes2003-lx,
  title = {{Probability theory: the Logic of Science}},
  shorttitle = {{Probability theory}},
  author = {Jaynes, E T},
  editor = {Bretthorst, G Larry},
  publisher = {Cambridge University Press},
  location = {Cambridge, UK ; New York, NY},
  date = {2003-04-10},
  pagetotal = {727},
  isbn = {9780521592710},
  abstract = {The standard rules of probability can be interpreted as uniquely
  valid principles in logic. In this book, E. T. Jaynes dispels the imaginary
  distinction between 'probability theory' and 'statistical inference', leaving
  a logical unity and simplicity, which provides greater technical power and
  flexibility in applications. This book goes beyond the conventional
  mathematics of probability theory, viewing the subject in a wider context. New
  results are discussed, along with applications of probability theory to a wide
  variety of problems in physics, mathematics, economics, chemistry and biology.
  It contains many exercises and problems, and is suitable for use as a textbook
  on graduate level courses involving data analysis. The material is aimed at
  readers who are already familiar with applied mathematics at an advanced
  undergraduate level or higher. The book will be of interest to scientists
  working in any area where inference from incomplete information is necessary.},
  url = {https://market.android.com/details?id=book-tTN4HuUNXjgC},
  keywords = {notion},
  language = {en}
}

@ARTICLE{Gelman2017-zp,
  title = {{The prior can often only be understood in the context of the
  likelihood}},
  author = {Gelman, Andrew and Simpson, Daniel and Betancourt, Michael},
  journaltitle = {Entropy},
  publisher = {Multidisciplinary Digital Publishing Institute},
  volume = {19},
  issue = {10},
  pages = {555},
  date = {2017-10-19},
  doi = {10.3390/e19100555},
  issn = {1099-4300},
  abstract = {A key sticking point of Bayesian analysis is the choice of prior
  distribution, and there is a vast literature on potential defaults including
  uniform priors, Jeffreys’ priors, reference priors, maximum entropy priors,
  and weakly informative priors. These methods, however, often manifest a key
  conceptual tension in prior modeling: a model encoding true prior information
  should be chosen without reference to the model of the measurement process,
  but almost all common prior modeling techniques are implicitly motivated by a
  reference likelihood. In this paper we resolve this apparent paradox by
  placing the choice of prior into the context of the entire Bayesian analysis,
  from inference to prediction to model evaluation.},
  url = {http://www.mdpi.com/1099-4300/19/10/555},
  urldate = {2021-06-15},
  file = {All_Papers/G/Gelman_et_al._2017_-_The_prior_can_often_only_be_understood_in_the_context_of_the_likelihood.pdf},
  keywords = {notion},
  language = {en}
}

@ARTICLE{Gelman2013-dw,
  title = {{Philosophy and the practice of Bayesian statistics}},
  shorttitle = {{Philosophy and the practice of bayesian statistics}},
  author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
  journaltitle = {Br. J. Math. Stat. Psychol.},
  volume = {66},
  issue = {1},
  pages = {8--38},
  date = {2013-02},
  doi = {10.1111/j.2044-8317.2011.02037.x},
  pmc = {PMC4476974},
  pmid = {22364575},
  issn = {0007-1102,2044-8317},
  abstract = {A substantial school in the philosophy of science identifies
  Bayesian inference with inductive inference and even rationality as such, and
  seems to be strengthened by the rise and practical success of Bayesian
  statistics. We argue that the most successful forms of Bayesian statistics do
  not actually support that particular philosophy but rather accord much better
  with sophisticated forms of hypothetico-deductivism. We examine the actual
  role played by prior distributions in Bayesian models, and the crucial aspects
  of model checking and model revision, which fall outside the scope of Bayesian
  confirmation theory. We draw on the literature on the consistency of Bayesian
  updating and also on our experience of applied work in social science. Clarity
  about these matters should benefit not just philosophy of science, but also
  statistical practice. At best, the inductivist view has encouraged researchers
  to fit and compare models without checking them; at worst, theorists have
  actively discouraged practitioners from performing model checking because it
  does not fit into their framework.},
  url = {http://doi.wiley.com/10.1111/j.2044-8317.2011.02037.x},
  urldate = {2021-06-15},
  file = {All_Papers/G/Gelman_and_Shalizi_2013_-_Philosophy_and_the_practice_of_Bayesian_statistics.pdf},
  keywords = {notion},
  language = {en}
}

@ARTICLE{Doss-Gollin2021-kc,
  title = {{How unprecedented was the February 2021 Texas cold snap?}},
  author = {Doss-Gollin, James and Farnham, David J and Lall, Upmanu and Modi,
  Vijay},
  journaltitle = {Environ. Res. Lett.},
  volume = {16},
  issue = {6},
  pages = {064056},
  date = {2021-06},
  doi = {10/gnswvt},
  issn = {1748-9326},
  abstract = {Winter storm Uri brought severe cold to the southern United States
  in February 2021, causing a cascading failure of interdependent systems in
  Texas where infrastructure was not adequately prepared for such cold. In
  particular, the failure of interconnected energy systems restricted
  electricity supply just as demand for heating spiked, leaving millions of
  Texans without heat or electricity, many for several days. This motivates the
  question: did historical storms suggest that such temperatures were known to
  occur, and if so with what frequency? We compute a temperature-based proxy for
  heating demand and use this metric to answer the question ‘what would the
  aggregate demand for heating have been had historic cold snaps occurred with
  today’s population?’. We find that local temperatures and the inferred demand
  for heating per capita across the region served by the Texas Interconnection
  were more severe during a storm in December 1989 than during February 2021,
  and that cold snaps in 1951 and 1983 were nearly as severe. Given anticipated
  population growth, future storms may lead to even greater infrastructure
  failures if adaptive investments are not made. Further, electricity system
  managers should prepare for trends in electrification of heating to drive peak
  annual loads on the Texas Interconnection during severe winter storms.},
  url = {https://doi.org/10.1088/1748-9326/ac0278},
  urldate = {2021-12-17},
  file = {All_Papers/D/Doss-Gollin_et_al._2021_-_How_unprecedented_was_the_February_2021_Texas_cold_snap.pdf},
  keywords = {notion},
  language = {en}
}
